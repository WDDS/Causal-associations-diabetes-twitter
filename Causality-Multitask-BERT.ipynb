{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e908c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 5434\n",
      "Labeled count: 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>Charline association0=no;1=yes</th>\n",
       "      <th>Remarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>908171203029868545</td>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1203645589214367745</td>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1310596731063525376</td>\n",
       "      <td>⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...</td>\n",
       "      <td>⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1125198453167022085</td>\n",
       "      <td>USER Cheers ! Have one for this diabetic too !</td>\n",
       "      <td>USER Cheers ! Have one for this diabetic too !</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1248600944138268673</td>\n",
       "      <td>USER Additionally the medicines are being char...</td>\n",
       "      <td>USER Additionally the medicines are being char...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medicines are being charged at MRP</td>\n",
       "      <td>costing much higher</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0   908171203029868545  tonight , I learned my older girl will back he...   \n",
       "1  1203645589214367745  USER USER I knew diabetes and fibromyalgia wer...   \n",
       "2  1310596731063525376  ⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...   \n",
       "3  1125198453167022085     USER Cheers ! Have one for this diabetic too !   \n",
       "4  1248600944138268673  USER Additionally the medicines are being char...   \n",
       "\n",
       "                                           full_text Intent  \\\n",
       "0  tonight , I learned my older girl will back he...    NaN   \n",
       "1  USER USER I knew diabetes and fibromyalgia wer...   joke   \n",
       "2  ⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...     mS   \n",
       "3     USER Cheers ! Have one for this diabetic too !     mS   \n",
       "4  USER Additionally the medicines are being char...    NaN   \n",
       "\n",
       "                                Cause               Effect  \\\n",
       "0                                 NaN                  NaN   \n",
       "1                                 NaN                  NaN   \n",
       "2                                 NaN                  NaN   \n",
       "3                                 NaN                  NaN   \n",
       "4  medicines are being charged at MRP  costing much higher   \n",
       "\n",
       "   Causal association  Charline association0=no;1=yes Remarks  \n",
       "0                 0.0                             NaN     NaN  \n",
       "1                 0.0                             NaN     NaN  \n",
       "2                 0.0                             NaN     NaN  \n",
       "3                 0.0                             NaN     NaN  \n",
       "4                 1.0                             NaN     NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from tqdm import tqdm, trange\n",
    "from utils import normalizeTweet, split_into_sentences, bio_tagging, create_training_data\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_excel(\"/home/adrian/workspace/causality/Causal-associations-diabetes-twitter/data/Causality + hypoglycemia.xlsx\", sheet_name=\">5000_samples_\")\n",
    "#data = pd.read_excel(\"/home/adrian/Downloads/Causality + hypoglycemia.xlsx\", sheet_name=\">5000_samples_\")\n",
    "print(\"Total count:\", data.shape[0])\n",
    "data = data[data[\"Causal association\"].notnull()]\n",
    "print(\"Labeled count:\", data.shape[0])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af559048",
   "metadata": {},
   "source": [
    "## Add BIO tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd7482f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>Charline association0=no;1=yes</th>\n",
       "      <th>Remarks</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>bio_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>908171203029868545</td>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tonight, ,, I, learned, my, older, girl, will...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1203645589214367745</td>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[USER, USER, I, knew, diabetes, and, fibromyal...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1310596731063525376</td>\n",
       "      <td>⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...</td>\n",
       "      <td>⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[:down_arrow:, :down_arrow:, :down_arrow:, THI...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1125198453167022085</td>\n",
       "      <td>USER Cheers ! Have one for this diabetic too !</td>\n",
       "      <td>USER Cheers ! Have one for this diabetic too !</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[USER, Cheers, !, Have, one, for, this, diabet...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1248600944138268673</td>\n",
       "      <td>USER Additionally the medicines are being char...</td>\n",
       "      <td>USER Additionally the medicines are being char...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medicines are being charged at MRP</td>\n",
       "      <td>costing much higher</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[USER, Additionally, the, medicines, are, bein...</td>\n",
       "      <td>[O, O, O, B-C, I-C, I-C, I-C, I-C, I-C, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0   908171203029868545  tonight , I learned my older girl will back he...   \n",
       "1  1203645589214367745  USER USER I knew diabetes and fibromyalgia wer...   \n",
       "2  1310596731063525376  ⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...   \n",
       "3  1125198453167022085     USER Cheers ! Have one for this diabetic too !   \n",
       "4  1248600944138268673  USER Additionally the medicines are being char...   \n",
       "\n",
       "                                           full_text Intent  \\\n",
       "0  tonight , I learned my older girl will back he...    NaN   \n",
       "1  USER USER I knew diabetes and fibromyalgia wer...   joke   \n",
       "2  ⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...     mS   \n",
       "3     USER Cheers ! Have one for this diabetic too !     mS   \n",
       "4  USER Additionally the medicines are being char...    NaN   \n",
       "\n",
       "                                Cause               Effect  \\\n",
       "0                                 NaN                  NaN   \n",
       "1                                 NaN                  NaN   \n",
       "2                                 NaN                  NaN   \n",
       "3                                 NaN                  NaN   \n",
       "4  medicines are being charged at MRP  costing much higher   \n",
       "\n",
       "   Causal association  Charline association0=no;1=yes Remarks  \\\n",
       "0                 0.0                             NaN     NaN   \n",
       "1                 0.0                             NaN     NaN   \n",
       "2                 0.0                             NaN     NaN   \n",
       "3                 0.0                             NaN     NaN   \n",
       "4                 1.0                             NaN     NaN   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [tonight, ,, I, learned, my, older, girl, will...   \n",
       "1  [USER, USER, I, knew, diabetes, and, fibromyal...   \n",
       "2  [:down_arrow:, :down_arrow:, :down_arrow:, THI...   \n",
       "3  [USER, Cheers, !, Have, one, for, this, diabet...   \n",
       "4  [USER, Additionally, the, medicines, are, bein...   \n",
       "\n",
       "                                            bio_tags  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3                     [O, O, O, O, O, O, O, O, O, O]  \n",
       "4  [O, O, O, B-C, I-C, I-C, I-C, I-C, I-C, O, O, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tokenized\"] = data[\"full_text\"].map(lambda tweet: normalizeTweet(tweet).split(\" \"))\n",
    "data[\"bio_tags\"] = data.apply(lambda row: bio_tagging(row[\"full_text\"],row[\"Cause\"], row[\"Effect\"]), axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb933866",
   "metadata": {},
   "source": [
    "### split tweets into sentences => new dataframe with more rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198c6ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tweets: 5000\n",
      "N sentences: 11784\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>bio_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[tonight, ,, I, learned, my, older, girl, will...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fiercely .</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[Fiercely, .]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#impressive #bigsister #type1 #type1times2</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[#impressive, #bigsister, #type1, #type1times2]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[USER, USER, I, knew, diabetes, and, fibromyal...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>:face_with_rolling_eyes:</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[:face_with_rolling_eyes:]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence Intent Cause Effect  \\\n",
       "0  tonight , I learned my older girl will back he...          NaN    NaN   \n",
       "1                                         Fiercely .          NaN    NaN   \n",
       "2         #impressive #bigsister #type1 #type1times2          NaN    NaN   \n",
       "3  USER USER I knew diabetes and fibromyalgia wer...   joke   NaN    NaN   \n",
       "4                           :face_with_rolling_eyes:   joke   NaN    NaN   \n",
       "\n",
       "  Causal association                                          tokenized  \\\n",
       "0                  0  [tonight, ,, I, learned, my, older, girl, will...   \n",
       "1                  0                                      [Fiercely, .]   \n",
       "2                  0    [#impressive, #bigsister, #type1, #type1times2]   \n",
       "3                  0  [USER, USER, I, knew, diabetes, and, fibromyal...   \n",
       "4                  0                         [:face_with_rolling_eyes:]   \n",
       "\n",
       "                                           bio_tags  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1                                            [O, O]  \n",
       "2                                      [O, O, O, O]  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4                                               [O]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_start_end_index_of_sentence_in_tweet(tweet, sentence):\n",
    "    \"\"\" \n",
    "    The sentence tokens are included in the tweet tokens.\n",
    "    Return the start end end indices of the sentence tokens in the tweet tokens\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_start_word = sentence[0]\n",
    "    start_indices = [i for i, x in enumerate(tweet) if x == sentence_start_word] # find all indices of the start word of the sentence \n",
    "    try:\n",
    "        for start_index in start_indices:\n",
    "            isTrueStartIndex = all([tweet[start_index+i] == sentence[i] for i in range(len(sentence))])\n",
    "            #print(\"start_index:\", start_index, \"isTrueStartIndex:\", isTrueStartIndex)\n",
    "            if isTrueStartIndex:\n",
    "                return start_index, start_index + len(sentence) \n",
    "    except:\n",
    "        print(\"ERROR: StartIndex should have been found for sentence:\")\n",
    "        print(\"tweet:\")\n",
    "        print(tweet)\n",
    "        print(\"sentence:\")\n",
    "        print(sentence)\n",
    "    return -1, -2 # should not be returned\n",
    "\n",
    "\n",
    "def split_tweets_to_sentences(data):\n",
    "    \"\"\" \n",
    "        Splits tweets into sentences and associates the appropriate intent, causes, effects and causal association\n",
    "        to each sentence.\n",
    "        \n",
    "        Parameters:\n",
    "        - min_words_in_sentences: Minimal number of words in a sentence such that the sentence is kept. \n",
    "                                  Assumption: A sentence with too few words does not have enough information\n",
    "                              \n",
    "                              \n",
    "                              \n",
    "        Ex.:\n",
    "        full_text                              | Intent | Cause | Effect | Causal association | ...\n",
    "        --------------------------------------------------------------------------------------------\n",
    "        what? type 1 causes insulin dependence | q;msS  | type 1|insulin dependence | 1       | ...  \n",
    "        \n",
    "        New dataframe returned: \n",
    "        full_text                              | Intent | Cause | Effect | Causal association | ...\n",
    "        --------------------------------------------------------------------------------------------\n",
    "        what?                                  |   q    |       |        |       0            | ...\n",
    "        type 1 causes insulin dependence       |        | type 1| insulin dependence | 1       | ...  \n",
    "    \"\"\"\n",
    "\n",
    "    newDF = pd.DataFrame(columns=[\"sentence\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\", \"tokenized\", \"bio_tags\"])\n",
    "    \n",
    "    for i,row in data.iterrows():\n",
    "        causes = row[\"Cause\"]\n",
    "        effects = row[\"Effect\"]\n",
    "        sentences = split_into_sentences(normalizeTweet(row[\"full_text\"]))\n",
    "\n",
    "        # single sentence in tweet\n",
    "        if len(sentences) == 1:\n",
    "            singleSentenceIntent = \"\"\n",
    "            if isinstance(row[\"Intent\"], str):\n",
    "                if len(row[\"Intent\"].split(\";\")) > 1:\n",
    "                    singleSentenceIntent = row[\"Intent\"].strip().replace(\";msS\", \"\").replace(\"msS;\", \"\").replace(\";mS\", \"\").replace(\"mS;\", \"\")\n",
    "                else:\n",
    "                    if row[\"Intent\"] == \"mS\" or row[\"Intent\"] == \"msS\":\n",
    "                        singleSentenceIntent = \"\"\n",
    "                    else:\n",
    "                        singleSentenceIntent = row[\"Intent\"].strip()\n",
    "                    \n",
    "            newDF=newDF.append(pd.Series({\"sentence\": sentences[0] # only one sentence\n",
    "                         , \"Intent\": singleSentenceIntent\n",
    "                         , \"Cause\" : row[\"Cause\"]\n",
    "                         , \"Effect\": row[\"Effect\"]\n",
    "                         , \"Causal association\" : row[\"Causal association\"]\n",
    "                         , \"tokenized\": row[\"tokenized\"]\n",
    "                         , \"bio_tags\": row[\"bio_tags\"]}), ignore_index=True)\n",
    "        \n",
    "        # tweet has several sentences\n",
    "        else: \n",
    "            intents = str(row[\"Intent\"]).strip().split(\";\")\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                sent_tokenized = sentence.split(\" \")\n",
    "                \n",
    "                causeInSentence = np.nan if not isinstance(causes, str) or not any([cause in sentence for cause in causes.split(\";\")]) else \";\".join([cause for cause in causes.split(\";\") if cause in sentence])\n",
    "                effectInSentence = np.nan if not isinstance(effects, str) or not any([effect in sentence for effect in effects.split(\";\")]) else \";\".join([effect for effect in effects.split(\";\") if effect in sentence])\n",
    "                causalAssociationInSentence = 1 if isinstance(causeInSentence, str) and isinstance(effectInSentence, str) else 0\n",
    "                \n",
    "                startIndex, endIndex = get_start_end_index_of_sentence_in_tweet(row[\"tokenized\"], sent_tokenized)\n",
    "                sentence_tokenized = row[\"tokenized\"][startIndex:endIndex]\n",
    "                sentence_bio_tags = row[\"bio_tags\"][startIndex:endIndex]\n",
    "                \n",
    "                if \"q\" in intents and sentence[-1] == \"?\": # if current sentence is question\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"q\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)                    \n",
    "                elif \"joke\" in intents: # all sentences with \"joke\" in tweet keep the intent \"joke\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"joke\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)   \n",
    "                elif \"neg\" in intents: # all sentences with \"neg\" in tweet keep intent \"neg\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"neg\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)               \n",
    "                elif isinstance(causeInSentence, str) and isinstance(effectInSentence, str): # cause effect sentence\n",
    "                    causalIntent = \"\"\n",
    "                    if len(causeInSentence.split(\";\")) > 1:\n",
    "                        causalIntent = \"mC\"\n",
    "                        if len(effectInSentence.split(\";\")) > 1:\n",
    "                            causalIntent = \"mC;mE\"\n",
    "                    elif len(effectInSentence.split(\";\")) > 1:\n",
    "                        causalIntent = \"mE\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": causalIntent, \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)                                  \n",
    "                else:\n",
    "                    nonCausalIntent = \"\"\n",
    "                    if isinstance(causeInSentence, str): # only cause is given\n",
    "                        if len(causeInSentence.split(\";\")) > 1:\n",
    "                            nonCausalIntent = \"mC\"\n",
    "                    elif isinstance(effectInSentence, str): # only effect is given\n",
    "                        if len(effectInSentence.split(\";\")) > 1:\n",
    "                            nonCausalIntent = \"mE\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": nonCausalIntent, \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)\n",
    "\n",
    "    return newDF\n",
    "       \n",
    "# sample: has one example for each possible \"Intent\" value\n",
    "#allIntents = data[\"Intent\"].value_counts().keys().tolist()\n",
    "#sample = data[data[\"Intent\"] == \"mS\"][0:1]\n",
    "#for intent in allIntents:\n",
    "#    sample = sample.append(data[data[\"Intent\"] == intent][1:2])\n",
    "#print(sample.shape)\n",
    "\n",
    "#i = 19\n",
    "#test = sample[i:i+1]\n",
    "#dataSentences = split_tweets_to_sentences(test)\n",
    "#dataSentences.head(30)\n",
    "#test.head()\n",
    "\n",
    "print(\"N tweets:\", data.shape[0])\n",
    "dataSentences = split_tweets_to_sentences(data)\n",
    "print(\"N sentences:\", dataSentences.shape[0])\n",
    "dataSentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec6412",
   "metadata": {},
   "source": [
    "### Filter out negation, jokes, questions and sentences with minimal token length of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f720a0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N sentences before filtering:  11784\n",
      "N sentences after filtering:  8835\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>bio_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[tonight, ,, I, learned, my, older, girl, will...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#impressive #bigsister #type1 #type1times2</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[#impressive, #bigsister, #type1, #type1times2]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>:down_arrow: :down_arrow: :down_arrow: THIS :d...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[:down_arrow:, :down_arrow:, :down_arrow:, THI...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I 'm a trans woman .</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, 'm, a, trans, woman, .]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Both of us could use a world where \" brave and...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[Both, of, us, could, use, a, world, where, \",...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence Intent Cause Effect  \\\n",
       "0  tonight , I learned my older girl will back he...          NaN    NaN   \n",
       "2         #impressive #bigsister #type1 #type1times2          NaN    NaN   \n",
       "5  :down_arrow: :down_arrow: :down_arrow: THIS :d...          NaN    NaN   \n",
       "6                               I 'm a trans woman .          NaN    NaN   \n",
       "7  Both of us could use a world where \" brave and...          NaN    NaN   \n",
       "\n",
       "  Causal association                                          tokenized  \\\n",
       "0                  0  [tonight, ,, I, learned, my, older, girl, will...   \n",
       "2                  0    [#impressive, #bigsister, #type1, #type1times2]   \n",
       "5                  0  [:down_arrow:, :down_arrow:, :down_arrow:, THI...   \n",
       "6                  0                        [I, 'm, a, trans, woman, .]   \n",
       "7                  0  [Both, of, us, could, use, a, world, where, \",...   \n",
       "\n",
       "                                            bio_tags  \n",
       "0   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "2                                       [O, O, O, O]  \n",
       "5         [O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "6                                 [O, O, O, O, O, O]  \n",
       "7  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"N sentences before filtering: \", dataSentences.shape[0])\n",
    "dataSentFiltered = dataSentences[~dataSentences[\"Intent\"].str.contains(\"neg|joke|q\")] # remove sentences with joke, q, neg\n",
    "dataSentFiltered = dataSentFiltered[dataSentFiltered[\"tokenized\"].map(len) >= 3] # only keep sentences with at least 3 words\n",
    "print(\"N sentences after filtering: \", dataSentFiltered.shape[0])\n",
    "dataSentFiltered.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a384a53",
   "metadata": {},
   "source": [
    "### only work on cause-effect tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6845147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take sentences with cause and effect\n",
    "#trainingData = dataSentFiltered[dataSentFiltered[\"Causal association\"] == 1]\n",
    "#trainingData.shape\n",
    "\n",
    "# not in the multitask setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606db122",
   "metadata": {},
   "source": [
    "### Create training, validation, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c092cc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: \tCount = 300, % of 0 = 0.8767, % of 1 = 0.1233\n",
      "Train: \tCount = 192, % of 0 = 0.875, % of 1 = 0.125\n",
      "Val: \tCount = 48, % of 0 = 0.875, % of 1 = 0.125\n",
      "Test: \tCount = 60, % of 0 = 0.8833, % of 1 = 0.1167\n",
      "Balancing class wts: for 0 = 0.5714, for 1 = 4.0\n"
     ]
    }
   ],
   "source": [
    "####################### Stratified splits ####################\n",
    "trainingData = dataSentFiltered#.sample(n=300, random_state=0) # TODO: remove sample\n",
    "train, test = train_test_split(trainingData, test_size=0.2, stratify=trainingData[[\"Causal association\"]], random_state=0)\n",
    "train, val = train_test_split(train, test_size=0.2, stratify=train[[\"Causal association\"]], random_state=0)\n",
    "\n",
    "data_count_info = trainingData[\"Causal association\"].value_counts(normalize=True)\n",
    "train_count_info = train[\"Causal association\"].value_counts(normalize=True)\n",
    "val_count_info = val[\"Causal association\"].value_counts(normalize=True)\n",
    "test_count_info = test[\"Causal association\"].value_counts(normalize=True)\n",
    "\n",
    "# for class-imbalanced dataset, the class weight for a ith class\n",
    "# to be specified for balancing in the loss function is given by:\n",
    "# weight[i] = num_samples / (num_classes * num_samples[i])\n",
    "# since train_count_info obtained above has fraction of samples\n",
    "# for ith class, hence the corresponding weight calculation is:\n",
    "class_weight = (1/train_count_info)/len(train_count_info)\n",
    "\n",
    "print(\"All: \\tCount = {}, % of 0 = {}, % of 1 = {}\".format(\n",
    "    len(trainingData[\"Causal association\"]), *data_count_info.round(4).to_list()))\n",
    "print(\"Train: \\tCount = {}, % of 0 = {}, % of 1 = {}\".format(\n",
    "    len(train[\"Causal association\"]), *train_count_info.round(4).to_list()))\n",
    "print(\"Val: \\tCount = {}, % of 0 = {}, % of 1 = {}\".format(\n",
    "    len(val[\"Causal association\"]), *val_count_info.round(4).to_list()))\n",
    "print(\"Test: \\tCount = {}, % of 0 = {}, % of 1 = {}\".format(\n",
    "    len(test[\"Causal association\"]), *test_count_info.round(4).to_list()))\n",
    "print(\"Balancing class wts: for 0 = {}, for 1 = {}\".format(\n",
    "    *class_weight.round(4).to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a011e870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    263\n",
       "1     37\n",
       "Name: Causal association, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData[\"Causal association\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6efd676f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:18: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:18: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<ipython-input-9-b05fabe831cb>:18: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(ids[idx]) == len(bio_tags_extended), \"token ids and BIO tags lengths do not match!\")\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n",
      "48\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# Transform labels + encodings into Pytorch DataSet object (including __len__, __getitem__)\n",
    "class TweetDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, labels, bio_tags, tokenizer):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bio_tags = bio_tags\n",
    "        self.tag2id = {label: idx for idx, label in enumerate([\"O\", \"B-C\", \"I-C\", \"B-E\", \"I-E\"])}\n",
    "        self.tag2id[-100] = -100\n",
    "        self.id2tag = {id:tag for tag,id in self.tag2id.items()}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.text, padding=True, truncation=True, return_token_type_ids=True)\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        bio_tags_extended = self.extend_tags(self.text[idx], self.bio_tags[idx], ids[idx])\n",
    "        assert(len(ids[idx]) == len(bio_tags_extended), \"token ids and BIO tags lengths do not match!\")\n",
    "        return {\n",
    "                \"input_ids\" : torch.tensor(ids[idx], dtype=torch.long)\n",
    "              , \"attention_mask\" : torch.tensor(mask[idx], dtype=torch.long)\n",
    "              , \"token_type_ids\" : torch.tensor(token_type_ids[idx], dtype=torch.long)\n",
    "              , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "              , \"bio_tags\" : torch.tensor(list(map(lambda bioTags: self.tag2id[bioTags], bio_tags_extended))\n",
    ", dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    \n",
    "    def extend_tags(self, tokens_old, tags_old, ids_tokenized_padded):\n",
    "        \"\"\" \n",
    "            Each token has a BIO tag label. \n",
    "            However BERT's tokenization splits tokens into subwords. How to label those subwords?\n",
    "            \n",
    "            Option 1:\n",
    "            ---------\n",
    "            \n",
    "            add the same label to each subword than the first subword. Only replace \"B\" by \"I\"\n",
    "            Ex. \n",
    "            #lowbloodsugar => '#low@@', 'blood@@', 'sugar@@'\n",
    "               \"B-C\"       =>   \"B-C\" ,   \"I-C\"  ,   \"I-C\"\n",
    "            \n",
    "            Option 2 (implemented):      \n",
    "            ---------\n",
    "            \n",
    "            From : https://huggingface.co/transformers/custom_datasets.html#token-classification-with-w-nut-emerging-entities\n",
    "            A common obstacle with using pre-trained models for token-level classification: many of the tokens in\n",
    "            the W-NUT corpus are not in DistilBert’s vocabulary. Bert and many models like it use a method called \n",
    "            WordPiece Tokenization, meaning that single words are split into multiple tokens such that each token\n",
    "            is likely to be in the vocabulary. For example, DistilBert’s tokenizer would split the Twitter \n",
    "            handle @huggingface into the tokens ['@', 'hugging', '##face']. This is a problem for us because we \n",
    "            have exactly one tag per token. If the tokenizer splits a token into multiple sub-tokens, then we will\n",
    "            end up with a mismatch between our tokens and our labels.\n",
    "\n",
    "            One way to handle this is to only train on the tag labels for the first subtoken of a split token. \n",
    "            We can do this in 🤗 Transformers by setting the labels we wish to ignore to -100. \n",
    "            In the example above, if the label for @HuggingFace is 3 (indexing B-corporation), we would set \n",
    "            the labels of ['@', 'hugging', '##face'] to [3, -100, -100].\n",
    "        \"\"\"\n",
    "        tags = [-100] # add for start token <CLS>\n",
    "        for token_old, tag in zip(tokens_old.split(\" \"), tags_old):\n",
    "#            print(F\"\\ntoken_old: {token_old};    tag: {tag}\")\n",
    "            for i, sub_token in enumerate(self.tokenizer.tokenize(token_old)):\n",
    "                if (i == 0):\n",
    "                    tags.append(tag)\n",
    "                else: \n",
    "                    tags.append(-100)\n",
    "           \n",
    "        tags.append(-100) # 0 for end of sentence token\n",
    "    \n",
    "        # append -100 for all padded elements\n",
    "        padded_elements = ids_tokenized_padded.count(1) # id 1 is <PAD> ; Alternative: where attention_mask == 0 add -100\n",
    "        tags.extend([-100]*padded_elements)\n",
    "        \n",
    "        return tags\n",
    "        \n",
    "        \n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "train_dataset = TweetDataSet(train[\"sentence\"].values.tolist()\n",
    "                           , train[\"Causal association\"].values.tolist()\n",
    "                           , train[\"bio_tags\"].values.tolist()\n",
    "                           , tokenizer)\n",
    "val_dataset = TweetDataSet(val[\"sentence\"].values.tolist()\n",
    "                           , val[\"Causal association\"].values.tolist()\n",
    "                           , val[\"bio_tags\"].values.tolist()\n",
    "                           , tokenizer)\n",
    "test_dataset = TweetDataSet(test[\"sentence\"].values.tolist()\n",
    "                           , test[\"Causal association\"].values.tolist()\n",
    "                           , test[\"bio_tags\"].values.tolist()\n",
    "                           , tokenizer)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))\n",
    "\n",
    "# put data to batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76fc4c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred, labels):\n",
    "    \"\"\"\n",
    "        Dataset is unbalanced -> measure weighted metrics\n",
    "        Calculate metrics for each label, and find their average wieghted by support (Number of true instances for each label)\n",
    "        This alters 'macro' to account for label imbalance;\n",
    "        it can result in an F-Score taht is not between precision and recall\n",
    "    \"\"\"\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='macro') #binary\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "class CausalMultiTask(torch.nn.Module):\n",
    "    \"\"\" Model Bert\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CausalMultiTask, self).__init__()\n",
    "        self.num_labels_NER = 5 # B-C, I-C, B-E, I-E, O\n",
    "        self.num_labels_CLS = 2 # 0, 1\n",
    "        self.bert = transformers.BertModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear1 = torch.nn.Linear(768, 256)\n",
    "        self.linear_NER = torch.nn.Linear(256, self.num_labels_NER)\n",
    "        self.linear_CLS = torch.nn.Linear(256, self.num_labels_CLS)\n",
    "        #self.softmax = torch.nn.Softmax(-1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_seq, output_cls = self.bert(input_ids, attention_mask = attention_mask, token_type_ids=token_type_ids, return_dict=False) # if output 1 is our cls token\n",
    "\n",
    "        # classification\n",
    "        output_cls_2 = self.dropout(output_cls)\n",
    "        output_cls_3 = self.linear1(output_cls_2)\n",
    "        output_cls_4 = self.dropout(output_cls_3)\n",
    "        output_cls_5 = self.linear_CLS(output_cls_4)\n",
    "        #logit_cls = self.softmax(output_cls_5)\n",
    "        \n",
    "        # named entity recognition\n",
    "        output_ner_2 = self.dropout(output_seq)\n",
    "        output_ner_3 = self.linear1(output_ner_2)\n",
    "        output_ner_4 = self.dropout(output_ner_3)\n",
    "        output_ner_5 = self.linear_NER(output_ner_4)\n",
    "        #logit_ner = self.softmax(output_ner_5)        \n",
    "        \n",
    "        return output_cls_5, output_ner_5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b7ca06",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "230803d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model parameters\n",
    "batchsize_train = 16\n",
    "lr = 5e-5\n",
    "adam_eps = 1e-8\n",
    "epochs = 3\n",
    "num_warmup_steps = 0\n",
    "num_training_steps = len(train_loader)*epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51e3e0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing BertModel: ['roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'lm_head.decoder.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.dense.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.embeddings.position_ids', 'lm_head.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.11.output.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['encoder.layer.3.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'pooler.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.4.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = CausalMultiTask()\n",
    "model.to(device)\n",
    "\n",
    "# TODO: Check in Multi-task setting, if underlying BERT parameters shall\n",
    "# be updated too to benefit from common training\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=lr, eps=adam_eps)\n",
    "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "loss_fn_ner = CrossEntropyLoss(ignore_index=-100) # ignore subwords/tokens with label -100 \n",
    "## penalising more for class with less number of exaplmes \n",
    "loss_fn_cls = CrossEntropyLoss(torch.tensor(class_weight.to_list()).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c6ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c125b3ac",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91c6ca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  8%|▊         | 1/12 [00:01<00:16,  1.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6297, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.5720, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.2017, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 17%|█▋        | 2/12 [00:02<00:14,  1.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6966, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.5875, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.2842, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 25%|██▌       | 3/12 [00:04<00:12,  1.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.7038, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.5799, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.2837, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 33%|███▎      | 4/12 [00:05<00:11,  1.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6732, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.4991, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.1723, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 42%|████▏     | 5/12 [00:07<00:10,  1.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6762, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.5257, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.2019, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 50%|█████     | 6/12 [00:08<00:08,  1.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6226, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.5028, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.1254, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 58%|█████▊    | 7/12 [00:10<00:07,  1.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.7475, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.5041, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.2517, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 67%|██████▋   | 8/12 [00:11<00:05,  1.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6475, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.5124, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.1599, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 75%|███████▌  | 9/12 [00:13<00:04,  1.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.7523, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.4820, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.2343, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 83%|████████▎ | 10/12 [00:14<00:03,  1.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6937, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.5823, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.2761, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 92%|█████████▏| 11/12 [00:16<00:01,  1.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6364, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.4915, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.1279, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 12/12 [00:17<00:00,  1.47s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6961, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.4245, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.1206, grad_fn=<AddBackward0>)\n",
      "\n",
      "\tTraining Loss: 1.203305612007777\n",
      "\n",
      "\tTraining cls acc: 0.7395833333333334\n",
      "\n",
      "\tTraining cls prec: 0.5279723748473748\n",
      "\n",
      "\tTraining cls rec: 0.5344169719169719\n",
      "\n",
      "\tTraining cls f1: 0.5147047477482259\n",
      "\n",
      "--\n",
      "\tTraining ner acc: 0.9529474373367991\n",
      "\n",
      "\tTraining ner prec: 0.21381219708526158\n",
      "\n",
      "\tTraining ner rec: 0.22336709929302526\n",
      "\n",
      "\tTraining ner f1: 0.21845238280744628\n",
      "\n",
      "\tCurrent Learning rate:  1.6666666666666667e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  2.06it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 33%|███▎      | 2/6 [00:00<00:01,  2.01it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 50%|█████     | 3/6 [00:01<00:01,  1.98it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.96it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 83%|████████▎ | 5/6 [00:02<00:00,  1.91it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 6/6 [00:03<00:00,  1.94it/s]\u001b[A\n",
      "Epoch:  33%|███▎      | 1/3 [00:20<00:41, 20.78s/it]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tValidation Loss: 0.985430101553599\n",
      "\n",
      "\tValidation cls acc: 0.875\n",
      "\n",
      "\tValidation cls prec: 0.5208333333333334\n",
      "\n",
      "\tValidation cls rec: 0.5833333333333334\n",
      "\n",
      "\tValidation cls f1: 0.5492063492063493\n",
      "\n",
      "--\n",
      "\tValidation ner acc: 0.9647872978379096\n",
      "\n",
      "\tValidation ner prec: 0.3779413764869827\n",
      "\n",
      "\tValidation ner rec: 0.38611111111111107\n",
      "\n",
      "\tValidation ner f1: 0.38191068719037785\n",
      "<====================== Epoch 2 ======================>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  8%|▊         | 1/12 [00:01<00:15,  1.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6990, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.4264, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.1255, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 17%|█▋        | 2/12 [00:02<00:14,  1.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6567, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.4286, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.0854, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 25%|██▌       | 3/12 [00:04<00:13,  1.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6741, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.3078, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(0.9819, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 33%|███▎      | 4/12 [00:06<00:12,  1.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.7031, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.4132, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.1163, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 42%|████▏     | 5/12 [00:07<00:10,  1.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.5790, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.3531, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(0.9321, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 50%|█████     | 6/12 [00:09<00:09,  1.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6855, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.3713, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.0568, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 58%|█████▊    | 7/12 [00:10<00:08,  1.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6458, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.3985, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.0443, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 67%|██████▋   | 8/12 [00:12<00:06,  1.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.7569, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.4279, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.1848, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 75%|███████▌  | 9/12 [00:14<00:04,  1.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6137, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.3612, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(0.9749, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 83%|████████▎ | 10/12 [00:15<00:03,  1.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.7671, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.3023, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.0694, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 92%|█████████▏| 11/12 [00:17<00:01,  1.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.7047, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.5797, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.2844, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 12/12 [00:19<00:00,  1.59s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6186, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.3233, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(0.9419, grad_fn=<AddBackward0>)\n",
      "\n",
      "\tTraining Loss: 1.0664655317862828\n",
      "\n",
      "\tTraining cls acc: 0.6979166666666666\n",
      "\n",
      "\tTraining cls prec: 0.5076555389055389\n",
      "\n",
      "\tTraining cls rec: 0.5582875457875459\n",
      "\n",
      "\tTraining cls f1: 0.497125009941102\n",
      "\n",
      "--\n",
      "\tTraining ner acc: 0.9534483094329529\n",
      "\n",
      "\tTraining ner prec: 0.20686699173052678\n",
      "\n",
      "\tTraining ner rec: 0.2166666666666667\n",
      "\n",
      "\tTraining ner f1: 0.21161547695056557\n",
      "\n",
      "\tCurrent Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  1.70it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 33%|███▎      | 2/6 [00:01<00:02,  1.73it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 50%|█████     | 3/6 [00:01<00:01,  1.74it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.75it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 83%|████████▎ | 5/6 [00:02<00:00,  1.78it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      "100%|██████████| 6/6 [00:03<00:00,  1.76it/s]\u001b[A\n",
      "Epoch:  67%|██████▋   | 2/3 [00:43<00:21, 21.79s/it]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tValidation Loss: 0.919186015923818\n",
      "\n",
      "\tValidation cls acc: 0.875\n",
      "\n",
      "\tValidation cls prec: 0.6875\n",
      "\n",
      "\tValidation cls rec: 0.75\n",
      "\n",
      "\tValidation cls f1: 0.7142857142857143\n",
      "\n",
      "--\n",
      "\tValidation ner acc: 0.9688355395550389\n",
      "\n",
      "\tValidation ner prec: 0.49026612171771394\n",
      "\n",
      "\tValidation ner rec: 0.49722222222222223\n",
      "\n",
      "\tValidation ner f1: 0.4936486821681747\n",
      "<====================== Epoch 3 ======================>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  8%|▊         | 1/12 [00:01<00:17,  1.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.7090, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.5453, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.2543, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 17%|█▋        | 2/12 [00:03<00:15,  1.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6096, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.3955, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.0051, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 25%|██▌       | 3/12 [00:04<00:14,  1.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.5958, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.2477, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(0.8436, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 33%|███▎      | 4/12 [00:06<00:12,  1.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.7938, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.3371, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.1309, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 42%|████▏     | 5/12 [00:07<00:11,  1.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6902, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.3228, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.0130, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 50%|█████     | 6/12 [00:09<00:09,  1.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6215, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.3893, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.0108, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 58%|█████▊    | 7/12 [00:11<00:07,  1.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6907, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.4559, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.1466, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 67%|██████▋   | 8/12 [00:12<00:06,  1.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.7285, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.3126, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.0411, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 75%|███████▌  | 9/12 [00:14<00:04,  1.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6973, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.4479, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.1452, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 83%|████████▎ | 10/12 [00:15<00:03,  1.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.6593, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.4517, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.1111, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 92%|█████████▏| 11/12 [00:17<00:01,  1.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.7119, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.3736, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(1.0855, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.58s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss_cls: tensor(0.7029, grad_fn=<NllLossBackward>)\n",
      "\tloss_ner: tensor(0.2710, grad_fn=<NllLossBackward>)\n",
      "loss: tensor(0.9739, grad_fn=<AddBackward0>)\n",
      "\n",
      "\tTraining Loss: 1.0634136249621708\n",
      "\n",
      "\tTraining cls acc: 0.7083333333333334\n",
      "\n",
      "\tTraining cls prec: 0.5063482813482814\n",
      "\n",
      "\tTraining cls rec: 0.5257613913863913\n",
      "\n",
      "\tTraining cls f1: 0.4889639713056952\n",
      "\n",
      "--\n",
      "\tTraining ner acc: 0.9545991294854549\n",
      "\n",
      "\tTraining ner prec: 0.28176025433699636\n",
      "\n",
      "\tTraining ner rec: 0.2916666666666667\n",
      "\n",
      "\tTraining ner f1: 0.2865565877035095\n",
      "\n",
      "\tCurrent Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  1.81it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 33%|███▎      | 2/6 [00:01<00:02,  1.79it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 50%|█████     | 3/6 [00:01<00:01,  1.80it/s]\u001b[A/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.78it/s]\u001b[A<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 83%|████████▎ | 5/6 [00:02<00:00,  1.79it/s]\u001b[A/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 6/6 [00:03<00:00,  1.78it/s]\u001b[A\n",
      "Epoch: 100%|██████████| 3/3 [01:05<00:00, 21.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tValidation Loss: 0.945095827182134\n",
      "\n",
      "\tValidation cls acc: 0.875\n",
      "\n",
      "\tValidation cls prec: 0.5208333333333334\n",
      "\n",
      "\tValidation cls rec: 0.5833333333333334\n",
      "\n",
      "\tValidation cls f1: 0.5492063492063493\n",
      "\n",
      "--\n",
      "\tValidation ner acc: 0.9659425461263754\n",
      "\n",
      "\tValidation ner prec: 0.3776989156576507\n",
      "\n",
      "\tValidation ner rec: 0.3861111111111111\n",
      "\n",
      "\tValidation ner f1: 0.3818128665449367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and learning rate for plotting\n",
    "learning_rate = []\n",
    "\n",
    "N_bio_tags = 5 # \"O\", \"B-C\", \"I-C\", \"B-E\", \"I-C\"\n",
    "for epoch in trange(1, epochs+1, desc='Epoch'):\n",
    "    print(\"<\" + \"=\"*22 + F\" Epoch {epoch} \"+ \"=\"*22 + \">\")\n",
    "\n",
    "    \n",
    "    ############ training eval metrics ######################\n",
    "    nb_tr_steps = 0 # Tracking variables\n",
    "    train_loss = []\n",
    "    train_cls_acc = []\n",
    "    train_cls_prec = []\n",
    "    train_cls_rec = []\n",
    "    train_cls_f1 = []\n",
    "    train_ner_acc = []\n",
    "    train_ner_prec = []\n",
    "    train_ner_rec = []\n",
    "    train_ner_f1 = []    \n",
    "    #########################################################\n",
    "    \n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        optim.zero_grad() # gradients get accumulated by default -> clear previous accumulated gradients\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        bio_tags = batch['bio_tags'].to(device)\n",
    "        \n",
    "        ################################################\n",
    "        model.train() # set model to training mode\n",
    "        logits_cls, logits_ner = model(**{\"input_ids\":input_ids, \"attention_mask\":attention_mask, \"token_type_ids\":token_type_ids}) # forward pass\n",
    "\n",
    "        ################# Loss function ############################### \n",
    "        ### CLS\n",
    "        loss_cls = loss_fn_cls(logits_cls, labels)\n",
    "        print(\"\\tloss_cls:\", loss_cls)\n",
    "        \n",
    "        ### NER\n",
    "        # similar to the class RobertaForToken classification in transformers: https://github.com/huggingface/transformers/blob/master/src/transformers/models/roberta/modeling_roberta.py\n",
    "        active_loss = attention_mask.view(-1) == 1  # either based on attention_mask (includes <CLS>, <SEP> token)\n",
    "        active_logits = logits_ner.view(-1, N_bio_tags)[active_loss] # N_bio_tags=5 \n",
    "        active_tags = bio_tags.view(-1)[active_loss]\n",
    "        loss_ner = loss_fn_ner(active_logits, active_tags)             \n",
    "        print(\"\\tloss_ner:\", loss_ner)   \n",
    "        \n",
    "        loss = loss_cls + loss_ner  # combine binary classification loss and named entity recognition loss\n",
    "        print(\"loss:\", loss)      \n",
    "        loss.backward() # backward pass\n",
    "        optim.step()    # update parameters and take a steup using the computed gradient\n",
    "        scheduler.step()# update learning rate scheduler\n",
    "        train_loss.append(loss.item())\n",
    "            \n",
    "            \n",
    "        ################## Training Performance Measures ##########\n",
    "        ### CLS\n",
    "        logits_cls = logits_cls.detach().to('cpu').numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "\n",
    "        pred_flat = np.argmax(logits_cls, axis=1).flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "        \n",
    "        metrics_cls = compute_metrics(pred_flat, labels_flat)\n",
    "        train_cls_acc.append(metrics_cls[\"accuracy\"])\n",
    "        train_cls_prec.append(metrics_cls[\"precision\"])\n",
    "        train_cls_rec.append(metrics_cls[\"recall\"])\n",
    "        train_cls_f1.append(metrics_cls[\"f1\"])\n",
    "        \n",
    "        #### NER \n",
    "        logits_ner = logits_ner.detach().to('cpu').numpy()\n",
    "        tags_ids = bio_tags.to('cpu').numpy()\n",
    "\n",
    "        # calculate performance measures only on tokens and not subwords or special tokens\n",
    "        tags_mask = tags_ids != -100 # only get token labels and not labels from subwords or special tokens\n",
    "        pred = np.argmax(logits_ner, axis=2)[tags_mask] #.flatten() # convert logits to list of predicted labels\n",
    "        tags = tags_ids[tags_mask]                      \n",
    "                \n",
    "        metrics_ner = compute_metrics(pred, tags)\n",
    "        train_ner_acc.append(metrics_ner[\"accuracy\"])\n",
    "        train_ner_prec.append(metrics_ner[\"precision\"])\n",
    "        train_ner_rec.append(metrics_ner[\"recall\"])\n",
    "        train_ner_f1.append(metrics_ner[\"f1\"])\n",
    "                          \n",
    "        nb_tr_steps += 1\n",
    "           \n",
    "    print(F'\\n\\tTraining Loss: {np.mean(train_loss)}')\n",
    "    print(F'\\n\\tTraining cls acc: {np.mean(train_cls_acc)}')\n",
    "    print(F'\\n\\tTraining cls prec: {np.mean(train_cls_prec)}')\n",
    "    print(F'\\n\\tTraining cls rec: {np.mean(train_cls_rec)}')\n",
    "    print(F'\\n\\tTraining cls f1: {np.mean(train_cls_f1)}')\n",
    "    print(F'\\n--\\n\\tTraining ner acc: {np.mean(train_ner_acc)}')\n",
    "    print(F'\\n\\tTraining ner prec: {np.mean(train_ner_prec)}')\n",
    "    print(F'\\n\\tTraining ner rec: {np.mean(train_ner_rec)}')\n",
    "    print(F'\\n\\tTraining ner f1: {np.mean(train_ner_f1)}')\n",
    "                          \n",
    "                          \n",
    "    # store the current learning rate\n",
    "    for param_group in optim.param_groups:\n",
    "        print(\"\\n\\tCurrent Learning rate: \", param_group['lr'])\n",
    "        learning_rate.append(param_group['lr'])\n",
    "    \n",
    "\n",
    "    ############# Validation ################\n",
    "    \n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    val_cls_acc = []\n",
    "    val_cls_prec = []\n",
    "    val_cls_rec = []\n",
    "    val_cls_f1 = []\n",
    "    val_ner_acc = []\n",
    "    val_ner_prec = []\n",
    "    val_ner_rec = []\n",
    "    val_ner_f1 = []\n",
    "    \n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(validation_loader):\n",
    "        batch = tuple(batch[t].to(device) for t in batch)      # batch to GPU\n",
    "        v_input_ids, v_input_mask, v_token_type_ids, v_labels, v_bio_tags = batch  # unpack inputs from dataloader\n",
    "        \n",
    "        with torch.no_grad(): # tell model not to compute or store gradients -> saves memory + speeds up validation\n",
    "            model.eval() # put model in evaluation mode for validation set\n",
    "            logits_cls, logits_ner = model(**{\"input_ids\":v_input_ids, \"attention_mask\":v_input_mask, \"token_type_ids\":v_token_type_ids}) # forward pass, calculates logit predictions\n",
    "\n",
    "        ############### LOSS Function #######################################\n",
    "        ### CLS\n",
    "        v_loss_cls = loss_fn_cls(logits_cls, v_labels)\n",
    "        \n",
    "        ### NER\n",
    "        # similar to the class RobertaForToken classification in transformers: https://github.com/huggingface/transformers/blob/master/src/transformers/models/roberta/modeling_roberta.py\n",
    "        v_active_loss = v_input_mask.view(-1) == 1  # either based on attention_mask (includes <CLS>, <SEP> token)\n",
    "        v_active_logits = logits_ner.view(-1, N_bio_tags)[v_active_loss] # 5 \n",
    "        v_active_tags = v_bio_tags.view(-1)[v_active_loss]\n",
    "        v_loss_ner = loss_fn_ner(v_active_logits, v_active_tags)             \n",
    "        v_loss = v_loss_cls + v_loss_ner\n",
    "        val_loss.append(v_loss.item())\n",
    "\n",
    "   \n",
    "        ################# PERFORMANCE MEASURES ########################################\n",
    "        ### CLS\n",
    "        logits_cls = logits_cls.detach().to('cpu').numpy()\n",
    "        label_ids = v_labels.to('cpu').numpy()\n",
    "\n",
    "        pred_flat = np.argmax(logits_cls, axis=1).flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "        \n",
    "        metrics_cls = compute_metrics(pred_flat, labels_flat)\n",
    "        val_cls_acc.append(metrics_cls[\"accuracy\"])\n",
    "        val_cls_prec.append(metrics_cls[\"precision\"])\n",
    "        val_cls_rec.append(metrics_cls[\"recall\"])\n",
    "        val_cls_f1.append(metrics_cls[\"f1\"])\n",
    "        \n",
    "        #### NER     \n",
    "        logits_ner = logits_ner.detach().to('cpu').numpy()\n",
    "        tags_ids = v_bio_tags.to('cpu').numpy()\n",
    "\n",
    "        # calculate performance measures only on tokens and not subwords or special tokens\n",
    "        tags_mask = tags_ids != -100 # only get token labels and not labels from subwords or special tokens\n",
    "        pred = np.argmax(logits_ner, axis=2)[tags_mask] #.flatten() # convert logits to list of predicted labels\n",
    "        tags = tags_ids[tags_mask]#.flatten()        \n",
    "        \n",
    "        metrics = compute_metrics(pred, tags)\n",
    "        val_ner_acc.append(metrics[\"accuracy\"])\n",
    "        val_ner_prec.append(metrics[\"precision\"])\n",
    "        val_ner_rec.append(metrics[\"recall\"])\n",
    "        val_ner_f1.append(metrics[\"f1\"])\n",
    "                              \n",
    "           \n",
    "    print(F'\\n\\tValidation Loss: {np.mean(val_loss)}')\n",
    "    print(F'\\n\\tValidation cls acc: {np.mean(val_cls_acc)}')\n",
    "    print(F'\\n\\tValidation cls prec: {np.mean(val_cls_prec)}')\n",
    "    print(F'\\n\\tValidation cls rec: {np.mean(val_cls_rec)}')\n",
    "    print(F'\\n\\tValidation cls f1: {np.mean(val_cls_f1)}')\n",
    "    print(F'\\n--\\n\\tValidation ner acc: {np.mean(val_ner_acc)}')\n",
    "    print(F'\\n\\tValidation ner prec: {np.mean(val_ner_prec)}')\n",
    "    print(F'\\n\\tValidation ner rec: {np.mean(val_ner_rec)}')\n",
    "    print(F'\\n\\tValidation ner f1: {np.mean(val_ner_f1)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e74dcb",
   "metadata": {},
   "source": [
    "### Evaluation on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec61fa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 12%|█▎        | 1/8 [00:00<00:03,  1.83it/s]<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 25%|██▌       | 2/8 [00:01<00:03,  1.88it/s]<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 38%|███▊      | 3/8 [00:01<00:02,  1.78it/s]<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 50%|█████     | 4/8 [00:02<00:02,  1.74it/s]/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 62%|██████▎   | 5/8 [00:02<00:01,  1.79it/s]<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 75%|███████▌  | 6/8 [00:03<00:01,  1.79it/s]<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 88%|████████▊ | 7/8 [00:03<00:00,  1.77it/s]<ipython-input-9-b05fabe831cb>:23: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "100%|██████████| 8/8 [00:04<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTest Loss: 0.9592961147427559\n",
      "\n",
      "\tTest cls acc: 0.890625\n",
      "\n",
      "\tTest cls prec: 0.6953125\n",
      "\n",
      "\tTest cls rec: 0.75\n",
      "\n",
      "\tTest cls f1: 0.7183150183150183\n",
      "\n",
      "--\n",
      "\tTest ner acc: 0.955632641696913\n",
      "\n",
      "\tTest ner prec: 0.44417688174184866\n",
      "\n",
      "\tTest ner rec: 0.45416666666666666\n",
      "\n",
      "\tTest ner f1: 0.44899543916206364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############ test eval metrics ######################\n",
    "test_loss = []\n",
    "test_loss = []\n",
    "test_cls_acc = []\n",
    "test_cls_prec = []\n",
    "test_cls_rec = []\n",
    "test_cls_f1 = []\n",
    "test_ner_acc = []\n",
    "test_ner_prec = []\n",
    "test_ner_rec = []\n",
    "test_ner_f1 = []\n",
    "\n",
    "########################################################\n",
    "for batch in tqdm(test_loader):\n",
    "    batch = tuple(batch[t].to(device) for t in batch)      # batch to GPU\n",
    "    t_input_ids, t_input_mask, t_token_type_ids, t_labels, t_bio_tags = batch     # unpack inputs from dataloader\n",
    "\n",
    "    with torch.no_grad(): # tell model not to compute or store gradients -> saves memory + speeds up validation\n",
    "        model.eval() # put model in evaluation mode for validation set\n",
    "        logits_cls, logits_ner = model(**{\"input_ids\":t_input_ids, \"attention_mask\":t_input_mask, \"token_type_ids\":t_token_type_ids}) # forward pass, calculates logit predictions\n",
    "\n",
    "\n",
    "    ############### LOSS Function #######################################\n",
    "    ### CLS\n",
    "    t_loss_cls = loss_fn_cls(logits_cls, t_labels)\n",
    "\n",
    "    ### NER\n",
    "    # similar to the class RobertaForToken classification in transformers: https://github.com/huggingface/transformers/blob/master/src/transformers/models/roberta/modeling_roberta.py\n",
    "    t_active_loss = t_input_mask.view(-1) == 1  # either based on attention_mask (includes <CLS>, <SEP> token)\n",
    "    t_active_logits = logits_ner.view(-1, N_bio_tags)[t_active_loss] # 5 \n",
    "    t_active_tags = t_bio_tags.view(-1)[t_active_loss]\n",
    "    t_loss_ner = loss_fn_ner(t_active_logits, t_active_tags)             \n",
    "    t_loss = t_loss_cls + t_loss_ner\n",
    "    test_loss.append(t_loss.item())\n",
    "\n",
    "\n",
    "    ################# PERFORMANCE MEASURES ########################################\n",
    "    ### CLS\n",
    "    logits_cls = logits_cls.detach().to('cpu').numpy()\n",
    "    label_ids = t_labels.to('cpu').numpy()\n",
    "\n",
    "    pred_flat = np.argmax(logits_cls, axis=1).flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "\n",
    "    metrics_cls = compute_metrics(pred_flat, labels_flat)\n",
    "    test_cls_acc.append(metrics_cls[\"accuracy\"])\n",
    "    test_cls_prec.append(metrics_cls[\"precision\"])\n",
    "    test_cls_rec.append(metrics_cls[\"recall\"])\n",
    "    test_cls_f1.append(metrics_cls[\"f1\"])\n",
    "\n",
    "    #### NER     \n",
    "    logits_ner = logits_ner.detach().to('cpu').numpy()\n",
    "    tags_ids = t_bio_tags.to('cpu').numpy()\n",
    "\n",
    "    # calculate performance measures only on tokens and not subwords or special tokens\n",
    "    tags_mask = tags_ids != -100 # only get token labels and not labels from subwords or special tokens\n",
    "    pred = np.argmax(logits_ner, axis=2)[tags_mask] #.flatten() # convert logits to list of predicted labels\n",
    "    tags = tags_ids[tags_mask]#.flatten()        \n",
    "\n",
    "    metrics = compute_metrics(pred, tags)\n",
    "    test_ner_acc.append(metrics[\"accuracy\"])\n",
    "    test_ner_prec.append(metrics[\"precision\"])\n",
    "    test_ner_rec.append(metrics[\"recall\"])\n",
    "    test_ner_f1.append(metrics[\"f1\"])\n",
    "\n",
    "\n",
    "print(F'\\n\\tTest Loss: {np.mean(test_loss)}')\n",
    "print(F'\\n\\tTest cls acc: {np.mean(test_cls_acc)}')\n",
    "print(F'\\n\\tTest cls prec: {np.mean(test_cls_prec)}')\n",
    "print(F'\\n\\tTest cls rec: {np.mean(test_cls_rec)}')\n",
    "print(F'\\n\\tTest cls f1: {np.mean(test_cls_f1)}')\n",
    "print(F'\\n--\\n\\tTest ner acc: {np.mean(test_ner_acc)}')\n",
    "print(F'\\n\\tTest ner prec: {np.mean(test_ner_prec)}')\n",
    "print(F'\\n\\tTest ner rec: {np.mean(test_ner_rec)}')\n",
    "print(F'\\n\\tTest ner f1: {np.mean(test_ner_f1)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b435946",
   "metadata": {},
   "source": [
    "### bio tags back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc0f4d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Padded Sentence:\n",
      "['<s>', 'Happens', 'more', 'often', ',', 'only', 'at', 'such', 'an', 'insane', 'crowded', 'place', 'as', 'CS', 'one', 'is', 'likely', 'to', 'hit', 'some', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "true labels:\n",
      "tensor([-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "Happens \t\ttrue: 0   pred: 0\n",
      "more \t\ttrue: 0   pred: 0\n",
      "often \t\ttrue: 0   pred: 0\n",
      ", \t\ttrue: 0   pred: 0\n",
      "only \t\ttrue: 0   pred: 0\n",
      "at \t\ttrue: 0   pred: 0\n",
      "such \t\ttrue: 0   pred: 0\n",
      "an \t\ttrue: 0   pred: 0\n",
      "insane \t\ttrue: 0   pred: 0\n",
      "crowded \t\ttrue: 0   pred: 0\n",
      "place \t\ttrue: 0   pred: 0\n",
      "as \t\ttrue: 0   pred: 0\n",
      "CS \t\ttrue: 0   pred: 0\n",
      "one \t\ttrue: 0   pred: 0\n",
      "is \t\ttrue: 0   pred: 0\n",
      "likely \t\ttrue: 0   pred: 0\n",
      "to \t\ttrue: 0   pred: 0\n",
      "hit \t\ttrue: 0   pred: 0\n",
      "some \t\ttrue: 0   pred: 0\n",
      ". \t\ttrue: 0   pred: 0\n"
     ]
    }
   ],
   "source": [
    "# take last batch of test set:\n",
    "t_input_ids, t_input_mask, t_token_type_ids, t_labels, t_bio_tags = batch \n",
    "\n",
    "for i in range(len(batch)):\n",
    "    tags_mask = t_bio_tags[i].to(\"cpu\").numpy() != -100 # only get token labels and not labels from subwords or special tokens\n",
    "    pred = np.argmax(logits_ner[i], axis=1)[tags_mask]\n",
    "    true_tags = t_bio_tags[i][tags_mask].to(\"cpu\").numpy()    \n",
    "    \n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(t_input_ids[i])\n",
    "\n",
    "    print(\"\\n\\nPadded Sentence:\")\n",
    "    print(tokens)\n",
    "    print(\"true labels:\")\n",
    "    print(t_bio_tags[i])\n",
    "    for token, true_label, pred in zip(np.array(tokens)[tags_mask], true_tags, pred):\n",
    "        print(token, \"\\t\\ttrue:\", true_label, \"  pred:\", pred)\n",
    "\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc2375",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"finetuned-NER-35-epochs.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f97047",
   "metadata": {},
   "source": [
    "### Load model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4483ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\", if torch.cuda.is_available() else \"cpu\")\n",
    "model = CausalityBERT()\n",
    "model.load_state_dict(torch.load(\"finetuned-35-epochs.pth\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7861017f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26d62ca8",
   "metadata": {},
   "source": [
    "### Small example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "262689d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 29, 768])\n",
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "output_seq, output_cls = model.bert(input_ids, attention_mask = attention_mask, token_type_ids=token_type_ids, return_dict=False) # if output 1 is our cls token\n",
    "print(output_seq.shape)\n",
    "print(output_cls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c027e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS:\n",
    "- Do we only update parameters of task-specific layer? Or the whole BERT model?\n",
    "In a multitask setting we have to update all parameters, otherwise\n",
    "they don't benefit ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
