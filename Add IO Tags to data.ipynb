{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df930ad",
   "metadata": {},
   "source": [
    "### IO-tagging\n",
    "\n",
    "A cause-effect dataset has been labeled. Transform each tweets into its IO-tagging scheme\n",
    "\n",
    "For instance the tweet: \n",
    "`Prediabetes forced me to change my lifestyle`\n",
    "with cause: `Prediabetes`\n",
    "and effect: `change my lifestyle` \n",
    "\n",
    "will be transformed into the IO-tagging scheme:\n",
    "\n",
    "```\n",
    "Prediabetes  forced  me  to  change  my  lifestyle\n",
    "   I-C         O      O   O    I-E   I-E   I-E\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b477ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import normalizeTweet, split_into_sentences, bio_tagging, EarlyStopping, manual_tagging_of_some_special_tweets\n",
    "\n",
    "\n",
    "########################### DATA FILE ###################################\n",
    "dataPath = \"data/Causality_tweets_data.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964e8fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data round 0 (tweets!):\n",
      "0.0    3710\n",
      "1.0    1290\n",
      "Name: Causal association, dtype: int64\n",
      "-------------------------\n",
      "Sentences round 1:\n",
      "0.0    1763\n",
      "1.0     429\n",
      "Name: Causal association, dtype: int64\n",
      "-------------------------\n",
      "sentences round 2:\n",
      "0    1658\n",
      "1     150\n",
      "Name: Causal association, dtype: int64\n",
      "-------------------------\n",
      "sentences round 3:\n",
      "0    1886\n",
      "1     215\n",
      "Name: Causal association, dtype: int64\n",
      "-------------------------\n",
      "sentences round 3:\n",
      "0    1886\n",
      "1     215\n",
      "Name: Causal association, dtype: int64\n",
      "\n",
      "After merge old data:\n",
      "0.0    10912\n",
      "1.0     2397\n",
      "Name: Causal association, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER Cheers ! Have one for this diabetic too !</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER Additionally the medicines are being char...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medicines are being charged at MRP</td>\n",
       "      <td>costing much higher</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text Intent  \\\n",
       "0  tonight , I learned my older girl will back he...    NaN   \n",
       "1  USER USER I knew diabetes and fibromyalgia wer...   joke   \n",
       "2  ⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...     mS   \n",
       "3     USER Cheers ! Have one for this diabetic too !     mS   \n",
       "4  USER Additionally the medicines are being char...    NaN   \n",
       "\n",
       "                                Cause               Effect  Causal association  \n",
       "0                                 NaN                  NaN                 0.0  \n",
       "1                                 NaN                  NaN                 0.0  \n",
       "2                                 NaN                  NaN                 0.0  \n",
       "3                                 NaN                  NaN                 0.0  \n",
       "4  medicines are being charged at MRP  costing much higher                 1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### DATA TO LOAD ######\n",
    "data_round0 = pd.read_excel(dataPath, sheet_name=\"round0\")\n",
    "data_round0 = data_round0[data_round0[\"Causal association\"].notnull()] # some tweets at the end are not labeled yet\n",
    "data_round0 = data_round0[[\"full_text\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\"]]\n",
    "print(\"Data round 0 (tweets!):\")\n",
    "print(data_round0[\"Causal association\"].value_counts())\n",
    "print(\"-----\"*5)\n",
    "\n",
    "\n",
    "##### additional data labeled through active learning strategy - round 1 ########\n",
    "data_round1 = pd.read_excel(dataPath, sheet_name=\"round1\")\n",
    "data_round1 = data_round1[data_round1[\"Causal association\"].notnull()]\n",
    "data_round1 = data_round1[[\"sentence\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\"]]\n",
    "data_round1.rename(columns ={\"sentence\":\"full_text\"}, inplace=True) # rename for merge\n",
    "print(\"Sentences round 1:\")\n",
    "print(data_round1[\"Causal association\"].value_counts())\n",
    "print(\"-----\"*5)\n",
    "\n",
    "##### additional data labeled through active learning strategy - round 2 ########\n",
    "data_round2 = pd.read_excel(dataPath, sheet_name=\"round2\")\n",
    "data_round2 = data_round2[data_round2[\"Causal association\"].notnull()]\n",
    "data_round2 = data_round2[[\"sentence\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\"]]\n",
    "data_round2.rename(columns ={\"sentence\":\"full_text\"}, inplace=True) # rename for merge\n",
    "print(\"sentences round 2:\")\n",
    "print(data_round2[\"Causal association\"].value_counts())\n",
    "print(\"-----\"*5)\n",
    "\n",
    "##### additional data labeled through active learning strategy - round 3 ########\n",
    "data_round3 = pd.read_excel(dataPath, sheet_name=\"round3\")\n",
    "data_round3 = data_round3[data_round3[\"Causal association\"].notnull()]\n",
    "data_round3 = data_round3[[\"sentence\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\"]]\n",
    "data_round3.rename(columns ={\"sentence\":\"full_text\"}, inplace=True) # rename for merge\n",
    "print(\"sentences round 3:\")\n",
    "print(data_round3[\"Causal association\"].value_counts())\n",
    "print(\"-----\"*5)\n",
    "\n",
    "##### additional data labeled through active learning strategy - round 4 ########\n",
    "data_round4 = pd.read_excel(dataPath, sheet_name=\"round4\")\n",
    "data_round4 = data_round4[data_round4[\"Causal association\"].notnull()]\n",
    "data_round4 = data_round4[[\"sentence\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\"]]\n",
    "data_round4.rename(columns ={\"sentence\":\"full_text\"}, inplace=True) # rename for merge\n",
    "print(\"sentences round 3:\")\n",
    "print(data_round3[\"Causal association\"].value_counts())\n",
    "\n",
    "#### merge both datasets ######\n",
    "data = data_round0.append(data_round1).append(data_round2).append(data_round3).append(data_round4)\n",
    "print(\"\\nAfter merge old data:\")\n",
    "print(data[\"Causal association\"].value_counts())\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411067b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[tonight, ,, I, learned, my, older, girl, will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[USER, USER, I, knew, diabetes, and, fibromyal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[:down_arrow:, :down_arrow:, :down_arrow:, THI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER Cheers ! Have one for this diabetic too !</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[USER, Cheers, !, Have, one, for, this, diabet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER Additionally the medicines are being char...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medicines are being charged at MRP</td>\n",
       "      <td>costing much higher</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[USER, Additionally, the, medicines, are, bein...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text Intent  \\\n",
       "0  tonight , I learned my older girl will back he...    NaN   \n",
       "1  USER USER I knew diabetes and fibromyalgia wer...   joke   \n",
       "2  ⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...     mS   \n",
       "3     USER Cheers ! Have one for this diabetic too !     mS   \n",
       "4  USER Additionally the medicines are being char...    NaN   \n",
       "\n",
       "                                Cause               Effect  \\\n",
       "0                                 NaN                  NaN   \n",
       "1                                 NaN                  NaN   \n",
       "2                                 NaN                  NaN   \n",
       "3                                 NaN                  NaN   \n",
       "4  medicines are being charged at MRP  costing much higher   \n",
       "\n",
       "   Causal association                                          tokenized  \n",
       "0                 0.0  [tonight, ,, I, learned, my, older, girl, will...  \n",
       "1                 0.0  [USER, USER, I, knew, diabetes, and, fibromyal...  \n",
       "2                 0.0  [:down_arrow:, :down_arrow:, :down_arrow:, THI...  \n",
       "3                 0.0  [USER, Cheers, !, Have, one, for, this, diabet...  \n",
       "4                 1.0  [USER, Additionally, the, medicines, are, bein...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tokenized\"] = data[\"full_text\"].map(lambda tweet: normalizeTweet(tweet).split(\" \"))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2580bae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>io_tags</th>\n",
       "      <th>bio_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[tonight, ,, I, learned, my, older, girl, will...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[USER, USER, I, knew, diabetes, and, fibromyal...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[:down_arrow:, :down_arrow:, :down_arrow:, THI...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER Cheers ! Have one for this diabetic too !</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[USER, Cheers, !, Have, one, for, this, diabet...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER Additionally the medicines are being char...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medicines are being charged at MRP</td>\n",
       "      <td>costing much higher</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[USER, Additionally, the, medicines, are, bein...</td>\n",
       "      <td>[O, O, O, I-C, I-C, I-C, I-C, I-C, I-C, O, O, ...</td>\n",
       "      <td>[O, O, O, I-C, I-C, I-C, I-C, I-C, I-C, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text Intent  \\\n",
       "0  tonight , I learned my older girl will back he...    NaN   \n",
       "1  USER USER I knew diabetes and fibromyalgia wer...   joke   \n",
       "2  ⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...     mS   \n",
       "3     USER Cheers ! Have one for this diabetic too !     mS   \n",
       "4  USER Additionally the medicines are being char...    NaN   \n",
       "\n",
       "                                Cause               Effect  \\\n",
       "0                                 NaN                  NaN   \n",
       "1                                 NaN                  NaN   \n",
       "2                                 NaN                  NaN   \n",
       "3                                 NaN                  NaN   \n",
       "4  medicines are being charged at MRP  costing much higher   \n",
       "\n",
       "   Causal association                                          tokenized  \\\n",
       "0                 0.0  [tonight, ,, I, learned, my, older, girl, will...   \n",
       "1                 0.0  [USER, USER, I, knew, diabetes, and, fibromyal...   \n",
       "2                 0.0  [:down_arrow:, :down_arrow:, :down_arrow:, THI...   \n",
       "3                 0.0  [USER, Cheers, !, Have, one, for, this, diabet...   \n",
       "4                 1.0  [USER, Additionally, the, medicines, are, bein...   \n",
       "\n",
       "                                             io_tags  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                     [O, O, O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, I-C, I-C, I-C, I-C, I-C, I-C, O, O, ...   \n",
       "\n",
       "                                            bio_tags  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3                     [O, O, O, O, O, O, O, O, O, O]  \n",
       "4  [O, O, O, I-C, I-C, I-C, I-C, I-C, I-C, O, O, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def io_tagging(tweet, causes, effects):\n",
    "    \"\"\"\n",
    "    Each token gets associated to one of the following labels:\n",
    "    I-C : Inside cause\n",
    "    I-C : Inside effect\n",
    "    O   : Outside\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = normalizeTweet(tweet).split(\" \")\n",
    "    causes = str(causes).strip().split(\";\")\n",
    "    effects = str(effects).strip().split(\";\")\n",
    "    io = [\"O\"] * len(tokens)\n",
    "\n",
    "    # if no cause and no effect return IO tags of \"O\"\n",
    "    if (not causes and not effects) or (causes == [\"nan\"] and effects == [\"nan\"]):\n",
    "        return io\n",
    "\n",
    "    # if only cause and no effect\n",
    "    if (causes or causes != [\"nan\"]) and not (effects or effects !=[\"nan\"]):\n",
    "        print(\"ERROR: only cause and no effect exists\\n\\n -------------\\n\\n\")\n",
    "\n",
    "    # if only effect and no cause\n",
    "    if not (causes or causes != [\"nan\"]) and  (effects or effects !=[\"nan\"]):\n",
    "        print(\"ERROR: only effect and no cause exists\\n\\n -------------\\n\\n\")\n",
    "\n",
    "\n",
    "    ########### SPECIAL CASES (Some causes or effects may occur several times in a tweet. Check here, if the current tweet is such a tweet and return the manually pre-labeled bio tag) ############\n",
    "    manual_io_tag = manual_tagging_of_some_special_tweets(tweet, tokens, io, ioTagging=True)\n",
    "    # usually 'io' has only \"O\", except if it got altered in manual_tagging_of_some_special_tweets\n",
    "    if manual_io_tag.count(\"O\") != len(manual_io_tag):\n",
    "        return manual_io_tag\n",
    "\n",
    "\n",
    "    ################## Add IO tags for causes and effects ########################\n",
    "\n",
    "    for cause in causes: # possible to have several causes\n",
    "\n",
    "        cause_words = normalizeTweet(cause).split(\" \") # a cause may consist of several words\n",
    "        cause_words_start = cause_words[0]\n",
    "        try:\n",
    "            ### Find index of first word of cause -> label with \"I-C\"\n",
    "            indices = [i for i, x in enumerate(tokens) if x == cause_words_start] # get all indices of the first word of the cause\n",
    "            N_cause_words = len(cause_words)\n",
    "            if len(indices) > 1 and N_cause_words > 1: # if several occurrences of the same cause start word in phrase\n",
    "                for cause_word_start_index in indices:\n",
    "                    causeIndexFound = all([tokens[cause_word_start_index+word_i] == cause_words[word_i] for word_i in range(N_cause_words)])\n",
    "                    if causeIndexFound:\n",
    "                        ind = cause_word_start_index\n",
    "                        break\n",
    "            else:\n",
    "                ind = tokens.index(cause_words_start) # get index of causal word in tokens list\n",
    "            io[ind] = \"I-C\"\n",
    "\n",
    "            ### If cause consists of several words -> label those words with \"I-C\"\n",
    "            i = 1\n",
    "            while i < len(cause_words):\n",
    "                if tokens[ind+i] == cause_words[i]:\n",
    "                    io[ind+i] = \"I-C\"\n",
    "                else:\n",
    "                    print(\"Error: token and causal word don't match!\\Tind:\", ind, \"i:\", i, \"token[ind+i]:\", tokens[ind+i], \"cause_words[i]:\", cause_words[i])\n",
    "                i += 1\n",
    "        except ValueError:\n",
    "            print(\"\\nINFO: cause word '{}' does not exist in sentence: \\n'{}', but should be in other sentence of the tweet\".format(cause_words_start, tokens))\n",
    "\n",
    "\n",
    "    for effect in effects: # possible to have several effects\n",
    "\n",
    "        effect_words = normalizeTweet(effect).split(\" \") # a effect may consist of several words\n",
    "        effect_words_start = effect_words[0]\n",
    "        try:\n",
    "            ### Find index of first word of effect -> label with \"I-E\"\n",
    "            indices = [i for i, x in enumerate(tokens) if x == effect_words_start]\n",
    "            N_effect_words = len(effect_words)\n",
    "            if len(indices) > 1 and N_effect_words > 1: # if several occurrences of the same cause start word in phrase\n",
    "                for effect_word_start_index in indices:\n",
    "                    effectIndexFound = all([tokens[effect_word_start_index+word_i] == effect_words[word_i] for word_i in range(N_effect_words)])\n",
    "                    if effectIndexFound:\n",
    "                        ind = effect_word_start_index\n",
    "                        break\n",
    "            else:\n",
    "                ind = tokens.index(effect_words_start) # get index of c_word in tokens list\n",
    "            io[ind] = \"I-E\"\n",
    "\n",
    "            ### If effect consists of several words -> label those words with \"I-E\"\n",
    "            i = 1\n",
    "            while i < len(effect_words):\n",
    "                if tokens[ind+i] == effect_words[i]:\n",
    "                    io[ind+i] = \"I-E\"\n",
    "                else:\n",
    "                    print(\"Error: token and effect word don't match! \\tind:\", ind, \"i:\", i, \"token[ind+i]\", tokens[ind+i], \"effect_words[i]\", cause_words[i])\n",
    "                i += 1\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"\\nError: effect word '{}' does not exist in sentence: \\n'{}', but should be in other sentence of the tweet\".format(effect_words_start, tokens))\n",
    "\n",
    "    return io\n",
    "\n",
    "\n",
    "data[\"bio_tags\"] = data.apply(lambda row: io_tagging(row[\"full_text\"],row[\"Cause\"], row[\"Effect\"]), axis=1)\n",
    "\n",
    "#for i, row in data[data[\"Causal association\"] == 1].sample(n=100).iterrows():\n",
    "#    io_tags = io_tagging(row[\"full_text\"],row[\"Cause\"], row[\"Effect\"])\n",
    "#    print(\"\\n\\nCause:\", row[\"Cause\"])\n",
    "#    print(\"Effect:\", row[\"Effect\"])\n",
    "#    for tok, io in zip(row[\"tokenized\"], io_tags):\n",
    "#        print(tok, \"\\t\", io)\n",
    "data.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fb33589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_index_of_sentence_in_tweet(tweet, sentence):\n",
    "    \"\"\" \n",
    "    The sentence tokens are included in the tweet tokens.\n",
    "    Return the start end end indices of the sentence tokens in the tweet tokens\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_start_word = sentence[0]\n",
    "    start_indices = [i for i, x in enumerate(tweet) if x == sentence_start_word] # find all indices of the start word of the sentence \n",
    "    try:\n",
    "        for start_index in start_indices:\n",
    "            isTrueStartIndex = all([tweet[start_index+i] == sentence[i] for i in range(len(sentence))])\n",
    "            #print(\"start_index:\", start_index, \"isTrueStartIndex:\", isTrueStartIndex)\n",
    "            if isTrueStartIndex:\n",
    "                return start_index, start_index + len(sentence) \n",
    "    except:\n",
    "        print(\"ERROR: StartIndex should have been found for sentence:\")\n",
    "        print(\"tweet:\")\n",
    "        print(tweet)\n",
    "        print(\"sentence:\")\n",
    "        print(sentence)\n",
    "    return -1, -2 # should not be returned\n",
    "\n",
    "\n",
    "def split_tweets_to_sentences(data):\n",
    "    \"\"\" \n",
    "        Splits tweets into sentences and associates the appropriate intent, causes, effects and causal association\n",
    "        to each sentence.\n",
    "        \n",
    "        Parameters:\n",
    "        - min_words_in_sentences: Minimal number of words in a sentence such that the sentence is kept. \n",
    "                                  Assumption: A sentence with too few words does not have enough information\n",
    "                              \n",
    "                              \n",
    "                              \n",
    "        Ex.:\n",
    "        full_text                              | Intent | Cause | Effect | Causal association | ...\n",
    "        --------------------------------------------------------------------------------------------\n",
    "        what? type 1 causes insulin dependence | q;msS  | type 1|insulin dependence | 1       | ...  \n",
    "        \n",
    "        New dataframe returned: \n",
    "        full_text                              | Intent | Cause | Effect | Causal association | ...\n",
    "        --------------------------------------------------------------------------------------------\n",
    "        what?                                  |   q    |       |        |       0            | ...\n",
    "        type 1 causes insulin dependence       |        | type 1| insulin dependence | 1       | ...  \n",
    "    \"\"\"\n",
    "\n",
    "    newDF = pd.DataFrame(columns=[\"sentence\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\", \"tokenized\", \"bio_tags\"])\n",
    "    \n",
    "    for i,row in data.iterrows():\n",
    "        causes = row[\"Cause\"]\n",
    "        effects = row[\"Effect\"]\n",
    "        sentences = split_into_sentences(normalizeTweet(row[\"full_text\"]))\n",
    "        \n",
    "        # single sentence in tweet\n",
    "        if len(sentences) == 1:\n",
    "            singleSentenceIntent = \"\"\n",
    "            if isinstance(row[\"Intent\"], str):\n",
    "                if len(row[\"Intent\"].split(\";\")) > 1:\n",
    "                    singleSentenceIntent = row[\"Intent\"].strip().replace(\";msS\", \"\").replace(\"msS;\", \"\").replace(\";mS\", \"\").replace(\"mS;\", \"\")\n",
    "                else:\n",
    "                    if row[\"Intent\"] == \"mS\" or row[\"Intent\"] == \"msS\":\n",
    "                        singleSentenceIntent = \"\"\n",
    "                    else:\n",
    "                        singleSentenceIntent = row[\"Intent\"].strip()\n",
    "                    \n",
    "            newDF=newDF.append(pd.Series({\"sentence\": sentences[0] # only one sentence\n",
    "                         , \"Intent\": singleSentenceIntent\n",
    "                         , \"Cause\" : row[\"Cause\"]\n",
    "                         , \"Effect\": row[\"Effect\"]\n",
    "                         , \"Causal association\" : row[\"Causal association\"]\n",
    "                         , \"tokenized\": row[\"tokenized\"]\n",
    "                         , \"bio_tags\": row[\"bio_tags\"]}), ignore_index=True)\n",
    "        \n",
    "        # tweet has several sentences\n",
    "        else: \n",
    "            intents = str(row[\"Intent\"]).strip().split(\";\")\n",
    "            for sentence in sentences:\n",
    "                sent_tokenized = sentence.split(\" \")\n",
    "                causeInSentence = np.nan if not isinstance(causes, str) or not any([cause in sentence for cause in causes.split(\";\")]) else \";\".join([cause for cause in causes.split(\";\") if cause in sentence])\n",
    "                effectInSentence = np.nan if not isinstance(effects, str) or not any([effect in sentence for effect in effects.split(\";\")]) else \";\".join([effect for effect in effects.split(\";\") if effect in sentence])\n",
    "                causalAssociationInSentence = 1 if isinstance(causeInSentence, str) and isinstance(effectInSentence, str) else 0\n",
    "                startIndex, endIndex = get_start_end_index_of_sentence_in_tweet(row[\"tokenized\"], sent_tokenized)\n",
    "                sentence_tokenized = row[\"tokenized\"][startIndex:endIndex]\n",
    "                sentence_bio_tags = row[\"bio_tags\"][startIndex:endIndex]\n",
    "                \n",
    "                if \"q\" in intents and sentence[-1] == \"?\": # if current sentence is question\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"q\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)                    \n",
    "                elif \"joke\" in intents: # all sentences with \"joke\" in tweet keep the intent \"joke\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"joke\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)   \n",
    "                elif \"neg\" in intents: # all sentences with \"neg\" in tweet keep intent \"neg\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"neg\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)               \n",
    "                elif isinstance(causeInSentence, str) and isinstance(effectInSentence, str): # cause effect sentence\n",
    "                    causalIntent = \"\"\n",
    "                    if len(causeInSentence.split(\";\")) > 1:\n",
    "                        causalIntent = \"mC\"\n",
    "                        if len(effectInSentence.split(\";\")) > 1:\n",
    "                            causalIntent = \"mC;mE\"\n",
    "                    elif len(effectInSentence.split(\";\")) > 1:\n",
    "                        causalIntent = \"mE\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": causalIntent, \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)                                  \n",
    "                else:\n",
    "                    nonCausalIntent = \"\"\n",
    "                    if isinstance(causeInSentence, str): # only cause is given\n",
    "                        if len(causeInSentence.split(\";\")) > 1:\n",
    "                            nonCausalIntent = \"mC\"\n",
    "                    elif isinstance(effectInSentence, str): # only effect is given\n",
    "                        if len(effectInSentence.split(\";\")) > 1:\n",
    "                            nonCausalIntent = \"mE\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": nonCausalIntent, \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)\n",
    "\n",
    "    return newDF\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6530916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tweets: 13309\n",
      "N sentences: 20065\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>bio_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[tonight, ,, I, learned, my, older, girl, will...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fiercely .</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[Fiercely, .]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#impressive #bigsister #type1 #type1times2</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[#impressive, #bigsister, #type1, #type1times2]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[USER, USER, I, knew, diabetes, and, fibromyal...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>:face_with_rolling_eyes:</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[:face_with_rolling_eyes:]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence Intent Cause Effect  \\\n",
       "0  tonight , I learned my older girl will back he...          NaN    NaN   \n",
       "1                                         Fiercely .          NaN    NaN   \n",
       "2         #impressive #bigsister #type1 #type1times2          NaN    NaN   \n",
       "3  USER USER I knew diabetes and fibromyalgia wer...   joke   NaN    NaN   \n",
       "4                           :face_with_rolling_eyes:   joke   NaN    NaN   \n",
       "\n",
       "  Causal association                                          tokenized  \\\n",
       "0                  0  [tonight, ,, I, learned, my, older, girl, will...   \n",
       "1                  0                                      [Fiercely, .]   \n",
       "2                  0    [#impressive, #bigsister, #type1, #type1times2]   \n",
       "3                  0  [USER, USER, I, knew, diabetes, and, fibromyal...   \n",
       "4                  0                         [:face_with_rolling_eyes:]   \n",
       "\n",
       "                                           bio_tags  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1                                            [O, O]  \n",
       "2                                      [O, O, O, O]  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4                                               [O]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Split tweets into sentences (train classifier on sentence level) ####\n",
    "\n",
    "print(\"N tweets:\", data.shape[0])\n",
    "dataSentences = split_tweets_to_sentences(data)\n",
    "print(\"N sentences:\", dataSentences.shape[0])\n",
    "dataSentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff034a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N sentences before filtering:  20065\n",
      "N sentences after filtering:  16868\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>bio_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[tonight, ,, I, learned, my, older, girl, will...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#impressive #bigsister #type1 #type1times2</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[#impressive, #bigsister, #type1, #type1times2]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>:down_arrow: :down_arrow: :down_arrow: THIS :d...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[:down_arrow:, :down_arrow:, :down_arrow:, THI...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I 'm a trans woman .</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, 'm, a, trans, woman, .]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Both of us could use a world where \" brave and...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[Both, of, us, could, use, a, world, where, \",...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence Intent Cause Effect  \\\n",
       "0  tonight , I learned my older girl will back he...          NaN    NaN   \n",
       "2         #impressive #bigsister #type1 #type1times2          NaN    NaN   \n",
       "5  :down_arrow: :down_arrow: :down_arrow: THIS :d...          NaN    NaN   \n",
       "6                               I 'm a trans woman .          NaN    NaN   \n",
       "7  Both of us could use a world where \" brave and...          NaN    NaN   \n",
       "\n",
       "  Causal association                                          tokenized  \\\n",
       "0                  0  [tonight, ,, I, learned, my, older, girl, will...   \n",
       "2                  0    [#impressive, #bigsister, #type1, #type1times2]   \n",
       "5                  0  [:down_arrow:, :down_arrow:, :down_arrow:, THI...   \n",
       "6                  0                        [I, 'm, a, trans, woman, .]   \n",
       "7                  0  [Both, of, us, could, use, a, world, where, \",...   \n",
       "\n",
       "                                            bio_tags  \n",
       "0   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "2                                       [O, O, O, O]  \n",
       "5         [O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "6                                 [O, O, O, O, O, O]  \n",
       "7  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## Remove sentences with joke, question, negation and keep only sentences with more than 3 tokens #####\n",
    "\n",
    "print(\"N sentences before filtering: \", dataSentences.shape[0])\n",
    "dataSentFiltered = dataSentences[~dataSentences[\"Intent\"].str.contains(\"neg|joke|q\")] # remove sentences with joke, q, neg\n",
    "dataSentFiltered = dataSentFiltered[dataSentFiltered[\"tokenized\"].map(len) >= 3] # only keep sentences with at least 3 words\n",
    "print(\"N sentences after filtering: \", dataSentFiltered.shape[0])\n",
    "dataSentFiltered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3853c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose sentences with cause or effect\n",
    "trainingData = dataSentFiltered[(dataSentFiltered[\"Cause\"].notnull()) | (dataSentFiltered[\"Effect\"].notnull())]\n",
    "\n",
    "trainingData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbb66da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.to_csv(\"/home/adrian/Downloads/cause_effect_sentences_with_IO_tags.csv\",index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77138256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
