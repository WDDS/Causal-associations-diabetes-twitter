{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e908c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 5434\n",
      "Labeled count: 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER Cheers ! Have one for this diabetic too !</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER Additionally the medicines are being char...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medicines are being charged at MRP</td>\n",
       "      <td>costing much higher</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text Intent  \\\n",
       "0  tonight , I learned my older girl will back he...    NaN   \n",
       "1  USER USER I knew diabetes and fibromyalgia wer...   joke   \n",
       "2  ⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...     mS   \n",
       "3     USER Cheers ! Have one for this diabetic too !     mS   \n",
       "4  USER Additionally the medicines are being char...    NaN   \n",
       "\n",
       "                                Cause               Effect  Causal association  \n",
       "0                                 NaN                  NaN                 0.0  \n",
       "1                                 NaN                  NaN                 0.0  \n",
       "2                                 NaN                  NaN                 0.0  \n",
       "3                                 NaN                  NaN                 0.0  \n",
       "4  medicines are being charged at MRP  costing much higher                 1.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from tqdm import tqdm, trange\n",
    "from utils import normalizeTweet, split_into_sentences, bio_tagging, create_training_data\n",
    "\n",
    "\n",
    "\n",
    "#data = pd.read_excel(\"/home/adrian/workspace/causality/Causal-associations-diabetes-twitter/data/Causality + hypoglycemia.xlsx\", sheet_name=\">5000_samples_\")\n",
    "data = pd.read_excel(\"/home/adrian/Downloads/Causality + hypoglycemia.xlsx\", sheet_name=\">5000_samples_\")\n",
    "print(\"Total count:\", data.shape[0])\n",
    "data = data[data[\"Causal association\"].notnull()]\n",
    "data = data[[\"full_text\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\"]]\n",
    "print(\"Labeled count:\", data.shape[0])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c39f416",
   "metadata": {},
   "source": [
    "## Add BIO tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f56d0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>bio_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[tonight, ,, I, learned, my, older, girl, will...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[USER, USER, I, knew, diabetes, and, fibromyal...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[:down_arrow:, :down_arrow:, :down_arrow:, THI...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER Cheers ! Have one for this diabetic too !</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[USER, Cheers, !, Have, one, for, this, diabet...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER Additionally the medicines are being char...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medicines are being charged at MRP</td>\n",
       "      <td>costing much higher</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[USER, Additionally, the, medicines, are, bein...</td>\n",
       "      <td>[O, O, O, B-C, I-C, I-C, I-C, I-C, I-C, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>USER USER We have those days Esp . if it inter...</td>\n",
       "      <td>msS</td>\n",
       "      <td>diabetic</td>\n",
       "      <td>hate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[USER, USER, We, have, those, days, Esp, ., if...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Why all of a sudden are people hungry and vuln...</td>\n",
       "      <td>q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Why, all, of, a, sudden, are, people, hungry,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i got lime for my glucose test , wasn't that b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>glucose test</td>\n",
       "      <td>nauseous</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[i, got, lime, for, my, glucose, test, ,, was,...</td>\n",
       "      <td>[O, O, O, O, O, B-C, I-C, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This stickur of Unkel Funny iz ware i am shave...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[This, stickur, of, Unkel, Funny, iz, ware, i,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>For the second time in my life I gave myself i...</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[For, the, second, time, in, my, life, I, gave...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>USER USER USER Yeah ! To hell with diabetic Gr...</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[USER, USER, USER, Yeah, !, To, hell, with, di...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The fact that an insurance company can just de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[The, fact, that, an, insurance, company, can,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ughh its so difficult on my own :/ Of you look...</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Ughh, its, so, difficult, on, my, own, :/, Of...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sounds like Willow's blood sugar level is real...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blood sugar level is real low</td>\n",
       "      <td>reduce her insulin shots</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Sounds, like, Willow, 's, blood, sugar, level...</td>\n",
       "      <td>[O, O, O, O, B-C, I-C, I-C, I-C, I-C, I-C, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Da fu I WISH my insulin cost $ 450 a month . B...</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Da, fu, I, WISH, my, insulin, cost, $, 450, a...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>USER How much total insulin you are talking ?</td>\n",
       "      <td>q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[USER, How, much, total, insulin, you, are, ta...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>USER I've always found it too sweet mustvsay ,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dreaded diabetes</td>\n",
       "      <td>sauces are used sparingly</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[USER, I, 've, always, found, it, too, sweet, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-E, I-E,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I have some amazing friendships b / c of my ch...</td>\n",
       "      <td>msS</td>\n",
       "      <td>child's #T1D</td>\n",
       "      <td>friendships</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[I, have, some, amazing, friendships, b, /, c,...</td>\n",
       "      <td>[O, O, O, O, B-E, O, O, O, O, O, B-C, I-C, I-C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>USER USER USER USER No he didn't . My fucking ...</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[USER, USER, USER, USER, No, he, did, n't, ., ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Dear USER What's happened to your ' No added s...</td>\n",
       "      <td>q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Dear, USER, What, 's, happened, to, your, ', ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            full_text Intent  \\\n",
       "0   tonight , I learned my older girl will back he...    NaN   \n",
       "1   USER USER I knew diabetes and fibromyalgia wer...   joke   \n",
       "2   ⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...     mS   \n",
       "3      USER Cheers ! Have one for this diabetic too !     mS   \n",
       "4   USER Additionally the medicines are being char...    NaN   \n",
       "5   USER USER We have those days Esp . if it inter...    msS   \n",
       "6   Why all of a sudden are people hungry and vuln...      q   \n",
       "7   i got lime for my glucose test , wasn't that b...    NaN   \n",
       "8   This stickur of Unkel Funny iz ware i am shave...    NaN   \n",
       "9   For the second time in my life I gave myself i...     mS   \n",
       "10  USER USER USER Yeah ! To hell with diabetic Gr...   joke   \n",
       "11  The fact that an insurance company can just de...    NaN   \n",
       "12  Ughh its so difficult on my own :/ Of you look...     mS   \n",
       "13  Sounds like Willow's blood sugar level is real...    NaN   \n",
       "14  Da fu I WISH my insulin cost $ 450 a month . B...     mS   \n",
       "15      USER How much total insulin you are talking ?      q   \n",
       "16  USER I've always found it too sweet mustvsay ,...    NaN   \n",
       "17  I have some amazing friendships b / c of my ch...    msS   \n",
       "18  USER USER USER USER No he didn't . My fucking ...     mS   \n",
       "19  Dear USER What's happened to your ' No added s...      q   \n",
       "\n",
       "                                 Cause                     Effect  \\\n",
       "0                                  NaN                        NaN   \n",
       "1                                  NaN                        NaN   \n",
       "2                                  NaN                        NaN   \n",
       "3                                  NaN                        NaN   \n",
       "4   medicines are being charged at MRP        costing much higher   \n",
       "5                             diabetic                       hate   \n",
       "6                                  NaN                        NaN   \n",
       "7                         glucose test                   nauseous   \n",
       "8                                  NaN                        NaN   \n",
       "9                                  NaN                        NaN   \n",
       "10                                 NaN                        NaN   \n",
       "11                                 NaN                        NaN   \n",
       "12                                 NaN                        NaN   \n",
       "13       blood sugar level is real low   reduce her insulin shots   \n",
       "14                                 NaN                        NaN   \n",
       "15                                 NaN                        NaN   \n",
       "16                    dreaded diabetes  sauces are used sparingly   \n",
       "17                        child's #T1D                friendships   \n",
       "18                                 NaN                        NaN   \n",
       "19                                 NaN                        NaN   \n",
       "\n",
       "    Causal association                                          tokenized  \\\n",
       "0                  0.0  [tonight, ,, I, learned, my, older, girl, will...   \n",
       "1                  0.0  [USER, USER, I, knew, diabetes, and, fibromyal...   \n",
       "2                  0.0  [:down_arrow:, :down_arrow:, :down_arrow:, THI...   \n",
       "3                  0.0  [USER, Cheers, !, Have, one, for, this, diabet...   \n",
       "4                  1.0  [USER, Additionally, the, medicines, are, bein...   \n",
       "5                  1.0  [USER, USER, We, have, those, days, Esp, ., if...   \n",
       "6                  0.0  [Why, all, of, a, sudden, are, people, hungry,...   \n",
       "7                  1.0  [i, got, lime, for, my, glucose, test, ,, was,...   \n",
       "8                  0.0  [This, stickur, of, Unkel, Funny, iz, ware, i,...   \n",
       "9                  0.0  [For, the, second, time, in, my, life, I, gave...   \n",
       "10                 0.0  [USER, USER, USER, Yeah, !, To, hell, with, di...   \n",
       "11                 0.0  [The, fact, that, an, insurance, company, can,...   \n",
       "12                 0.0  [Ughh, its, so, difficult, on, my, own, :/, Of...   \n",
       "13                 1.0  [Sounds, like, Willow, 's, blood, sugar, level...   \n",
       "14                 0.0  [Da, fu, I, WISH, my, insulin, cost, $, 450, a...   \n",
       "15                 0.0  [USER, How, much, total, insulin, you, are, ta...   \n",
       "16                 1.0  [USER, I, 've, always, found, it, too, sweet, ...   \n",
       "17                 1.0  [I, have, some, amazing, friendships, b, /, c,...   \n",
       "18                 0.0  [USER, USER, USER, USER, No, he, did, n't, ., ...   \n",
       "19                 0.0  [Dear, USER, What, 's, happened, to, your, ', ...   \n",
       "\n",
       "                                             bio_tags  \n",
       "0   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3                      [O, O, O, O, O, O, O, O, O, O]  \n",
       "4   [O, O, O, B-C, I-C, I-C, I-C, I-C, I-C, O, O, ...  \n",
       "5   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "6   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "7   [O, O, O, O, O, B-C, I-C, O, O, O, O, O, O, O,...  \n",
       "8   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "9   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "10   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "11  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "12  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "13  [O, O, O, O, B-C, I-C, I-C, I-C, I-C, I-C, O, ...  \n",
       "14  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "15                        [O, O, O, O, O, O, O, O, O]  \n",
       "16  [O, O, O, O, O, O, O, O, O, O, O, O, B-E, I-E,...  \n",
       "17  [O, O, O, O, B-E, O, O, O, O, O, B-C, I-C, I-C...  \n",
       "18  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "19  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tokenized\"] = data[\"full_text\"].map(lambda tweet: normalizeTweet(tweet).split(\" \"))\n",
    "data[\"bio_tags\"] = data.apply(lambda row: bio_tagging(row[\"full_text\"],row[\"Cause\"], row[\"Effect\"]), axis=1)\n",
    "data.head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7683e2",
   "metadata": {},
   "source": [
    "## Split all tweets into sentences => new dataframe with more rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf52df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tweets: 5000\n",
      "N sentences: 11784\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>bio_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[tonight, ,, I, learned, my, older, girl, will...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fiercely .</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[Fiercely, .]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#impressive #bigsister #type1 #type1times2</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[#impressive, #bigsister, #type1, #type1times2]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[USER, USER, I, knew, diabetes, and, fibromyal...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>:face_with_rolling_eyes:</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[:face_with_rolling_eyes:]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence Intent Cause Effect  \\\n",
       "0  tonight , I learned my older girl will back he...          NaN    NaN   \n",
       "1                                         Fiercely .          NaN    NaN   \n",
       "2         #impressive #bigsister #type1 #type1times2          NaN    NaN   \n",
       "3  USER USER I knew diabetes and fibromyalgia wer...   joke   NaN    NaN   \n",
       "4                           :face_with_rolling_eyes:   joke   NaN    NaN   \n",
       "\n",
       "  Causal association                                          tokenized  \\\n",
       "0                  0  [tonight, ,, I, learned, my, older, girl, will...   \n",
       "1                  0                                      [Fiercely, .]   \n",
       "2                  0    [#impressive, #bigsister, #type1, #type1times2]   \n",
       "3                  0  [USER, USER, I, knew, diabetes, and, fibromyal...   \n",
       "4                  0                         [:face_with_rolling_eyes:]   \n",
       "\n",
       "                                           bio_tags  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1                                            [O, O]  \n",
       "2                                      [O, O, O, O]  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4                                               [O]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_start_end_index_of_sentence_in_tweet(tweet, sentence):\n",
    "    \"\"\" \n",
    "    The sentence tokens are included in the tweet tokens.\n",
    "    Return the start end end indices of the sentence tokens in the tweet tokens\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_start_word = sentence[0]\n",
    "    start_indices = [i for i, x in enumerate(tweet) if x == sentence_start_word] # find all indices of the start word of the sentence \n",
    "    try:\n",
    "        for start_index in start_indices:\n",
    "            isTrueStartIndex = all([tweet[start_index+i] == sentence[i] for i in range(len(sentence))])\n",
    "            #print(\"start_index:\", start_index, \"isTrueStartIndex:\", isTrueStartIndex)\n",
    "            if isTrueStartIndex:\n",
    "                return start_index, start_index + len(sentence) \n",
    "    except:\n",
    "        print(\"ERROR: StartIndex should have been found for sentence:\")\n",
    "        print(\"tweet:\")\n",
    "        print(tweet)\n",
    "        print(\"sentence:\")\n",
    "        print(sentence)\n",
    "    return -1, -2 # should not be returned\n",
    "\n",
    "\n",
    "def split_tweets_to_sentences(data):\n",
    "    \"\"\" \n",
    "        Splits tweets into sentences and associates the appropriate intent, causes, effects and causal association\n",
    "        to each sentence.\n",
    "        \n",
    "        Parameters:\n",
    "        - min_words_in_sentences: Minimal number of words in a sentence such that the sentence is kept. \n",
    "                                  Assumption: A sentence with too few words does not have enough information\n",
    "                              \n",
    "                              \n",
    "                              \n",
    "        Ex.:\n",
    "        full_text                              | Intent | Cause | Effect | Causal association | ...\n",
    "        --------------------------------------------------------------------------------------------\n",
    "        what? type 1 causes insulin dependence | q;msS  | type 1|insulin dependence | 1       | ...  \n",
    "        \n",
    "        New dataframe returned: \n",
    "        full_text                              | Intent | Cause | Effect | Causal association | ...\n",
    "        --------------------------------------------------------------------------------------------\n",
    "        what?                                  |   q    |       |        |       0            | ...\n",
    "        type 1 causes insulin dependence       |        | type 1| insulin dependence | 1       | ...  \n",
    "    \"\"\"\n",
    "\n",
    "    newDF = pd.DataFrame(columns=[\"sentence\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\", \"tokenized\", \"bio_tags\"])\n",
    "    \n",
    "    for i,row in data.iterrows():\n",
    "        causes = row[\"Cause\"]\n",
    "        effects = row[\"Effect\"]\n",
    "        sentences = split_into_sentences(normalizeTweet(row[\"full_text\"]))\n",
    "\n",
    "        # single sentence in tweet\n",
    "        if len(sentences) == 1:\n",
    "            singleSentenceIntent = \"\"\n",
    "            if isinstance(row[\"Intent\"], str):\n",
    "                if len(row[\"Intent\"].split(\";\")) > 1:\n",
    "                    singleSentenceIntent = row[\"Intent\"].strip().replace(\";msS\", \"\").replace(\"msS;\", \"\").replace(\";mS\", \"\").replace(\"mS;\", \"\")\n",
    "                else:\n",
    "                    if row[\"Intent\"] == \"mS\" or row[\"Intent\"] == \"msS\":\n",
    "                        singleSentenceIntent = \"\"\n",
    "                    else:\n",
    "                        singleSentenceIntent = row[\"Intent\"].strip()\n",
    "                    \n",
    "            newDF=newDF.append(pd.Series({\"sentence\": sentences[0] # only one sentence\n",
    "                         , \"Intent\": singleSentenceIntent\n",
    "                         , \"Cause\" : row[\"Cause\"]\n",
    "                         , \"Effect\": row[\"Effect\"]\n",
    "                         , \"Causal association\" : row[\"Causal association\"]\n",
    "                         , \"tokenized\": row[\"tokenized\"]\n",
    "                         , \"bio_tags\": row[\"bio_tags\"]}), ignore_index=True)\n",
    "        \n",
    "        # tweet has several sentences\n",
    "        else: \n",
    "            intents = str(row[\"Intent\"]).strip().split(\";\")\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                sent_tokenized = sentence.split(\" \")\n",
    "                \n",
    "                causeInSentence = np.nan if not isinstance(causes, str) or not any([cause in sentence for cause in causes.split(\";\")]) else \";\".join([cause for cause in causes.split(\";\") if cause in sentence])\n",
    "                effectInSentence = np.nan if not isinstance(effects, str) or not any([effect in sentence for effect in effects.split(\";\")]) else \";\".join([effect for effect in effects.split(\";\") if effect in sentence])\n",
    "                causalAssociationInSentence = 1 if isinstance(causeInSentence, str) and isinstance(effectInSentence, str) else 0\n",
    "                \n",
    "                startIndex, endIndex = get_start_end_index_of_sentence_in_tweet(row[\"tokenized\"], sent_tokenized)\n",
    "                sentence_tokenized = row[\"tokenized\"][startIndex:endIndex]\n",
    "                sentence_bio_tags = row[\"bio_tags\"][startIndex:endIndex]\n",
    "                \n",
    "                if \"q\" in intents and sentence[-1] == \"?\": # if current sentence is question\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"q\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)                    \n",
    "                elif \"joke\" in intents: # all sentences with \"joke\" in tweet keep the intent \"joke\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"joke\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)   \n",
    "                elif \"neg\" in intents: # all sentences with \"neg\" in tweet keep intent \"neg\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"neg\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)               \n",
    "                elif isinstance(causeInSentence, str) and isinstance(effectInSentence, str): # cause effect sentence\n",
    "                    causalIntent = \"\"\n",
    "                    if len(causeInSentence.split(\";\")) > 1:\n",
    "                        causalIntent = \"mC\"\n",
    "                        if len(effectInSentence.split(\";\")) > 1:\n",
    "                            causalIntent = \"mC;mE\"\n",
    "                    elif len(effectInSentence.split(\";\")) > 1:\n",
    "                        causalIntent = \"mE\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": causalIntent, \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)                                  \n",
    "                else:\n",
    "                    nonCausalIntent = \"\"\n",
    "                    if isinstance(causeInSentence, str): # only cause is given\n",
    "                        if len(causeInSentence.split(\";\")) > 1:\n",
    "                            nonCausalIntent = \"mC\"\n",
    "                    elif isinstance(effectInSentence, str): # only effect is given\n",
    "                        if len(effectInSentence.split(\";\")) > 1:\n",
    "                            nonCausalIntent = \"mE\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": nonCausalIntent, \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized, \"bio_tags\": sentence_bio_tags}), ignore_index=True)\n",
    "\n",
    "    return newDF\n",
    "       \n",
    "# sample: has one example for each possible \"Intent\" value\n",
    "#allIntents = data[\"Intent\"].value_counts().keys().tolist()\n",
    "#sample = data[data[\"Intent\"] == \"mS\"][0:1]\n",
    "#for intent in allIntents:\n",
    "#    sample = sample.append(data[data[\"Intent\"] == intent][1:2])\n",
    "#print(sample.shape)\n",
    "\n",
    "#i = 19\n",
    "#test = sample[i:i+1]\n",
    "#dataSentences = split_tweets_to_sentences(test)\n",
    "#dataSentences.head(30)\n",
    "#test.head()\n",
    "\n",
    "print(\"N tweets:\", data.shape[0])\n",
    "dataSentences = split_tweets_to_sentences(data)\n",
    "print(\"N sentences:\", dataSentences.shape[0])\n",
    "dataSentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ea962",
   "metadata": {},
   "source": [
    "### Filter out negation, jokes, questions and sentences with a minimal token length of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9eff383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N sentences before filtering:  11784\n",
      "N sentences after filtering:  8835\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>bio_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[tonight, ,, I, learned, my, older, girl, will...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#impressive #bigsister #type1 #type1times2</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[#impressive, #bigsister, #type1, #type1times2]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>:down_arrow: :down_arrow: :down_arrow: THIS :d...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[:down_arrow:, :down_arrow:, :down_arrow:, THI...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I 'm a trans woman .</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, 'm, a, trans, woman, .]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Both of us could use a world where \" brave and...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[Both, of, us, could, use, a, world, where, \",...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence Intent Cause Effect  \\\n",
       "0  tonight , I learned my older girl will back he...          NaN    NaN   \n",
       "2         #impressive #bigsister #type1 #type1times2          NaN    NaN   \n",
       "5  :down_arrow: :down_arrow: :down_arrow: THIS :d...          NaN    NaN   \n",
       "6                               I 'm a trans woman .          NaN    NaN   \n",
       "7  Both of us could use a world where \" brave and...          NaN    NaN   \n",
       "\n",
       "  Causal association                                          tokenized  \\\n",
       "0                  0  [tonight, ,, I, learned, my, older, girl, will...   \n",
       "2                  0    [#impressive, #bigsister, #type1, #type1times2]   \n",
       "5                  0  [:down_arrow:, :down_arrow:, :down_arrow:, THI...   \n",
       "6                  0                        [I, 'm, a, trans, woman, .]   \n",
       "7                  0  [Both, of, us, could, use, a, world, where, \",...   \n",
       "\n",
       "                                            bio_tags  \n",
       "0   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "2                                       [O, O, O, O]  \n",
       "5         [O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "6                                 [O, O, O, O, O, O]  \n",
       "7  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"N sentences before filtering: \", dataSentences.shape[0])\n",
    "dataSentFiltered = dataSentences[~dataSentences[\"Intent\"].str.contains(\"neg|joke|q\")] # remove sentences with joke, q, neg\n",
    "dataSentFiltered = dataSentFiltered[dataSentFiltered[\"tokenized\"].map(len) >= 3] # only keep sentences with at least 3 words\n",
    "print(\"N sentences after filtering: \", dataSentFiltered.shape[0])\n",
    "dataSentFiltered.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1df624f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         8705\n",
       "mE         72\n",
       "mC         47\n",
       "mC;mE      10\n",
       "mE;mC       1\n",
       "Name: Intent, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSentFiltered[\"Intent\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606db122",
   "metadata": {},
   "source": [
    "### Only work on cause-effect tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb21b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    7799\n",
       "1.0    1036\n",
       "Name: Causal association, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSentFiltered[\"Causal association\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdb2626a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1036, 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only take sentences with cause and effect\n",
    "trainingData = dataSentFiltered[dataSentFiltered[\"Causal association\"] == 1]\n",
    "trainingData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ea773",
   "metadata": {},
   "source": [
    "### Create training, validation, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31d3e1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (128, 7)\n",
      "Validate: (32, 7)\n",
      "Test: (40, 7)\n"
     ]
    }
   ],
   "source": [
    "trainingDataSample = trainingData#.sample(n=200)   # VIVEK: DELETE TAKING SAMPLE. THIS WAS ONLY FOR TESTING\n",
    "train = trainingDataSample.sample(frac=0.8, random_state=0)\n",
    "test = trainingDataSample.drop(train.index)\n",
    "validate = train.sample(frac=0.2, random_state=0)\n",
    "train = train.drop(validate.index)\n",
    "print(\"Train:\", train.shape)\n",
    "print(\"Validate:\", validate.shape)\n",
    "print(\"Test:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9547ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:18: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:19: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:18: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:19: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<ipython-input-46-0d77030b8cba>:18: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(ids[idx]) == len(bio_tags_extended), \"token ids and BIO tags lengths do not match!\")\n",
      "<ipython-input-46-0d77030b8cba>:19: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(ids[idx]) == len(bio_tags_extended), \"token ids and BIO tags lengths do not match!\")\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "32\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transform labels + encodings into Pytorch DataSet object (including __len__, __getitem__)\n",
    "class TweetDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, labels, bio_tags, tokenizer):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bio_tags = bio_tags\n",
    "        self.tag2id = {label: idx for idx, label in enumerate([\"O\", \"B-C\", \"I-C\", \"B-E\", \"I-E\"])}\n",
    "        self.tag2id[-100] = -100\n",
    "        self.id2tag = {id:tag for tag,id in self.tag2id.items()}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.text, padding=True, truncation=True, return_token_type_ids=True)\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        bio_tags_extended = self.extend_tags(self.text[idx], self.bio_tags[idx], ids[idx])\n",
    "        assert(len(ids[idx]) == len(bio_tags_extended), \"token ids and BIO tags lengths do not match!\")\n",
    "        return {\n",
    "                \"input_ids\" : torch.tensor(ids[idx], dtype=torch.long)\n",
    "              , \"attention_mask\" : torch.tensor(mask[idx], dtype=torch.long)\n",
    "              , \"token_type_ids\" : torch.tensor(token_type_ids[idx], dtype=torch.long)\n",
    "              , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "              , \"bio_tags\" : torch.tensor(list(map(lambda bioTags: self.tag2id[bioTags], bio_tags_extended))\n",
    ", dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    \n",
    "    def extend_tags(self, tokens_old, tags_old, ids_tokenized_padded):\n",
    "        \"\"\" \n",
    "            Each token has a BIO tag label. \n",
    "            However BERT's tokenization splits tokens into subwords. How to label those subwords?\n",
    "            \n",
    "            Option 1:\n",
    "            ---------\n",
    "            \n",
    "            add the same label to each subword than the first subword. Only replace \"B\" by \"I\"\n",
    "            Ex. \n",
    "            #lowbloodsugar => '#low@@', 'blood@@', 'sugar@@'\n",
    "               \"B-C\"       =>   \"B-C\" ,   \"I-C\"  ,   \"I-C\"\n",
    "            \n",
    "            Option 2 (implemented):      \n",
    "            ---------\n",
    "            \n",
    "            From : https://huggingface.co/transformers/custom_datasets.html#token-classification-with-w-nut-emerging-entities\n",
    "            A common obstacle with using pre-trained models for token-level classification: many of the tokens in\n",
    "            the W-NUT corpus are not in DistilBert’s vocabulary. Bert and many models like it use a method called \n",
    "            WordPiece Tokenization, meaning that single words are split into multiple tokens such that each token\n",
    "            is likely to be in the vocabulary. For example, DistilBert’s tokenizer would split the Twitter \n",
    "            handle @huggingface into the tokens ['@', 'hugging', '##face']. This is a problem for us because we \n",
    "            have exactly one tag per token. If the tokenizer splits a token into multiple sub-tokens, then we will\n",
    "            end up with a mismatch between our tokens and our labels.\n",
    "\n",
    "            One way to handle this is to only train on the tag labels for the first subtoken of a split token. \n",
    "            We can do this in 🤗 Transformers by setting the labels we wish to ignore to -100. \n",
    "            In the example above, if the label for @HuggingFace is 3 (indexing B-corporation), we would set \n",
    "            the labels of ['@', 'hugging', '##face'] to [3, -100, -100].\n",
    "        \"\"\"\n",
    "        tags = [-100] # add for start token <CLS>\n",
    "        for token_old, tag in zip(tokens_old.split(\" \"), tags_old):\n",
    "#            print(F\"\\ntoken_old: {token_old};    tag: {tag}\")\n",
    "            for i, sub_token in enumerate(self.tokenizer.tokenize(token_old)):\n",
    "                if (i == 0):\n",
    "                    tags.append(tag)\n",
    "                else: \n",
    "                    tags.append(-100)\n",
    "           \n",
    "        tags.append(-100) # 0 for end of sentence token\n",
    "    \n",
    "        # append -100 for all padded elements\n",
    "        padded_elements = ids_tokenized_padded.count(1) # id 1 is <PAD> ; Alternative: where attention_mask == 0 add -100\n",
    "        tags.extend([-100]*padded_elements)\n",
    "        \n",
    "        return tags\n",
    "        \n",
    "        \n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "train_dataset = TweetDataSet(train[\"sentence\"].values.tolist()\n",
    "                           , train[\"Causal association\"].values.tolist()\n",
    "                           , train[\"bio_tags\"].values.tolist()\n",
    "                           , tokenizer)\n",
    "val_dataset = TweetDataSet(validate[\"sentence\"].values.tolist()\n",
    "                           , validate[\"Causal association\"].values.tolist()\n",
    "                           , validate[\"bio_tags\"].values.tolist()\n",
    "                           , tokenizer)\n",
    "test_dataset = TweetDataSet(test[\"sentence\"].values.tolist()\n",
    "                           , test[\"Causal association\"].values.tolist()\n",
    "                           , test[\"bio_tags\"].values.tolist()\n",
    "                           , tokenizer)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))\n",
    "\n",
    "# put data to batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76fc4c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred, labels):\n",
    "    \"\"\"\n",
    "        Dataset is unbalanced -> measure weighted metrics\n",
    "        Calculate metrics for each label, and find their average wieghted by support (Number of true instances for each label)\n",
    "        This alters 'macro' to account for label imbalance;\n",
    "        it can result in an F-Score taht is not between precision and recall\n",
    "    \"\"\"\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='macro') # TODO: check weightin\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "class CausalNER(torch.nn.Module):\n",
    "    \"\"\" Model Bert\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CausalNER, self).__init__()\n",
    "        self.num_labels = 5 # B-C, I-C, B-E, I-E, O\n",
    "        self.bert = transformers.BertModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear1 = torch.nn.Linear(768, 256)\n",
    "        self.linear2 = torch.nn.Linear(256, self.num_labels)\n",
    "        self.softmax = torch.nn.Softmax(-1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "#        _, output_1 = self.bert(input_ids, attention_mask = attention_mask, token_type_ids=token_type_ids, return_dict=False) # if output 1 is our cls token\n",
    "        output_seq, _ = self.bert(input_ids, attention_mask = attention_mask, token_type_ids=token_type_ids, return_dict=False) # if output 1 is our cls token\n",
    "        output_2 = self.dropout(output_seq)\n",
    "        output_3 = self.linear1(output_2)\n",
    "        output_4 = self.dropout(output_3)\n",
    "        output_5 = self.linear2(output_4)\n",
    "        return output_5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e17161",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9d9303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize_train = 16\n",
    "lr = 1e-3\n",
    "adam_eps = 1e-8\n",
    "epochs = 3\n",
    "num_warmup_steps = 0\n",
    "num_training_steps = len(train_loader)*epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "edc924b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing BertModel: ['lm_head.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'lm_head.decoder.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.5.output.dense.bias', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.embeddings.word_embeddings.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.dense.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.0.attention.self.value.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['encoder.layer.10.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'pooler.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = CausalNER()\n",
    "model.to(device)\n",
    "\n",
    "# fine-tune only the task-specific parameters -> Vivek? \n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=lr, eps=adam_eps)\n",
    "# scheduler with a linearly decreasing learning rate from the initial lr set in the optimizer to 0;\n",
    "# after a warmup period during which it increases linearly from to the initial lr set in the optimizer\n",
    "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps) \n",
    "\n",
    "loss_fn = CrossEntropyLoss(ignore_index=-100) # ignore subwords/tokens with label -100 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d3218c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce7a5ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 12%|█▎        | 1/8 [00:01<00:10,  1.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.8383, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 25%|██▌       | 2/8 [00:03<00:09,  1.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9779, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 38%|███▊      | 3/8 [00:04<00:07,  1.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(1.1006, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 50%|█████     | 4/8 [00:06<00:06,  1.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(1.1350, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 62%|██████▎   | 5/8 [00:07<00:04,  1.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9612, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 75%|███████▌  | 6/8 [00:09<00:03,  1.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9111, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 88%|████████▊ | 7/8 [00:10<00:01,  1.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(1.1876, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 8/8 [00:12<00:00,  1.54s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.7778, grad_fn=<NllLossBackward>)\n",
      "\n",
      "\tTraining Loss: 0.9861996546387672\n",
      "\n",
      "\tTraining acc: 0.8106001263088918\n",
      "\n",
      "\tTraining prec: 0.16212002526177835\n",
      "\n",
      "\tTraining rec: 0.2\n",
      "\n",
      "\tTraining f1: 0.17902499396597074\n",
      "\n",
      "\tCurrent Learning rate:  0.000625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 25%|██▌       | 1/4 [00:00<00:02,  1.36it/s]\u001b[A<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 50%|█████     | 2/4 [00:01<00:01,  1.49it/s]\u001b[A<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 75%|███████▌  | 3/4 [00:02<00:00,  1.52it/s]\u001b[A<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.52it/s]\u001b[A\n",
      "Epoch:  33%|███▎      | 1/3 [00:15<00:30, 15.01s/it]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tValidation Loss: 0.7920175939798355\n",
      "\n",
      "\tValidation acc: 0.7953049207707834\n",
      "\n",
      "\tValidation prec: 0.15906098415415668\n",
      "\n",
      "\tValidation rec: 0.2\n",
      "\n",
      "\tValidation f1: 0.17697670027160403\n",
      "<====================== Epoch 2 ======================>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 12%|█▎        | 1/8 [00:01<00:10,  1.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.7760, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 25%|██▌       | 2/8 [00:03<00:09,  1.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.7718, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 38%|███▊      | 3/8 [00:04<00:07,  1.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.7309, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 50%|█████     | 4/8 [00:06<00:06,  1.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.8183, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 62%|██████▎   | 5/8 [00:07<00:04,  1.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.7252, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 75%|███████▌  | 6/8 [00:09<00:03,  1.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.8045, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 88%|████████▊ | 7/8 [00:10<00:01,  1.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.8654, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 8/8 [00:12<00:00,  1.53s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.8671, grad_fn=<NllLossBackward>)\n",
      "\n",
      "\tTraining Loss: 0.7948975712060928\n",
      "\n",
      "\tTraining acc: 0.8109252852908607\n",
      "\n",
      "\tTraining prec: 0.3002201303231583\n",
      "\n",
      "\tTraining rec: 0.2206228476804964\n",
      "\n",
      "\tTraining f1: 0.2151295626815598\n",
      "\n",
      "\tCurrent Learning rate:  0.0002916666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  1.57it/s]\u001b[A<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 50%|█████     | 2/4 [00:01<00:01,  1.57it/s]\u001b[A<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  1.62it/s]\u001b[A<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.62it/s]\u001b[A\n",
      "Epoch:  67%|██████▋   | 2/3 [00:29<00:14, 14.86s/it]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tValidation Loss: 0.7585615515708923\n",
      "\n",
      "\tValidation acc: 0.7948491849013122\n",
      "\n",
      "\tValidation prec: 0.1589698369802624\n",
      "\n",
      "\tValidation rec: 0.2\n",
      "\n",
      "\tValidation f1: 0.17701825090093454\n",
      "<====================== Epoch 3 ======================>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 12%|█▎        | 1/8 [00:01<00:10,  1.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.8501, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 25%|██▌       | 2/8 [00:03<00:09,  1.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.8858, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 38%|███▊      | 3/8 [00:04<00:07,  1.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6826, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 50%|█████     | 4/8 [00:06<00:06,  1.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.5518, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 62%|██████▎   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.8981, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 75%|███████▌  | 6/8 [00:09<00:03,  1.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.5713, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 88%|████████▊ | 7/8 [00:10<00:01,  1.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6077, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 8/8 [00:12<00:00,  1.53s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6349, grad_fn=<NllLossBackward>)\n",
      "\n",
      "\tTraining Loss: 0.7102793455123901\n",
      "\n",
      "\tTraining acc: 0.8101320199562618\n",
      "\n",
      "\tTraining prec: 0.2662504280606497\n",
      "\n",
      "\tTraining rec: 0.210345840549647\n",
      "\n",
      "\tTraining f1: 0.19761690470600646\n",
      "\n",
      "\tCurrent Learning rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  1.56it/s]\u001b[A<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 50%|█████     | 2/4 [00:01<00:01,  1.54it/s]\u001b[A<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  1.51it/s]\u001b[A<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.54it/s]\u001b[A\n",
      "Epoch: 100%|██████████| 3/3 [00:44<00:00, 14.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tValidation Loss: 0.7914430946111679\n",
      "\n",
      "\tValidation acc: 0.7942995787643284\n",
      "\n",
      "\tValidation prec: 0.1588599157528657\n",
      "\n",
      "\tValidation rec: 0.2\n",
      "\n",
      "\tValidation f1: 0.17694625245016293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and learning rate for plotting\n",
    "learning_rate = []\n",
    "\n",
    "N_bio_tags = 5 # \"O\", \"B-C\", \"I-C\", \"B-E\", \"I-C\"\n",
    "for epoch in trange(1, epochs+1, desc='Epoch'):\n",
    "    print(\"<\" + \"=\"*22 + F\" Epoch {epoch} \"+ \"=\"*22 + \">\")\n",
    "\n",
    "    \n",
    "    ############ training eval metrics ######################\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    train_prec = []\n",
    "    train_rec = []\n",
    "    train_f1 = []\n",
    "    \n",
    "    #########################################################\n",
    "    \n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        optim.zero_grad() # gradients get accumulated by default -> clear previous accumulated gradients\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        bio_tags = batch['bio_tags'].to(device)\n",
    "        \n",
    "        ################################################\n",
    "        model.train() # set model to training mode\n",
    "        logits = model(**{\"input_ids\":input_ids, \"attention_mask\":attention_mask, \"token_type_ids\":token_type_ids}) # forward pass\n",
    "\n",
    "        ################################################ \n",
    "        # similar to the class RobertaForToken classification in transformers: https://github.com/huggingface/transformers/blob/master/src/transformers/models/roberta/modeling_roberta.py\n",
    "        active_loss = attention_mask.view(-1) == 1  # either based on attention_mask (includes <CLS>, <SEP> token)\n",
    "        active_logits = logits.view(-1, N_bio_tags)[active_loss] # N_bio_tags=5 \n",
    "        active_tags = bio_tags.view(-1)[active_loss]\n",
    "        loss = loss_fn(active_logits, active_tags)             \n",
    "        print(\"loss:\", loss)       ## TODO VIVEK: check loss function calculation\n",
    "        loss.backward() # backward pass\n",
    "        optim.step()    # update parameters and take a steup using the computed gradient\n",
    "        scheduler.step()# update learning rate scheduler\n",
    "        train_loss.append(loss.item())\n",
    "            \n",
    "            \n",
    "        ################## Training Performance Measures ##########\n",
    "        logits = logits.detach().to('cpu').numpy()\n",
    "        tags_ids = bio_tags.to('cpu').numpy()\n",
    "\n",
    "        # calculate performance measures only on tokens and not subwords or special tokens\n",
    "        tags_mask = tags_ids != -100 # only get token labels and not labels from subwords or special tokens\n",
    "        pred = np.argmax(logits, axis=2)[tags_mask] #.flatten() # convert logits to list of predicted labels\n",
    "        tags = tags_ids[tags_mask]                      \n",
    "                \n",
    "        metrics = compute_metrics(pred, tags)\n",
    "        train_acc.append(metrics[\"accuracy\"])\n",
    "        train_prec.append(metrics[\"precision\"])\n",
    "        train_rec.append(metrics[\"recall\"])\n",
    "        train_f1.append(metrics[\"f1\"])\n",
    "                          \n",
    "           \n",
    "    print(F'\\n\\tTraining Loss: {np.mean(train_loss)}')\n",
    "    print(F'\\n\\tTraining acc: {np.mean(train_acc)}')\n",
    "    print(F'\\n\\tTraining prec: {np.mean(train_prec)}')\n",
    "    print(F'\\n\\tTraining rec: {np.mean(train_rec)}')\n",
    "    print(F'\\n\\tTraining f1: {np.mean(train_f1)}')\n",
    "                          \n",
    "                          \n",
    "    # store the current learning rate\n",
    "    for param_group in optim.param_groups:\n",
    "        print(\"\\n\\tCurrent Learning rate: \", param_group['lr'])\n",
    "        learning_rate.append(param_group['lr'])\n",
    "    \n",
    "\n",
    "    ############# Validation ################\n",
    "    \n",
    "    nb_eval_steps = 0 # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_f1 = []\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(validation_loader):\n",
    "        batch = tuple(batch[t].to(device) for t in batch)      # batch to GPU\n",
    "        v_input_ids, v_input_mask, v_token_type_ids, v_labels, v_bio_tags = batch  # unpack inputs from dataloader\n",
    "        \n",
    "        with torch.no_grad(): # tell model not to compute or store gradients -> saves memory + speeds up validation\n",
    "            model.eval() # put model in evaluation mode for validation set\n",
    "            logits = model(**{\"input_ids\":v_input_ids, \"attention_mask\":v_input_mask, \"token_type_ids\":v_token_type_ids}) # forward pass, calculates logit predictions\n",
    "\n",
    "        ######################################################\n",
    "        \n",
    "        # similar to the class RobertaForToken classification in transformers: https://github.com/huggingface/transformers/blob/master/src/transformers/models/roberta/modeling_roberta.py\n",
    "        v_active_loss = v_input_mask.view(-1) == 1  # either based on attention_mask (includes <CLS>, <SEP> token)\n",
    "        v_active_logits = logits.view(-1, N_bio_tags)[v_active_loss] # 5 \n",
    "        v_active_tags = v_bio_tags.view(-1)[v_active_loss]\n",
    "        v_loss = loss_fn(v_active_logits, v_active_tags)             \n",
    "        val_loss.append(v_loss.item())\n",
    "              \n",
    "        #########################################################\n",
    "        logits = logits.detach().to('cpu').numpy()\n",
    "        tags_ids = v_bio_tags.to('cpu').numpy()\n",
    "\n",
    "        # calculate performance measures only on tokens and not subwords or special tokens\n",
    "        tags_mask = tags_ids != -100 # only get token labels and not labels from subwords or special tokens\n",
    "        pred = np.argmax(logits, axis=2)[tags_mask] #.flatten() # convert logits to list of predicted labels\n",
    "        tags = tags_ids[tags_mask]#.flatten()        \n",
    "        \n",
    "        metrics = compute_metrics(pred, tags)\n",
    "        val_acc.append(metrics[\"accuracy\"])\n",
    "        val_prec.append(metrics[\"precision\"])\n",
    "        val_rec.append(metrics[\"recall\"])\n",
    "        val_f1.append(metrics[\"f1\"])\n",
    "                              \n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    print(F'\\n\\tValidation Loss: {np.mean(val_loss)}')\n",
    "    print(F'\\n\\tValidation acc: {np.mean(val_acc)}')\n",
    "    print(F'\\n\\tValidation prec: {np.mean(val_prec)}')\n",
    "    print(F'\\n\\tValidation rec: {np.mean(val_rec)}')\n",
    "    print(F'\\n\\tValidation f1: {np.mean(val_f1)}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e74dcb",
   "metadata": {},
   "source": [
    "### Evaluation on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec61fa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.61it/s]<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 40%|████      | 2/5 [00:01<00:01,  1.54it/s]<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.45it/s]<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.46it/s]<ipython-input-46-0d77030b8cba>:24: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTest Loss: 0.7850914478302002\n",
      "\n",
      "\tTest acc: 0.7923388343776085\n",
      "\n",
      "\tTest prec: 0.15846776687552172\n",
      "\n",
      "\tTest rec: 0.2\n",
      "\n",
      "\tTest f1: 0.17676806023077402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############ test eval metrics ######################\n",
    "nb_test_steps = 0 # Tracking variables\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "test_prec = []\n",
    "test_rec = []\n",
    "test_f1 = []\n",
    "\n",
    "########################################################\n",
    "for batch in tqdm(test_loader):\n",
    "    batch = tuple(batch[t].to(device) for t in batch)      # batch to GPU\n",
    "    t_input_ids, t_input_mask, t_token_type_ids, t_labels, t_bio_tags = batch     # unpack inputs from dataloader\n",
    "\n",
    "    with torch.no_grad(): # tell model not to compute or store gradients -> saves memory + speeds up validation\n",
    "        model.eval() # put model in evaluation mode for validation set\n",
    "        logits = model(**{\"input_ids\":t_input_ids, \"attention_mask\":t_input_mask, \"token_type_ids\":t_token_type_ids}) # forward pass, calculates logit predictions\n",
    "\n",
    "    ######################################################\n",
    "\n",
    "    # similar to the class RobertaForToken classification in transformers: https://github.com/huggingface/transformers/blob/master/src/transformers/models/roberta/modeling_roberta.py\n",
    "    t_active_loss = t_input_mask.view(-1) == 1  # either based on attention_mask (includes <CLS>, <SEP> token)\n",
    "    t_active_logits = logits.view(-1, N_bio_tags)[t_active_loss] # 5 \n",
    "    t_active_tags = t_bio_tags.view(-1)[t_active_loss]\n",
    "    t_loss = loss_fn(t_active_logits, t_active_tags)             \n",
    "    test_loss.append(t_loss.item())\n",
    "\n",
    "    #########################################################\n",
    "    logits = logits.detach().to('cpu').numpy()\n",
    "    tags_ids = t_bio_tags.to('cpu').numpy()\n",
    "\n",
    "    # calculate performance measures only on tokens and not subwords or special tokens\n",
    "    tags_mask = tags_ids != -100 # only get token labels and not labels from subwords or special tokens\n",
    "    pred = np.argmax(logits, axis=2)[tags_mask] #.flatten() # convert logits to list of predicted labels\n",
    "    tags = tags_ids[tags_mask]#.flatten()                          \n",
    "\n",
    "    metrics = compute_metrics(pred, tags)\n",
    "    test_acc.append(metrics[\"accuracy\"])\n",
    "    test_prec.append(metrics[\"precision\"])\n",
    "    test_rec.append(metrics[\"recall\"])\n",
    "    test_f1.append(metrics[\"f1\"])\n",
    "\n",
    "    nb_test_steps += 1\n",
    "\n",
    "print(F'\\n\\tTest Loss: {np.mean(test_loss)}')\n",
    "print(F'\\n\\tTest acc: {np.mean(test_acc)}')\n",
    "print(F'\\n\\tTest prec: {np.mean(test_prec)}')\n",
    "print(F'\\n\\tTest rec: {np.mean(test_rec)}')\n",
    "print(F'\\n\\tTest f1: {np.mean(test_f1)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f7fdf",
   "metadata": {},
   "source": [
    "### bio tags back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bc0f4d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Padded Sentence:\n",
      "['<s>', 'Diabetes', 'is', 'the', 'worst', 'fucking', 'thing', 'to', 'have', 'to', 'deal', 'with', 'when', 'you', \"'re\", 'trying', 'to', 'perform', 'well', 'in', 'anything', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "true labels:\n",
      "tensor([-100,    1,    0,    0,    3,    4,    4,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100])\n",
      "Diabetes \t\ttrue: 1   pred: 0\n",
      "is \t\ttrue: 0   pred: 0\n",
      "the \t\ttrue: 0   pred: 0\n",
      "worst \t\ttrue: 3   pred: 0\n",
      "fucking \t\ttrue: 4   pred: 0\n",
      "thing \t\ttrue: 4   pred: 0\n",
      "to \t\ttrue: 0   pred: 0\n",
      "have \t\ttrue: 0   pred: 0\n",
      "to \t\ttrue: 0   pred: 0\n",
      "deal \t\ttrue: 0   pred: 0\n",
      "with \t\ttrue: 0   pred: 0\n",
      "when \t\ttrue: 0   pred: 0\n",
      "you \t\ttrue: 0   pred: 0\n",
      "'re \t\ttrue: 0   pred: 0\n",
      "trying \t\ttrue: 0   pred: 0\n",
      "to \t\ttrue: 0   pred: 0\n",
      "perform \t\ttrue: 0   pred: 0\n",
      "well \t\ttrue: 0   pred: 0\n",
      "in \t\ttrue: 0   pred: 0\n",
      "anything \t\ttrue: 0   pred: 0\n",
      ". \t\ttrue: 0   pred: 0\n"
     ]
    }
   ],
   "source": [
    "# take last batch of test set:\n",
    "t_input_ids, t_input_mask, t_token_type_ids, t_labels, t_bio_tags = batch \n",
    "\n",
    "for i in range(len(batch)):\n",
    "    tags_mask = t_bio_tags[i].to(\"cpu\").numpy() != -100 # only get token labels and not labels from subwords or special tokens\n",
    "    pred = np.argmax(logits[i], axis=1)[tags_mask]\n",
    "    true_tags = t_bio_tags[i][tags_mask].to(\"cpu\").numpy()    \n",
    "    \n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(t_input_ids[i])\n",
    "\n",
    "    print(\"\\n\\nPadded Sentence:\")\n",
    "    print(tokens)\n",
    "    print(\"true labels:\")\n",
    "    print(t_bio_tags[i])\n",
    "    for token, true_label, pred in zip(np.array(tokens)[tags_mask], true_tags, pred):\n",
    "        print(token, \"\\t\\ttrue:\", true_label, \"  pred:\", pred)\n",
    "\n",
    "    \n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed75fdc9",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b92eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"finetuned-NER-35-epochs.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6819275d",
   "metadata": {},
   "source": [
    "### Load model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e8ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\", if torch.cuda.is_available() else \"cpu\")\n",
    "model = CausalityBERT()\n",
    "model.load_state_dict(torch.load(\"finetuned-35-epochs.pth\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7861017f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
