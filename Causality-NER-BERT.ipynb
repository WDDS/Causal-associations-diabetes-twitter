{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e908c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 5456\n",
      "Labeled count: 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>Charline association0=no;1=yes</th>\n",
       "      <th>Remarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>908171203029868545</td>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1203645589214367745</td>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1310596731063525376</td>\n",
       "      <td>⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...</td>\n",
       "      <td>⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1125198453167022085</td>\n",
       "      <td>USER Cheers ! Have one for this diabetic too !</td>\n",
       "      <td>USER Cheers ! Have one for this diabetic too !</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1248600944138268673</td>\n",
       "      <td>USER Additionally the medicines are being char...</td>\n",
       "      <td>USER Additionally the medicines are being char...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medicines are being charged at MRP</td>\n",
       "      <td>costing much higher</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0   908171203029868545  tonight , I learned my older girl will back he...   \n",
       "1  1203645589214367745  USER USER I knew diabetes and fibromyalgia wer...   \n",
       "2  1310596731063525376  ⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...   \n",
       "3  1125198453167022085     USER Cheers ! Have one for this diabetic too !   \n",
       "4  1248600944138268673  USER Additionally the medicines are being char...   \n",
       "\n",
       "                                           full_text Intent  \\\n",
       "0  tonight , I learned my older girl will back he...    NaN   \n",
       "1  USER USER I knew diabetes and fibromyalgia wer...   joke   \n",
       "2  ⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...     mS   \n",
       "3     USER Cheers ! Have one for this diabetic too !     mS   \n",
       "4  USER Additionally the medicines are being char...    NaN   \n",
       "\n",
       "                                Cause               Effect  \\\n",
       "0                                 NaN                  NaN   \n",
       "1                                 NaN                  NaN   \n",
       "2                                 NaN                  NaN   \n",
       "3                                 NaN                  NaN   \n",
       "4  medicines are being charged at MRP  costing much higher   \n",
       "\n",
       "   Causal association  Charline association0=no;1=yes Remarks  \n",
       "0                 0.0                             NaN     NaN  \n",
       "1                 0.0                             NaN     NaN  \n",
       "2                 0.0                             NaN     NaN  \n",
       "3                 0.0                             NaN     NaN  \n",
       "4                 1.0                             NaN     NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from tqdm import tqdm, trange\n",
    "from utils import normalizeTweet, split_into_sentences, bio_tagging, create_training_data\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_excel(\"/home/adrian/workspace/causality/Causal-associations-diabetes-twitter/data/Causality + hypoglycemia.xlsx\", sheet_name=\">5000_samples_\")\n",
    "#data = pd.read_excel(\"/home/adrian/Downloads/Causality + hypoglycemia.xlsx\", sheet_name=\">5000_samples_\")\n",
    "print(\"Total count:\", data.shape[0])\n",
    "data = data[data[\"Causal association\"].notnull()]\n",
    "print(\"Labeled count:\", data.shape[0])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1bea9f",
   "metadata": {},
   "source": [
    "### Interrater-reliabilty measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ac1c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "charline = data[data[\"Charline association0=no;1=yes\"].notnull()]\n",
    "coder1 = charline[\"Causal association\"].values\n",
    "coder2 = charline[\"Charline association0=no;1=yes\"]\n",
    "score = cohen_kappa_score(coder1,coder2)\n",
    "#print('Cohen\\'s Kappa:',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606db122",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "957c75e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    3720\n",
       "1.0    1280\n",
       "Name: Causal association, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Causal association\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb21b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>BIOtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm a trans woman .</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Both of us could use a world where \" brave and...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Make a world where people can just be , withou...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  Causal association  \\\n",
       "0  tonight , I learned my older girl will back he...                 0.0   \n",
       "1  ⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...                 0.0   \n",
       "2                                I'm a trans woman .                 0.0   \n",
       "3  Both of us could use a world where \" brave and...                 0.0   \n",
       "4  Make a world where people can just be , withou...                 0.0   \n",
       "\n",
       "                                             BIOtags  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1         [O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "2                                 [O, O, O, O, O, O]  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData = create_training_data(data, min_words_in_sentences=3)\n",
    "trainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2b76629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Things I have to do to keep a USER sensor stuck to me ! #T1D HTTPURL\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "USER USER USER Scary tho i have a 13 year old diabetic daughter however i read 4 thousand or more people a year die in UK jist from flu so why this fuss & panic over corona .\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Btw stay safe Hana !\n",
      "['O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "USER Hey !\n",
      "['O', 'O', 'O']\n",
      "\n",
      "\n",
      "call me crazy but I feel a little weird when people talk about their gestational diabetes on fb\n",
      "['O', 'O', 'O', 'O', 'O', 'B-E', 'I-E', 'I-E', 'I-E', 'O', 'O', 'B-C', 'I-C', 'I-C', 'I-C', 'I-C', 'O', 'O']\n",
      "\n",
      "\n",
      "She told him no eating after bed time .\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "i have type 2 diabetes .\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "But our ppl have been working everyday who have diabetes , high blood pressure and are overweight !\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "He seems like a great kid .\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Had the finger prick and I'm anemic so couldn't donate 😭\n",
      "['O', 'O', 'B-C', 'I-C', 'O', 'O', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "for i,row in trainingData.sample(n=10).iterrows():\n",
    "    print(\"\\n\")\n",
    "    print(row[\"tweet\"])\n",
    "    print(row[\"BIOtags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fabebee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    7607\n",
       "1.0    1019\n",
       "Name: Causal association, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData[\"Causal association\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ea773",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70004a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (5521, 3)\n",
      "Validate: (1380, 3)\n",
      "Test: (1725, 3)\n"
     ]
    }
   ],
   "source": [
    "trainingDataSample = trainingData#.sample(n=200)\n",
    "train = trainingDataSample.sample(frac=0.8, random_state=0)\n",
    "test = trainingDataSample.drop(train.index)\n",
    "validate = train.sample(frac=0.2, random_state=0)\n",
    "train = train.drop(validate.index)\n",
    "print(\"Train:\", train.shape)\n",
    "print(\"Validate:\", validate.shape)\n",
    "print(\"Test:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d08e047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:18: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:18: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<ipython-input-62-81586474d13f>:18: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(ids[idx]) == len(bio_tags_extended), \"token ids and BIO tags lengths do not match!\")\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5521\n",
      "1380\n",
      "1725\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transform labels + encodings into Pytorch DataSet object (including __len__, __getitem__)\n",
    "class TweetDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, labels, bio_tags, tokenizer):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bio_tags = bio_tags\n",
    "        self.tag2id = {label: idx for idx, label in enumerate([\"O\", \"B-C\", \"I-C\", \"B-E\", \"I-E\"])}\n",
    "        self.tag2id[-100] = -100\n",
    "        self.id2tag = {id:tag for tag,id in self.tag2id.items()}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.text, padding=True, truncation=True, return_token_type_ids=True)\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        bio_tags_extended = self.extend_tags(self.text[idx], self.bio_tags[idx], ids[idx])\n",
    "        assert(len(ids[idx]) == len(bio_tags_extended), \"token ids and BIO tags lengths do not match!\")\n",
    "        return {\n",
    "                \"input_ids\" : torch.tensor(ids[idx], dtype=torch.long)\n",
    "              , \"attention_mask\" : torch.tensor(mask[idx], dtype=torch.long)\n",
    "              , \"token_type_ids\" : torch.tensor(token_type_ids[idx], dtype=torch.long)\n",
    "              , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "              , \"bio_tags\" : torch.tensor(list(map(lambda bioTags: self.tag2id[bioTags], bio_tags_extended))\n",
    ", dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    \n",
    "    def extend_tags(self, tokens_old, tags_old, ids_tokenized_padded):\n",
    "        \"\"\" \n",
    "            Each token has a BIO tag label. \n",
    "            However BERT's tokenization splits tokens into subwords. How to label those subwords?\n",
    "            \n",
    "            Option 1:\n",
    "            ---------\n",
    "            \n",
    "            add the same label to each subword than the first subword. Only replace \"B\" by \"I\"\n",
    "            Ex. \n",
    "            #lowbloodsugar => '#low@@', 'blood@@', 'sugar@@'\n",
    "               \"B-C\"       =>   \"B-C\" ,   \"I-C\"  ,   \"I-C\"\n",
    "            \n",
    "            Option 2 (implemented):      \n",
    "            ---------\n",
    "            \n",
    "            From : https://huggingface.co/transformers/custom_datasets.html#token-classification-with-w-nut-emerging-entities\n",
    "            A common obstacle with using pre-trained models for token-level classification: many of the tokens in\n",
    "            the W-NUT corpus are not in DistilBert’s vocabulary. Bert and many models like it use a method called \n",
    "            WordPiece Tokenization, meaning that single words are split into multiple tokens such that each token\n",
    "            is likely to be in the vocabulary. For example, DistilBert’s tokenizer would split the Twitter \n",
    "            handle @huggingface into the tokens ['@', 'hugging', '##face']. This is a problem for us because we \n",
    "            have exactly one tag per token. If the tokenizer splits a token into multiple sub-tokens, then we will\n",
    "            end up with a mismatch between our tokens and our labels.\n",
    "\n",
    "            One way to handle this is to only train on the tag labels for the first subtoken of a split token. \n",
    "            We can do this in 🤗 Transformers by setting the labels we wish to ignore to -100. \n",
    "            In the example above, if the label for @HuggingFace is 3 (indexing B-corporation), we would set \n",
    "            the labels of ['@', 'hugging', '##face'] to [3, -100, -100].\n",
    "        \"\"\"\n",
    "        tags = [-100] # add for start token <CLS>\n",
    "        for token_old, tag in zip(tokens_old.split(\" \"), tags_old):\n",
    "#            print(F\"\\ntoken_old: {token_old};    tag: {tag}\")\n",
    "            for i, sub_token in enumerate(self.tokenizer.tokenize(token_old)):\n",
    "                if (i == 0):\n",
    "                    tags.append(tag)\n",
    "                else: \n",
    "                    tags.append(-100)\n",
    "           \n",
    "        tags.append(-100) # 0 for end of sentence token\n",
    "    \n",
    "        # append -100 for all padded elements\n",
    "        padded_elements = ids_tokenized_padded.count(1) # id 1 is <PAD> ; Alternative: where attention_mask == 0 add -100\n",
    "        tags.extend([-100]*padded_elements)\n",
    "        \n",
    "        return tags\n",
    "        \n",
    "        \n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "train_dataset = TweetDataSet(train[\"tweet\"].map(normalizeTweet).values.tolist()\n",
    "                           , train[\"Causal association\"].values.tolist()\n",
    "                           , train[\"BIOtags\"].values.tolist()\n",
    "                           , tokenizer)\n",
    "val_dataset = TweetDataSet(validate[\"tweet\"].map(normalizeTweet).values.tolist()\n",
    "                           , validate[\"Causal association\"].values.tolist()\n",
    "                           , validate[\"BIOtags\"].values.tolist()\n",
    "                           , tokenizer)\n",
    "test_dataset = TweetDataSet(test[\"tweet\"].map(normalizeTweet).values.tolist()\n",
    "                           , test[\"Causal association\"].values.tolist()\n",
    "                           , test[\"BIOtags\"].values.tolist()\n",
    "                           , tokenizer)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))\n",
    "\n",
    "# put data to batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76fc4c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing BertModel: ['roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.0.output.dense.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'lm_head.decoder.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['encoder.layer.9.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.10.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1) Trainer \n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred, labels):\n",
    "    \"\"\"\n",
    "        Dataset is unbalanced -> measure weighted metrics\n",
    "        Calculate metrics for each label, and find their average wieghted by support (Number of true instances for each label)\n",
    "        This alters 'macro' to account for label imbalance;\n",
    "        it can result in an F-Score taht is not between precision and recall\n",
    "    \"\"\"\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='weighted') #binary\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "class CausalNER(torch.nn.Module):\n",
    "    \"\"\" Model Bert\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CausalNER, self).__init__()\n",
    "        self.num_labels = 5 # B-C, I-C, B-E, I-E, O\n",
    "        self.bert = transformers.BertModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear1 = torch.nn.Linear(768, 256)\n",
    "        self.linear2 = torch.nn.Linear(256, self.num_labels)\n",
    "        self.softmax = torch.nn.Softmax(-1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "#        _, output_1 = self.bert(input_ids, attention_mask = attention_mask, token_type_ids=token_type_ids, return_dict=False) # if output 1 is our cls token\n",
    "        output_seq, _ = self.bert(input_ids, attention_mask = attention_mask, token_type_ids=token_type_ids, return_dict=False) # if output 1 is our cls token\n",
    "        output_2 = self.dropout(output_seq)\n",
    "        output_3 = self.linear1(output_2)\n",
    "        output_4 = self.dropout(output_3)\n",
    "        output_5 = self.linear2(output_4)\n",
    "        logit = self.softmax(output_5)\n",
    "        return logit\n",
    "\n",
    "\n",
    "## Model parameters\n",
    "batchsize_train = 16\n",
    "lr = 5e-5\n",
    "adam_eps = 1e-8\n",
    "epochs = 3\n",
    "num_warmup_steps = 0\n",
    "num_training_steps = len(train_loader)*epochs\n",
    "\n",
    "# Store our loss and learning rate for plotting\n",
    "train_loss_set = []\n",
    "learning_rate = []\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = CausalNER()\n",
    "model.to(device)\n",
    "\n",
    "# fine-tune only the task-specific parameters -> Vivek? \n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=lr, eps=adam_eps)\n",
    "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "loss_fn = CrossEntropyLoss(ignore_index=-100) # ignore subwords/tokens with label -100 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506651ca",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d961333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/346 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  0%|          | 1/346 [00:09<53:29,  9.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9987, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  1%|          | 2/346 [00:18<53:17,  9.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9289, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  1%|          | 3/346 [00:27<51:57,  9.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9189, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  1%|          | 4/346 [00:36<52:41,  9.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9415, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  1%|▏         | 5/346 [00:46<52:30,  9.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9550, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  2%|▏         | 6/346 [00:55<53:15,  9.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(1.0059, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  2%|▏         | 7/346 [01:05<53:46,  9.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9575, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 8/346 [01:14<52:13,  9.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9094, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  3%|▎         | 9/346 [01:23<51:18,  9.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9881, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 10/346 [01:31<50:14,  8.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9096, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  3%|▎         | 11/346 [01:41<51:33,  9.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9886, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 11/346 [01:47<54:21,  9.73s/it]\n",
      "Epoch:   0%|          | 0/3 [01:47<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-9751d2319fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gradients get accumulated by default -> clear previous accumulated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-81586474d13f>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2286\u001b[0m                 )\n\u001b[1;32m   2287\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2288\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2289\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2290\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m         )\n\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         batch_outputs = self._batch_prepare_for_model(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m         batch_outputs = self.pad(\n\u001b[0m\u001b[1;32m    622\u001b[0m             \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2664\u001b[0m         \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m             outputs = self._pad(\n\u001b[1;32m   2668\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_bio_tags = 5 # \"O\", \"B-C\", \"I-C\", \"B-E\", \"I-C\"\n",
    "for epoch in trange(1, epochs+1, desc='Epoch'):\n",
    "    print(\"<\" + \"=\"*22 + F\" Epoch {epoch} \"+ \"=\"*22 + \">\")\n",
    "\n",
    "    \n",
    "    ############ training eval metrics ######################\n",
    "    nb_tr_steps = 0 # Tracking variables\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    train_prec = []\n",
    "    train_rec = []\n",
    "    train_f1 = []\n",
    "    \n",
    "    #########################################################\n",
    "    \n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        optim.zero_grad() # gradients get accumulated by default -> clear previous accumulated gradients\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        bio_tags = batch['bio_tags'].to(device)\n",
    "        \n",
    "        ################################################\n",
    "        model.train() # set model to training mode\n",
    "        logits = model(**{\"input_ids\":input_ids, \"attention_mask\":attention_mask, \"token_type_ids\":token_type_ids}) # forward pass\n",
    "\n",
    "        ################################################ \n",
    "        # similar to the class RobertaForToken classification in transformers: https://github.com/huggingface/transformers/blob/master/src/transformers/models/roberta/modeling_roberta.py\n",
    "        active_loss = attention_mask.view(-1) == 1  # either based on attention_mask (includes <CLS>, <SEP> token)\n",
    "        active_logits = logits.view(-1, N_bio_tags)[active_loss] # N_bio_tags=5 \n",
    "        active_tags = bio_tags.view(-1)[active_loss]\n",
    "        loss = loss_fn(active_logits, active_tags)             \n",
    "        print(\"loss:\", loss)       ## TODO VIVEK: check loss function calculation\n",
    "        loss.backward() # backward pass\n",
    "        optim.step()    # update parameters and take a steup using the computed gradient\n",
    "        scheduler.step()# update learning rate scheduler\n",
    "        train_loss.append(loss.item())\n",
    "            \n",
    "            \n",
    "        ################## Training Performance Measures ##########\n",
    "        logits = logits.detach().to('cpu').numpy()\n",
    "        tags_ids = bio_tags.to('cpu').numpy()\n",
    "\n",
    "        # calculate performance measures only on tokens and not subwords or special tokens\n",
    "        tags_mask = tags_ids != -100 # only get token labels and not labels from subwords or special tokens\n",
    "        pred = np.argmax(logits, axis=2)[tags_mask] #.flatten() # convert logits to list of predicted labels\n",
    "        tags = tags_ids[tags_mask]                      \n",
    "                \n",
    "        metrics = compute_metrics(pred, tags)\n",
    "        train_acc.append(metrics[\"accuracy\"])\n",
    "        train_prec.append(metrics[\"precision\"])\n",
    "        train_rec.append(metrics[\"recall\"])\n",
    "        train_f1.append(metrics[\"f1\"])\n",
    "                          \n",
    "        nb_tr_steps += 1\n",
    "           \n",
    "    print(F'\\n\\tTraining Loss: {np.mean(train_loss)}')\n",
    "    print(F'\\n\\tTraining acc: {np.mean(train_acc)}')\n",
    "    print(F'\\n\\tTraining prec: {np.mean(train_prec)}')\n",
    "    print(F'\\n\\tTraining rec: {np.mean(train_rec)}')\n",
    "    print(F'\\n\\tTraining f1: {np.mean(train_f1)}')\n",
    "                          \n",
    "                          \n",
    "    # store the current learning rate\n",
    "    for param_group in optim.param_groups:\n",
    "        print(\"\\n\\tCurrent Learning rate: \", param_group['lr'])\n",
    "        learning_rate.append(param_group['lr'])\n",
    "    \n",
    "\n",
    "    ############# Validation ################\n",
    "    \n",
    "    nb_eval_steps = 0 # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_f1 = []\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(validation_loader):\n",
    "        batch = tuple(batch[t].to(device) for t in batch)      # batch to GPU\n",
    "        v_input_ids, v_input_mask, v_token_type_ids, v_labels, v_bio_tags = batch  # unpack inputs from dataloader\n",
    "        \n",
    "        with torch.no_grad(): # tell model not to compute or store gradients -> saves memory + speeds up validation\n",
    "            model.eval() # put model in evaluation mode for validation set\n",
    "            logits = model(**{\"input_ids\":v_input_ids, \"attention_mask\":v_input_mask, \"token_type_ids\":v_token_type_ids}) # forward pass, calculates logit predictions\n",
    "\n",
    "        ######################################################\n",
    "        \n",
    "        # similar to the class RobertaForToken classification in transformers: https://github.com/huggingface/transformers/blob/master/src/transformers/models/roberta/modeling_roberta.py\n",
    "        v_active_loss = v_input_mask.view(-1) == 1  # either based on attention_mask (includes <CLS>, <SEP> token)\n",
    "        v_active_logits = logits.view(-1, N_bio_tags)[v_active_loss] # 5 \n",
    "        v_active_tags = v_bio_tags.view(-1)[v_active_loss]\n",
    "        v_loss = loss_fn(v_active_logits, v_active_tags)             \n",
    "        val_loss.append(v_loss.item())\n",
    "              \n",
    "        #########################################################\n",
    "        logits = logits.detach().to('cpu').numpy()\n",
    "        tags_ids = v_bio_tags.to('cpu').numpy()\n",
    "\n",
    "        # calculate performance measures only on tokens and not subwords or special tokens\n",
    "        tags_mask = tags_ids != -100 # only get token labels and not labels from subwords or special tokens\n",
    "        pred = np.argmax(logits, axis=2)[tags_mask] #.flatten() # convert logits to list of predicted labels\n",
    "        tags = tags_ids[tags_mask]#.flatten()        \n",
    "        \n",
    "        metrics = compute_metrics(pred, tags)\n",
    "        val_acc.append(metrics[\"accuracy\"])\n",
    "        val_prec.append(metrics[\"precision\"])\n",
    "        val_rec.append(metrics[\"recall\"])\n",
    "        val_f1.append(metrics[\"f1\"])\n",
    "                              \n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    print(F'\\n\\tValidation Loss: {np.mean(val_loss)}')\n",
    "    print(F'\\n\\tValidation acc: {np.mean(val_acc)}')\n",
    "    print(F'\\n\\tValidation prec: {np.mean(val_prec)}')\n",
    "    print(F'\\n\\tValidation rec: {np.mean(val_rec)}')\n",
    "    print(F'\\n\\tValidation f1: {np.mean(val_f1)}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e74dcb",
   "metadata": {},
   "source": [
    "### Evaluation on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec61fa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.90it/s]/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 40%|████      | 2/5 [00:01<00:01,  2.00it/s]/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      " 60%|██████    | 3/5 [00:01<00:01,  1.86it/s]/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.77it/s]/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTest Loss: 0.9306409001350403\n",
      "\n",
      "\tTest acc: 0.9757000656404777\n",
      "\n",
      "\tTest MCC acc: 0.0\n",
      "\n",
      "\tTest prec: 0.9524102891686322\n",
      "\n",
      "\tTest rec: 0.9757000656404777\n",
      "\n",
      "\tTest f1: 0.9638081231229314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############ test eval metrics ######################\n",
    "nb_test_steps = 0 # Tracking variables\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "test_prec = []\n",
    "test_rec = []\n",
    "test_f1 = []\n",
    "\n",
    "########################################################\n",
    "for batch in tqdm(test_loader):\n",
    "    batch = tuple(batch[t].to(device) for t in batch)      # batch to GPU\n",
    "    t_input_ids, t_input_mask, t_token_type_ids, t_labels, t_bio_tags = batch     # unpack inputs from dataloader\n",
    "\n",
    "    with torch.no_grad(): # tell model not to compute or store gradients -> saves memory + speeds up validation\n",
    "        model.eval() # put model in evaluation mode for validation set\n",
    "        logits = model(**{\"input_ids\":t_input_ids, \"attention_mask\":t_input_mask, \"token_type_ids\":t_token_type_ids}) # forward pass, calculates logit predictions\n",
    "\n",
    "    ######################################################\n",
    "\n",
    "    # similar to the class RobertaForToken classification in transformers: https://github.com/huggingface/transformers/blob/master/src/transformers/models/roberta/modeling_roberta.py\n",
    "    t_active_loss = t_input_mask.view(-1) == 1  # either based on attention_mask (includes <CLS>, <SEP> token)\n",
    "    t_active_logits = logits.view(-1, N_bio_tags)[t_active_loss] # 5 \n",
    "    t_active_tags = t_bio_tags.view(-1)[t_active_loss]\n",
    "    t_loss = loss_fn(t_active_logits, t_active_tags)             \n",
    "    test_loss.append(t_loss.item())\n",
    "\n",
    "    #########################################################\n",
    "    logits = logits.detach().to('cpu').numpy()\n",
    "    tags_ids = t_bio_tags.to('cpu').numpy()\n",
    "\n",
    "    # calculate performance measures only on tokens and not subwords or special tokens\n",
    "    tags_mask = tags_ids != -100 # only get token labels and not labels from subwords or special tokens\n",
    "    pred = np.argmax(logits, axis=2)[tags_mask] #.flatten() # convert logits to list of predicted labels\n",
    "    tags = tags_ids[tags_mask]#.flatten()                          \n",
    "\n",
    "    metrics = compute_metrics(pred, tags)\n",
    "    test_acc.append(metrics[\"accuracy\"])\n",
    "    test_prec.append(metrics[\"precision\"])\n",
    "    test_rec.append(metrics[\"recall\"])\n",
    "    test_f1.append(metrics[\"f1\"])\n",
    "\n",
    "    nb_test_steps += 1\n",
    "\n",
    "print(F'\\n\\tTest Loss: {np.mean(test_loss)}')\n",
    "print(F'\\n\\tTest acc: {np.mean(test_acc)}')\n",
    "print(F'\\n\\tTest prec: {np.mean(test_prec)}')\n",
    "print(F'\\n\\tTest rec: {np.mean(test_rec)}')\n",
    "print(F'\\n\\tTest f1: {np.mean(test_f1)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7533934b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f4d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c7951a2",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de743400",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"finetuned-NER-35-epochs.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6987102d",
   "metadata": {},
   "source": [
    "### Load model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a9844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\", if torch.cuda.is_available() else \"cpu\")\n",
    "model = CausalityBERT()\n",
    "model.load_state_dict(torch.load(\"finetuned-35-epochs.pth\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55974c",
   "metadata": {},
   "source": [
    "# Questions to Vivek?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c2aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "If there is only one cause and no effect ; or only one effect and no cause => ignore ? -> YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7861017f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc08df56",
   "metadata": {},
   "source": [
    "### Small example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad29e594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>BIOtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>I've been light headed and shakey for the last...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[O, O, O, B-E, I-E, O, B-E, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7584</th>\n",
       "      <td>2 before to 0.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  Causal association  \\\n",
       "447   I've been light headed and shakey for the last...                 1.0   \n",
       "7584                                     2 before to 0.                 0.0   \n",
       "\n",
       "                                                BIOtags  \n",
       "447   [O, O, O, B-E, I-E, O, B-E, O, O, O, O, O, O, ...  \n",
       "7584                                    [O, O, O, O, O]  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small steps\n",
    "sample = trainingData.sample(n=5, random_state=11)[3:]\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e0626df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:\n",
      "I've been light headed and shakey for the last 4 hours due to low blood sugar and it's uncomfortable and debilitating !\n",
      "BIO tags:\n",
      "['O', 'O', 'O', 'B-E', 'I-E', 'O', 'B-E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-C', 'I-C', 'I-C', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "tokenized:\n",
      "['<s>', '2', 'before', 'to', '0', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "BIO tags extended:\n",
      "tensor([-100,    0,    0,    0,    3,    4,    0,    3, -100,    0,    0,    0,\n",
      "           0,    0,    0,    0,    1,    2,    2,    0,    0,    0,    0,    0,\n",
      "           0, -100, -100,    0, -100])\n",
      "\n",
      "ids:\n",
      "tensor([    0,     8,   120,   108,   937,  4432,    13,  2258,  1499,    19,\n",
      "            6,   175,   204,   493,  1006,     9,  1101,  1945,  4057,    13,\n",
      "           18,    20,  6976,    13, 13084, 41480,  1526,    12,     2])\n",
      "BIO tags extended:\n",
      "tensor([-100,    0,    0,    0,    3,    4,    0,    3, -100,    0,    0,    0,\n",
      "           0,    0,    0,    0,    1,    2,    2,    0,    0,    0,    0,    0,\n",
      "           0, -100, -100,    0, -100])\n",
      "attention mask:\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "N_bio_tags = 5 \n",
    "train_dataset = TweetDataSet(sample[\"tweet\"].map(normalizeTweet).values.tolist()\n",
    "                           , sample[\"Causal association\"].values.tolist()\n",
    "                           , sample[\"BIOtags\"].values.tolist()\n",
    "                           , tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "print(\"Tweet:\")\n",
    "print(sample.iloc[0][\"tweet\"])\n",
    "print(\"BIO tags:\")\n",
    "print(sample.iloc[0][\"BIOtags\"])\n",
    "print(\"\\ntokenized:\")\n",
    "print(tokenizer.convert_ids_to_tokens(train_dataset[1][\"input_ids\"]))\n",
    "print(\"BIO tags extended:\")\n",
    "print(train_dataset[0][\"bio_tags\"])\n",
    "print(\"\\nids:\")\n",
    "print(train_dataset[0][\"input_ids\"])\n",
    "print(\"BIO tags extended:\")\n",
    "print(train_dataset[0][\"bio_tags\"])\n",
    "print(\"attention mask:\")\n",
    "print(train_dataset[0][\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e42579ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH:\n",
      "tweet A: ['<s>', 'I', \"'ve\", 'been', 'light', 'headed', 'and', 'sha@@', 'key', 'for', 'the', 'last', '4', 'hours', 'due', 'to', 'low', 'blood', 'sugar', 'and', 'it', \"'s\", 'uncomfortable', 'and', 'deb@@', 'ilit@@', 'ating', '!', '</s>']\n",
      "tweet B: ['<s>', '2', 'before', 'to', '0', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "tweet A shape: 29\n",
      "tweet B shape: 29\n",
      "============\n",
      "\n",
      "logits.shape: torch.Size([2, 29, 5])\n",
      "bio_tags.shape: torch.Size([2, 29])\n",
      "============\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(train_loader):\n",
    "    optim.zero_grad() # gradients get accumulated by default -> clear previous accumulated gradients\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "    bio_tags = batch['bio_tags'].to(device)\n",
    "    print(\"BATCH:\")\n",
    "    print(\"tweet A:\", tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
    "    print(\"tweet B:\", tokenizer.convert_ids_to_tokens(input_ids[1]))\n",
    "    print(\"tweet A shape:\", len(tokenizer.convert_ids_to_tokens(input_ids[0])))\n",
    "    print(\"tweet B shape:\", len(tokenizer.convert_ids_to_tokens(input_ids[1])))    \n",
    "    print(\"============\\n\")\n",
    "    \n",
    "    ################################################\n",
    "    model.train() # set model to training mode\n",
    "    logits = model(**{\"input_ids\":input_ids, \"attention_mask\":attention_mask, \"token_type_ids\":token_type_ids}) # forward pass\n",
    "\n",
    "    print(\"logits.shape:\", logits.shape)\n",
    "    print(\"bio_tags.shape:\", bio_tags.shape)\n",
    "    print(\"============\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b2c2194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active_loss.shape: torch.Size([58])\n",
      "active_loss: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False])\n",
      "active_logits: torch.Size([36, 5])\n",
      "active_tags: torch.Size([36])\n",
      "loss: tensor(1.1157, grad_fn=<NllLossBackward>)\n",
      "============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#################################################\n",
    "# similar to the class RobertaForToken classification in transformers: https://github.com/huggingface/transformers/blob/master/src/transformers/models/roberta/modeling_roberta.py\n",
    "active_loss = attention_mask.view(-1) == 1  # either based on attention_mask (includes <CLS>, <SEP> token)\n",
    "print(\"active_loss.shape:\", active_loss.shape)\n",
    "print(\"active_loss:\", active_loss)\n",
    "\n",
    "#active_loss2 = bio_tags.view(-1) != -100   # excludes all special tokens including <CLS>, <SEP>\n",
    "active_logits = logits.view(-1, N_bio_tags)[active_loss] # 5 \n",
    "active_tags = bio_tags.view(-1)[active_loss]\n",
    "loss = loss_fn(active_logits, active_tags)\n",
    "print(\"active_logits:\", active_logits.shape)\n",
    "print(\"active_tags:\", active_tags.shape)\n",
    "print(\"loss:\", loss)\n",
    "print(\"============\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99c2bb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred.shape: (29,)\n",
      "pred: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "tags.shape (29,)\n",
      "tags: [0 0 0 3 4 0 3 0 0 0 0 0 0 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "acc: 0.7931034482758621\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logits = logits.detach().to('cpu').numpy()\n",
    "tags_ids = bio_tags.to('cpu').numpy()\n",
    "\n",
    "# calculate performance measures only on tokens and not subwords or special tokens\n",
    "tags_mask = tags_ids != -100 # only get token labels and not labels from subwords or special tokens\n",
    "pred = np.argmax(logits, axis=2)[tags_mask] #.flatten() # convert logits to list of predicted labels\n",
    "print(\"pred.shape:\", pred.shape)\n",
    "print(\"pred:\", pred)    \n",
    "tags = tags_ids[tags_mask]#.flatten()\n",
    "print(\"tags.shape\", tags.shape)\n",
    "print(\"tags:\", tags)\n",
    "\n",
    "print(\"acc:\", accuracy_score(tags, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f672bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TODO: \n",
    "    - write annotation guidelines\n",
    "    - check model predictions, where does it fail?\n",
    "    - \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
