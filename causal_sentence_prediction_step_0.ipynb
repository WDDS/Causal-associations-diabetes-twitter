{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A model build using TweetBERT to classify tweet as causal or non-causal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The causal sentence prediction model will be trained in several steps using an active learning approach, where in each step the training dataset will be augmented.\n",
    "In each step the causal sentence classifier is trained and applied on a subsample of unlabeled tweets to identify tweets with causal elements. Those tweets are then manually labeled for the two tasks: causal sentence prediction and cause-effect identification (NER). The newly labeled data will be added to the training dataset and the causal sentence classifier will be retrained with the augmented dataset to increase performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "id": "Dy8FcfUkfobe",
    "outputId": "c8784c32-777a-4b4d-c735-cff4472e017a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy \n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import transformers\n",
    "from tqdm import tqdm, trange\n",
    "import io\n",
    "from utils import normalizeTweet, split_into_sentences, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "########################### DATA FILE ###################################\n",
    "dataPath = \"/home/adrian/workspace/causality/Causal-associations-diabetes-twitter/data/Causality_tweets_data.xlsx\"\n",
    "\n",
    "\n",
    "########################### MODEL PARAMETERS ############################\n",
    "lr = 1e-3    \n",
    "adam_eps = 1e-8\n",
    "epochs = 3\n",
    "num_warmup_steps = 0\n",
    "early_patience = 3 # how long to wait after last time validation loss improved\n",
    "\n",
    "train_batch_size = 64\n",
    "val_batch_size = 32\n",
    "test_batch_size = 32\n",
    "test_to_train_ratio = 0.1 # 10% test and 90% train\n",
    "val_to_train_ratio = 0.2\n",
    "\n",
    "saveModelName = \"finetuned-{}-epochs-lr_{}.pth\".format(epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data (tweets!):\n",
      "0.0    3710\n",
      "1.0    1290\n",
      "Name: Causal association, dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER Cheers ! Have one for this diabetic too !</td>\n",
       "      <td>mS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER Additionally the medicines are being char...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medicines are being charged at MRP</td>\n",
       "      <td>costing much higher</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text Intent  \\\n",
       "0  tonight , I learned my older girl will back he...    NaN   \n",
       "1  USER USER I knew diabetes and fibromyalgia wer...   joke   \n",
       "2  ⬇ ️ ⬇ ️ ⬇ ️ THIS ⬇ ️ ⬇ ️ ⬇ ️ My wife has type ...     mS   \n",
       "3     USER Cheers ! Have one for this diabetic too !     mS   \n",
       "4  USER Additionally the medicines are being char...    NaN   \n",
       "\n",
       "                                Cause               Effect  Causal association  \n",
       "0                                 NaN                  NaN                 0.0  \n",
       "1                                 NaN                  NaN                 0.0  \n",
       "2                                 NaN                  NaN                 0.0  \n",
       "3                                 NaN                  NaN                 0.0  \n",
       "4  medicines are being charged at MRP  costing much higher                 1.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### DATA TO LOAD ######\n",
    "\n",
    "data = pd.read_excel(dataPath, sheet_name=\"round0\")\n",
    "data = data[data[\"Causal association\"].notnull()] # some tweets at the end are not labeled yet\n",
    "data = data[[\"full_text\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\"]]\n",
    "print(\"Data (tweets!):\")\n",
    "print(data[\"Causal association\"].value_counts())\n",
    "print()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_CQUEp5g_NC"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_index_of_sentence_in_tweet(tweet, sentence):\n",
    "    \"\"\" \n",
    "    The sentence tokens are included in the tweet tokens.\n",
    "    Return the start end end indices of the sentence tokens in the tweet tokens\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_start_word = sentence[0]\n",
    "    start_indices = [i for i, x in enumerate(tweet) if x == sentence_start_word] # find all indices of the start word of the sentence \n",
    "    try:\n",
    "        for start_index in start_indices:\n",
    "            isTrueStartIndex = all([tweet[start_index+i] == sentence[i] for i in range(len(sentence))])\n",
    "            #print(\"start_index:\", start_index, \"isTrueStartIndex:\", isTrueStartIndex)\n",
    "            if isTrueStartIndex:\n",
    "                return start_index, start_index + len(sentence) \n",
    "    except:\n",
    "        print(\"ERROR: StartIndex should have been found for sentence:\")\n",
    "        print(\"tweet:\")\n",
    "        print(tweet)\n",
    "        print(\"sentence:\")\n",
    "        print(sentence)\n",
    "    return -1, -2 # should not be returned\n",
    "\n",
    "\n",
    "def split_tweets_to_sentences(data):\n",
    "    \"\"\" \n",
    "        Splits tweets into sentences and associates the appropriate intent, causes, effects and causal association\n",
    "        to each sentence.\n",
    "        \n",
    "        Parameters:\n",
    "        - min_words_in_sentences: Minimal number of words in a sentence such that the sentence is kept. \n",
    "                                  Assumption: A sentence with too few words does not have enough information\n",
    "                              \n",
    "                              \n",
    "                              \n",
    "        Ex.:\n",
    "        full_text                              | Intent | Cause | Effect | Causal association | ...\n",
    "        --------------------------------------------------------------------------------------------\n",
    "        what? type 1 causes insulin dependence | q;msS  | type 1|insulin dependence | 1       | ...  \n",
    "        \n",
    "        New dataframe returned: \n",
    "        full_text                              | Intent | Cause | Effect | Causal association | ...\n",
    "        --------------------------------------------------------------------------------------------\n",
    "        what?                                  |   q    |       |        |       0            | ...\n",
    "        type 1 causes insulin dependence       |        | type 1| insulin dependence | 1       | ...  \n",
    "    \"\"\"\n",
    "\n",
    "    newDF = pd.DataFrame(columns=[\"sentence\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\", \"tokenized\"])\n",
    "    \n",
    "    for i,row in data.iterrows():\n",
    "        causes = row[\"Cause\"]\n",
    "        effects = row[\"Effect\"]\n",
    "        sentences = split_into_sentences(normalizeTweet(row[\"full_text\"]))\n",
    "        \n",
    "        # single sentence in tweet\n",
    "        if len(sentences) == 1:\n",
    "            singleSentenceIntent = \"\"\n",
    "            if isinstance(row[\"Intent\"], str):\n",
    "                if len(row[\"Intent\"].split(\";\")) > 1:\n",
    "                    singleSentenceIntent = row[\"Intent\"].strip().replace(\";msS\", \"\").replace(\"msS;\", \"\").replace(\";mS\", \"\").replace(\"mS;\", \"\")\n",
    "                else:\n",
    "                    if row[\"Intent\"] == \"mS\" or row[\"Intent\"] == \"msS\":\n",
    "                        singleSentenceIntent = \"\"\n",
    "                    else:\n",
    "                        singleSentenceIntent = row[\"Intent\"].strip()\n",
    "                    \n",
    "            newDF=newDF.append(pd.Series({\"sentence\": sentences[0] # only one sentence\n",
    "                         , \"Intent\": singleSentenceIntent\n",
    "                         , \"Cause\" : row[\"Cause\"]\n",
    "                         , \"Effect\": row[\"Effect\"]\n",
    "                         , \"Causal association\" : row[\"Causal association\"]\n",
    "                         , \"tokenized\": row[\"tokenized\"]}), ignore_index=True)\n",
    "        \n",
    "        # tweet has several sentences\n",
    "        else: \n",
    "            intents = str(row[\"Intent\"]).strip().split(\";\")\n",
    "            for sentence in sentences:\n",
    "                sent_tokenized = sentence.split(\" \")\n",
    "                causeInSentence = np.nan if not isinstance(causes, str) or not any([cause in sentence for cause in causes.split(\";\")]) else \";\".join([cause for cause in causes.split(\";\") if cause in sentence])\n",
    "                effectInSentence = np.nan if not isinstance(effects, str) or not any([effect in sentence for effect in effects.split(\";\")]) else \";\".join([effect for effect in effects.split(\";\") if effect in sentence])\n",
    "                causalAssociationInSentence = 1 if isinstance(causeInSentence, str) and isinstance(effectInSentence, str) else 0\n",
    "                startIndex, endIndex = get_start_end_index_of_sentence_in_tweet(row[\"tokenized\"], sent_tokenized)\n",
    "                sentence_tokenized = row[\"tokenized\"][startIndex:endIndex]\n",
    "                \n",
    "                if \"q\" in intents and sentence[-1] == \"?\": # if current sentence is question\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"q\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized}), ignore_index=True)                    \n",
    "                elif \"joke\" in intents: # all sentences with \"joke\" in tweet keep the intent \"joke\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"joke\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized}), ignore_index=True)   \n",
    "                elif \"neg\" in intents: # all sentences with \"neg\" in tweet keep intent \"neg\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"neg\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized}), ignore_index=True)               \n",
    "                elif isinstance(causeInSentence, str) and isinstance(effectInSentence, str): # cause effect sentence\n",
    "                    causalIntent = \"\"\n",
    "                    if len(causeInSentence.split(\";\")) > 1:\n",
    "                        causalIntent = \"mC\"\n",
    "                        if len(effectInSentence.split(\";\")) > 1:\n",
    "                            causalIntent = \"mC;mE\"\n",
    "                    elif len(effectInSentence.split(\";\")) > 1:\n",
    "                        causalIntent = \"mE\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": causalIntent, \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized}), ignore_index=True)                                  \n",
    "                else:\n",
    "                    nonCausalIntent = \"\"\n",
    "                    if isinstance(causeInSentence, str): # only cause is given\n",
    "                        if len(causeInSentence.split(\";\")) > 1:\n",
    "                            nonCausalIntent = \"mC\"\n",
    "                    elif isinstance(effectInSentence, str): # only effect is given\n",
    "                        if len(effectInSentence.split(\";\")) > 1:\n",
    "                            nonCausalIntent = \"mE\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": nonCausalIntent, \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized}), ignore_index=True)\n",
    "\n",
    "    return newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tweets: 5000\n",
      "N sentences: 11756\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[tonight, ,, I, learned, my, older, girl, will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fiercely .</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[Fiercely, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#impressive #bigsister #type1 #type1times2</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[#impressive, #bigsister, #type1, #type1times2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER I knew diabetes and fibromyalgia wer...</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[USER, USER, I, knew, diabetes, and, fibromyal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>:face_with_rolling_eyes:</td>\n",
       "      <td>joke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[:face_with_rolling_eyes:]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence Intent Cause Effect  \\\n",
       "0  tonight , I learned my older girl will back he...          NaN    NaN   \n",
       "1                                         Fiercely .          NaN    NaN   \n",
       "2         #impressive #bigsister #type1 #type1times2          NaN    NaN   \n",
       "3  USER USER I knew diabetes and fibromyalgia wer...   joke   NaN    NaN   \n",
       "4                           :face_with_rolling_eyes:   joke   NaN    NaN   \n",
       "\n",
       "  Causal association                                          tokenized  \n",
       "0                  0  [tonight, ,, I, learned, my, older, girl, will...  \n",
       "1                  0                                      [Fiercely, .]  \n",
       "2                  0    [#impressive, #bigsister, #type1, #type1times2]  \n",
       "3                  0  [USER, USER, I, knew, diabetes, and, fibromyal...  \n",
       "4                  0                         [:face_with_rolling_eyes:]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Split tweets into sentences (train classifier on sentence level) ####\n",
    "\n",
    "print(\"N tweets:\", data.shape[0])\n",
    "\n",
    "data[\"tokenized\"] = data[\"full_text\"].map(lambda tweet: normalizeTweet(tweet).split(\" \"))\n",
    "dataSentences = split_tweets_to_sentences(data)\n",
    "print(\"N sentences:\", dataSentences.shape[0])\n",
    "dataSentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N sentences before filtering:  11756\n",
      "N sentences after filtering:  8368\n",
      "Distribution:\n",
      "0.0    7320\n",
      "1.0    1048\n",
      "Name: Causal association, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tonight , I learned my older girl will back he...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[tonight, ,, I, learned, my, older, girl, will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#impressive #bigsister #type1 #type1times2</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[#impressive, #bigsister, #type1, #type1times2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>:down_arrow: :down_arrow: :down_arrow: THIS :d...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[:down_arrow:, :down_arrow:, :down_arrow:, THI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I 'm a trans woman .</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, 'm, a, trans, woman, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Both of us could use a world where \" brave and...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[Both, of, us, could, use, a, world, where, \",...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence Intent Cause Effect  \\\n",
       "0  tonight , I learned my older girl will back he...          NaN    NaN   \n",
       "2         #impressive #bigsister #type1 #type1times2          NaN    NaN   \n",
       "5  :down_arrow: :down_arrow: :down_arrow: THIS :d...          NaN    NaN   \n",
       "6                               I 'm a trans woman .          NaN    NaN   \n",
       "7  Both of us could use a world where \" brave and...          NaN    NaN   \n",
       "\n",
       "  Causal association                                          tokenized  \n",
       "0                  0  [tonight, ,, I, learned, my, older, girl, will...  \n",
       "2                  0    [#impressive, #bigsister, #type1, #type1times2]  \n",
       "5                  0  [:down_arrow:, :down_arrow:, :down_arrow:, THI...  \n",
       "6                  0                        [I, 'm, a, trans, woman, .]  \n",
       "7                  0  [Both, of, us, could, use, a, world, where, \",...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## Remove sentences with joke, question, negation and keep only sentences with more than 3 tokens #####\n",
    "\n",
    "print(\"N sentences before filtering: \", dataSentences.shape[0])\n",
    "dataSentFiltered = dataSentences[~dataSentences[\"Intent\"].str.contains(\"neg|joke|q\")] \n",
    "dataSentFiltered = dataSentFiltered[dataSentFiltered[\"tokenized\"].map(len) > 3] \n",
    "print(\"N sentences after filtering: \", dataSentFiltered.shape[0])\n",
    "print(\"Distribution:\")\n",
    "print(dataSentFiltered[\"Causal association\"].value_counts())\n",
    "dataSentFiltered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89yCGSj6hWSY"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: Count = 500, % of 0 = 0.852, % of 1 = 0.148\n",
      "Train: Count = 450, % of 0 = 0.8511, % of 1 = 0.1489\n",
      "Test: Count = 50, % of 0 = 0.86, % of 1 = 0.14\n",
      "Balancing class wts: for 0 = 0.5875, for 1 = 3.3582\n"
     ]
    }
   ],
   "source": [
    "####################### Stratified splits ####################\n",
    "\n",
    "\n",
    "## ONLY FOR TESTING ---------------\n",
    "#dataSentFiltered = dataSentFiltered[0:500] # for testing\n",
    "\n",
    "text = dataSentFiltered[\"sentence\"].map(normalizeTweet).values.tolist()\n",
    "labels = dataSentFiltered[\"Causal association\"].values.tolist()\n",
    "# first split the data into training and testing label in the ratio of 90:10\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(text, labels, test_size=test_to_train_ratio, stratify=labels, random_state=9)\n",
    "\n",
    "# Optional:split the new training data (80% of actual data) to get train and validation set\n",
    "# But we choose in each epoch another part of the training data to be the validation set\n",
    "#train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, stratify=train_labels, random_state=9)\n",
    "\n",
    "\n",
    "\n",
    "data_count_info = pd.Series(labels).value_counts(normalize=True)\n",
    "train_count_info = pd.Series(train_labels).value_counts(normalize=True)\n",
    "#val_count_info = pd.Series(val_labels).value_counts(normalize=True)\n",
    "test_count_info = pd.Series(test_labels).value_counts(normalize=True)\n",
    "\n",
    "# for class-imbalanced dataset, the class weight for a ith class\n",
    "# to be specified for balancing in the loss function is given by:\n",
    "# weight[i] = num_samples / (num_classes * num_samples[i])\n",
    "# since train_count_info obtained above has fraction of samples\n",
    "# for ith class, hence the corresponding weight calculation is:\n",
    "class_weight = (1/train_count_info)/len(train_count_info)\n",
    "\n",
    "print(\"All: Count = {}, % of 0 = {}, % of 1 = {}\".format(\n",
    "    len(labels), *data_count_info.round(4).to_list()))\n",
    "print(\"Train: Count = {}, % of 0 = {}, % of 1 = {}\".format(\n",
    "    len(train_labels), *train_count_info.round(4).to_list()))\n",
    "#print(\"Val: Count = {}, % of 0 = {}, % of 1 = {}\".format(\n",
    "#    len(val_labels), *val_count_info.round(4).to_list()))\n",
    "print(\"Test: Count = {}, % of 0 = {}, % of 1 = {}\".format(\n",
    "    len(test_labels), *test_count_info.round(4).to_list()))\n",
    "print(\"Balancing class wts: for 0 = {}, for 1 = {}\".format(\n",
    "    *class_weight.round(4).to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zm2V9HvYhcDj",
    "outputId": "71ae75b7-ccb2-48e0-f6c7-4ee71c785f82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "class TweetDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, labels, tokenizer):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.text, padding=True, truncation=True, return_token_type_ids=True)\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        return {\n",
    "                \"input_ids\" : torch.tensor(ids[idx], dtype=torch.long)\n",
    "              , \"attention_mask\" : torch.tensor(mask[idx], dtype=torch.long)\n",
    "              , \"token_type_ids\" : torch.tensor(token_type_ids[idx], dtype=torch.long)\n",
    "              , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "train_dataset = TweetDataSet(train_texts, train_labels, tokenizer)\n",
    "#val_dataset = TweetDataSet(val_texts, val_labels, tokenizer)\n",
    "test_dataset = TweetDataSet(test_texts, test_labels, tokenizer)\n",
    "print(len(train_dataset))\n",
    "#print(len(val_dataset))\n",
    "print(len(test_dataset))\n",
    "\n",
    "# During training: In each epoch one part of the training data will be used as validation set\n",
    "#train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "#validation_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## we are measuring weighted metrics - as our dataset is imbalanced \n",
    "# Calculate metrics for each label, and find their average weighted by support\n",
    "# (the number of true instances for each label). \n",
    "# This alters ‘macro’ to account for label imbalance; \n",
    "# it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef\n",
    "\n",
    "def compute_metrics(pred, labels):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='weighted')\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "class CausalityBERT(torch.nn.Module):\n",
    "    \"\"\" Model Bert\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CausalityBERT, self).__init__()\n",
    "        self.num_labels = 2\n",
    "        self.bert = transformers.BertModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear1 = torch.nn.Linear(768, 256)\n",
    "        self.linear2 = torch.nn.Linear(256, self.num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, output_1 = self.bert(input_ids, attention_mask = attention_mask, token_type_ids=token_type_ids, return_dict=False) # if output 1 is our cls token        \n",
    "        output_2 = self.dropout(output_1)\n",
    "        output_3 = self.linear1(output_2)  \n",
    "        output_4 = self.dropout(output_3)\n",
    "        output_5 = self.linear2(output_4)\n",
    "        return output_5\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing BertModel: ['lm_head.layer_norm.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'lm_head.decoder.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'lm_head.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.2.attention.self.key.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.7.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'pooler.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'pooler.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.query.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = CausalityBERT()\n",
    "model.to(device)\n",
    "\n",
    "# fine-tune only the task-specific parameters\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_training_steps = np.ceil(len(train_dataset)/train_batch_size)*epochs\n",
    "optim = AdamW(model.parameters(), lr=lr, eps=adam_eps)\n",
    "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps) # scheduler with a linearly decreasing learning rate from the initial lr set in the optimizer to 0; after a warmup period durnig which it increases linearly from 0 to the initial lr set in the optimizer\n",
    "\n",
    "## penalising more for class with less number of exaplmes \n",
    "loss_fn = CrossEntropyLoss(torch.tensor(class_weight.to_list()).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n",
      "{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}\n",
      "[331, 296, 142, 144, 190, 92, 373, 276, 70, 90, 359, 10, 143, 269, 49, 203, 176, 230, 89, 417, 411, 105, 383, 306, 391, 321, 37, 68, 397, 47, 344, 125, 377, 385, 135, 15, 290, 79, 360, 41, 130, 174, 376, 51, 301, 60, 138, 140, 45, 336, 83, 19, 14, 31, 82, 284, 57, 286, 328, 351, 4, 160, 356, 310, 339, 433, 226, 87, 39, 32, 9, 146, 94, 136, 38, 175, 204, 332, 414, 114, 295, 314, 307, 149, 361, 211, 173, 153, 5, 20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█▋        | 1/6 [00:08<00:44,  8.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.7333, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 33%|███▎      | 2/6 [00:17<00:35,  8.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9998, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 50%|█████     | 3/6 [00:26<00:27,  9.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.7118, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 67%|██████▋   | 4/6 [00:35<00:17,  8.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6729, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 83%|████████▎ | 5/6 [00:44<00:09,  9.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9102, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 6/6 [00:50<00:00,  8.45s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6902, grad_fn=<NllLossBackward>)\n",
      "\n",
      "\tTrain loss: 0.7863632440567017\n",
      "\n",
      "\ttrain acc: 0.4005208333333334\n",
      "\n",
      "\ttraining prec: 0.5039967764433103\n",
      "\n",
      "\ttraining rec: 0.4005208333333334\n",
      "\n",
      "\ttraining f1: 0.33839122195300964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 33%|███▎      | 1/3 [00:04<00:08,  4.08s/it]\u001b[A<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:08<00:04,  4.33s/it]\u001b[A<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 3/3 [00:11<00:00,  3.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tValidation loss: 0.6613787015279134\n",
      "\n",
      "\tValidation acc: 0.1658653846153846\n",
      "\n",
      "\tValidation prec: 0.02774631718441815\n",
      "\n",
      "\tValidation rec: 0.1658653846153846\n",
      "\n",
      "\tValidation f1: 0.04748863235705342\n",
      "Validation loss decreased (inf --> 0.661379).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|███▎      | 1/3 [01:03<02:06, 63.42s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 2 ======================>\n",
      "{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}\n",
      "[185, 210, 259, 288, 342, 208, 293, 180, 402, 392, 378, 24, 245, 407, 275, 292, 334, 421, 271, 150, 352, 106, 301, 111, 281, 308, 31, 182, 87, 202, 341, 346, 319, 358, 214, 284, 5, 58, 148, 119, 8, 187, 398, 309, 146, 449, 143, 443, 362, 98, 427, 164, 14, 400, 389, 131, 234, 96, 287, 282, 73, 171, 114, 440, 141, 395, 81, 68, 413, 274, 104, 12, 173, 297, 30, 192, 419, 228, 43, 162, 163, 391, 100, 340, 135, 240, 432, 377, 248, 125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█▋        | 1/6 [00:08<00:43,  8.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6493, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 33%|███▎      | 2/6 [00:17<00:35,  9.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6737, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 50%|█████     | 3/6 [00:26<00:26,  8.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.7390, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 67%|██████▋   | 4/6 [00:36<00:18,  9.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.5806, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 83%|████████▎ | 5/6 [00:45<00:09,  9.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6926, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      "100%|██████████| 6/6 [00:50<00:00,  8.49s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6541, grad_fn=<NllLossBackward>)\n",
      "\n",
      "\tTrain loss: 0.6648796101411184\n",
      "\n",
      "\ttrain acc: 0.6843750000000001\n",
      "\n",
      "\ttraining prec: 0.7889891306488469\n",
      "\n",
      "\ttraining rec: 0.6843750000000001\n",
      "\n",
      "\ttraining f1: 0.6844228952644545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 33%|███▎      | 1/3 [00:04<00:08,  4.08s/it]\u001b[A<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:08<00:04,  4.04s/it]\u001b[A<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 3/3 [00:11<00:00,  3.83s/it]\u001b[A\n",
      "Epoch:  67%|██████▋   | 2/3 [02:05<01:02, 62.86s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tValidation loss: 0.6942933201789856\n",
      "\n",
      "\tValidation acc: 0.8317307692307692\n",
      "\n",
      "\tValidation prec: 0.6920649963017752\n",
      "\n",
      "\tValidation rec: 0.8317307692307692\n",
      "\n",
      "\tValidation f1: 0.7554197065105828\n",
      "EarlyStopping counter: 1 out of 3\n",
      "<====================== Epoch 3 ======================>\n",
      "{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}\n",
      "[191, 278, 439, 416, 421, 194, 198, 84, 113, 438, 265, 284, 122, 263, 396, 77, 252, 332, 336, 227, 64, 429, 173, 415, 388, 375, 139, 290, 196, 365, 211, 33, 98, 202, 161, 447, 247, 371, 132, 51, 296, 372, 9, 75, 308, 311, 183, 235, 412, 411, 96, 258, 78, 127, 180, 190, 189, 435, 369, 231, 120, 366, 237, 17, 307, 295, 25, 221, 222, 449, 71, 370, 144, 376, 131, 342, 385, 243, 337, 286, 118, 324, 343, 69, 338, 16, 59, 381, 432, 309]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 17%|█▋        | 1/6 [00:09<00:46,  9.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6273, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 33%|███▎      | 2/6 [00:18<00:37,  9.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.7226, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 50%|█████     | 3/6 [00:27<00:27,  9.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6697, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 67%|██████▋   | 4/6 [00:38<00:19,  9.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6684, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 83%|████████▎ | 5/6 [00:47<00:09,  9.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6319, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      "100%|██████████| 6/6 [00:53<00:00,  8.88s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6466, grad_fn=<NllLossBackward>)\n",
      "\n",
      "\tTrain loss: 0.6610898673534393\n",
      "\n",
      "\ttrain acc: 0.7182291666666667\n",
      "\n",
      "\ttraining prec: 0.8086913246285846\n",
      "\n",
      "\ttraining rec: 0.7182291666666667\n",
      "\n",
      "\ttraining f1: 0.7414985415582943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 33%|███▎      | 1/3 [00:04<00:08,  4.08s/it]\u001b[A<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:08<00:04,  4.45s/it]\u001b[A<ipython-input-11-fd35bfb1853b>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "\n",
      "100%|██████████| 3/3 [00:12<00:00,  4.05s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tValidation loss: 0.6273249785105387\n",
      "\n",
      "\tValidation acc: 0.53125\n",
      "\n",
      "\tValidation prec: 0.8334754153010732\n",
      "\n",
      "\tValidation rec: 0.53125\n",
      "\n",
      "\tValidation f1: 0.5864355458782703\n",
      "Validation loss decreased (0.661379 --> 0.627325).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 3/3 [03:12<00:00, 64.11s/it]\n"
     ]
    }
   ],
   "source": [
    "############ TRAINING #############\n",
    "\n",
    "# initialise the early_stopping object\n",
    "early_stopping = EarlyStopping(patience=early_patience, path=saveModelName, verbose=True)\n",
    "\n",
    "train_size = len(train_texts)\n",
    "train_indices = list(range(train_size))\n",
    "\n",
    "train_avg_loss = [] # avg training loss per epoch\n",
    "val_avg_loss = [] # avg validation loss per epoch\n",
    "train_avg_acc = [] # avg training accuracy per epoch\n",
    "val_avg_acc = [] # avg val accuracy per epoch\n",
    "n_trained_epochs = 0\n",
    "\n",
    "\n",
    "for epoch in trange(1, epochs+1, desc='Epoch'):\n",
    "    print(\"<\" + \"=\"*22 + F\" Epoch {epoch} \"+ \"=\"*22 + \">\")\n",
    "    \n",
    "    \n",
    "    ###### Trick: Take in each epoch different indices from training data to be used as validation  ########\n",
    "    np.random.shuffle(train_indices)\n",
    "    val_split_index = int(np.floor(val_to_train_ratio * train_size))\n",
    "    # Validation goes from index 0..val_split_index; training set goes from index val_split_index..end \n",
    "    train_sampler = SubsetRandomSampler(train_indices[val_split_index:])\n",
    "    val_sampler = SubsetRandomSampler(train_indices[:val_split_index])\n",
    "    train_loader = DataLoader(dataset=train_dataset, shuffle=False, batch_size=train_batch_size,sampler=train_sampler)\n",
    "    validation_loader = DataLoader(dataset=train_dataset, shuffle=False, batch_size=val_batch_size,sampler=val_sampler)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########### training eval metrics #############################\n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    train_prec = []\n",
    "    train_rec = []\n",
    "    train_f1 = []\n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        optim.zero_grad() # gradients get accumulated by default -> clear previous accumulated gradients\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        ###########################################################################\n",
    "        model.train()\n",
    "        logits = model(**{\"input_ids\":input_ids, \"attention_mask\":attention_mask, \"token_type_ids\":token_type_ids}) # forward pass\n",
    "        #############################################################################\n",
    "        loss = loss_fn(logits, labels)\n",
    "        print(\"loss:\", loss)\n",
    "        loss.backward() # backward pass\n",
    "        optim.step()    # update parameters and take a step up using the computed gradient\n",
    "        scheduler.step()# update learning rate scheduler\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    \n",
    "        ############# Training Accuracy Measure ###################################\n",
    "\n",
    "        # move logits and labels to CPU\n",
    "        logits = logits.detach().to('cpu').numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "\n",
    "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "\n",
    "        metrics = compute_metrics(pred_flat, labels_flat)        \n",
    "        \n",
    "        train_acc.append(metrics[\"accuracy\"])\n",
    "        train_prec.append(metrics[\"precision\"])\n",
    "        train_rec.append(metrics[\"recall\"])\n",
    "        train_f1.append(metrics[\"f1\"])\n",
    "        \n",
    "    train_avg_loss.append(np.mean(train_loss))\n",
    "    train_avg_acc.append(np.mean(train_acc))\n",
    "    print(F'\\n\\tTrain loss: {np.mean(train_loss)}')\n",
    "    print(F'\\n\\ttrain acc: {np.mean(train_acc)}')\n",
    "    print(F'\\n\\ttraining prec: {np.mean(train_prec)}')\n",
    "    print(F'\\n\\ttraining rec: {np.mean(train_rec)}')\n",
    "    print(F'\\n\\ttraining f1: {np.mean(train_f1)}')\n",
    "    \n",
    "    n_trained_epochs += 1\n",
    "    \n",
    "    ###################################################################################\n",
    "\n",
    "    \n",
    "    ## ---- Validation ------\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_f1 = []\n",
    "    \n",
    "    \n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(validation_loader):\n",
    "        batch = tuple(batch[t].to(device) for t in batch)      # batch to GPU\n",
    "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch     # unpack inputs from dataloader\n",
    "        \n",
    "        with torch.no_grad(): # tell model not to compute or store gradients -> saves memory + speeds up validation\n",
    "            ##################################################################################\n",
    "            model.eval()\n",
    "            logits = model(**{\"input_ids\":b_input_ids, \"attention_mask\":b_input_mask, \"token_type_ids\":b_token_type_ids}) # forward pass, calculates logit predictions \n",
    "\n",
    "                        \n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "        \n",
    "        # move logits and labels to CPU\n",
    "        logits = logits.detach().to('cpu').numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "        \n",
    "        metrics = compute_metrics(pred_flat, labels_flat)\n",
    "        val_acc.append(metrics[\"accuracy\"])\n",
    "        val_prec.append(metrics[\"precision\"])\n",
    "        val_rec.append(metrics[\"recall\"])\n",
    "        val_f1.append(metrics[\"f1\"])\n",
    "\n",
    "    val_avg_loss.append(np.mean(val_loss))\n",
    "    val_avg_acc.append(np.mean(val_acc))\n",
    "    print(F'\\n\\tValidation loss: {np.mean(val_loss)}')\n",
    "    print(F'\\n\\tValidation acc: {np.mean(val_acc)}')\n",
    "    print(F'\\n\\tValidation prec: {np.mean(val_prec)}')\n",
    "    print(F'\\n\\tValidation rec: {np.mean(val_rec)}')\n",
    "    print(F'\\n\\tValidation f1: {np.mean(val_f1)}')\n",
    "\n",
    "    # early_stopping needs the validation loss to check if it has decreased,\n",
    "    # and if it has, it will make a checkpoint of the current model\n",
    "    early_stopping(np.average(val_loss), model)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[0.7863632440567017, 0.6648796101411184, 0.6610898673534393]\n",
      "[0.6613787015279134, 0.6942933201789856, 0.6273249785105387]\n"
     ]
    }
   ],
   "source": [
    "print(n_trained_epochs)\n",
    "print(train_avg_loss)\n",
    "print(val_avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, n_trained_epochs+1), train_avg_loss, label=\"train loss\")\n",
    "plt.plot(range(1, n_trained_epochs+1), val_avg_loss, label=\"val loss\")\n",
    "plt.title(\"Training Curve (lr={})\".format(lr))\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "9icIGkKhh9Sh"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, n_trained_epochs+1), train_avg_acc, label=\"train acc\")\n",
    "plt.plot(range(1, n_trained_epochs+1), val_avg_acc, label=\"val acc\")\n",
    "plt.title(\"Training Curve (lr={})\".format(lr))\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Train Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the model \n",
    "##torch.save(model.state_dict(), \"finetuned-35-epochs-1e3-lr-with-weighted-loss.pth\") # early stopping saves model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing BertModel: ['roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'lm_head.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.embeddings.token_type_embeddings.weight', 'lm_head.layer_norm.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.embeddings.position_ids', 'lm_head.dense.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'lm_head.decoder.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.9.attention.self.key.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CausalityBERT(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the locally saved model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CausalityBERT()\n",
    "model.load_state_dict(torch.load(\"finetuned-35-epochs-1e3-lr-with-weighted-loss.pth\"))\n",
    "## Move the model to the GPU \n",
    "model.to(device)\n",
    "model.eval() # gettign in the eval mode \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation on the test dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]<ipython-input-34-285408ed5860>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 50%|█████     | 1/2 [00:02<00:02,  2.83s/it]<ipython-input-34-285408ed5860>:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ttest loss: nan\n",
      "\n",
      "\ttest acc: 0.5347222222222222\n",
      "\n",
      "\ttest prec: 0.8143518518518519\n",
      "\n",
      "\ttest rec: 0.5347222222222222\n",
      "\n",
      "\ttest f1: 0.5866545893719807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/adrian/miniconda3/envs/pytorch/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#loss_fn = CrossEntropyLoss()\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "test_prec = []\n",
    "test_rec = []\n",
    "test_f1 = []\n",
    "\n",
    "\n",
    "for batch in tqdm(test_loader):\n",
    "    batch = tuple(batch[t].to(device) for t in batch)      # batch to GPU\n",
    "    b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch     # unpack inputs from dataloader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits = model(**{\"input_ids\":b_input_ids, \"attention_mask\":b_input_mask, \"token_type_ids\":b_token_type_ids}) # forward pass, calculates logit predictions \n",
    "    \n",
    "    # move logits and labels to CPU\n",
    "    logits = logits.detach().to('cpu').numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "    \n",
    "    metrics = compute_metrics(pred_flat, labels_flat)\n",
    "    test_acc.append(metrics[\"accuracy\"])\n",
    "    test_prec.append(metrics[\"precision\"])\n",
    "    test_rec.append(metrics[\"recall\"])\n",
    "    test_f1.append(metrics[\"f1\"])\n",
    "\n",
    "    \n",
    "print(F'\\n\\ttest loss: {np.mean(test_loss)}')\n",
    "print(F'\\n\\ttest acc: {np.mean(test_acc)}')\n",
    "print(F'\\n\\ttest prec: {np.mean(test_prec)}')\n",
    "print(F'\\n\\ttest rec: {np.mean(test_rec)}')\n",
    "print(F'\\n\\ttest f1: {np.mean(test_f1)}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print predictions of last test set batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded Sentence:\n",
      "['<s>', '100', '%', 'life', '/', 'death', 'situation', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "prediction: 0\n",
      "\n",
      "Padded Sentence:\n",
      "['<s>', 'USER', 'I', \"'m\", 'diabetic', ',', 'but', 'I', 'would', 'buy', 'a', 'dozen', 'anyway', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "prediction: 0\n",
      "\n",
      "Padded Sentence:\n",
      "['<s>', 'I', 'think', 'the', 'chronic', 'fatigue', 'part', 'of', 'diabetes', 'has', 'hit', 'me', 'full', 'whack', 'again', ':sleeping_face:', ':persevering_face:', '#run@@', 'ning@@', 'on@@', 'empty', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "prediction: 0\n",
      "\n",
      "Padded Sentence:\n",
      "['<s>', 'USER', 'USER', 'Liquid', 'sac@@', 'char@@', 'in', 'is', 'the', 'only', 'swee@@', 'tener', 'to', 'my', 'knowledge', 'that', 'does', 'not', 'affect', 'gluc@@', 'ose', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "# take last batch of test set:\n",
    "\n",
    "for i in range(len(batch)):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(b_input_ids[i])\n",
    "    print(\"\\nPadded Sentence:\")\n",
    "    print(tokens)\n",
    "    print(\"prediction:\", pred_flat[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add seed => check\n",
    "# add binary accuracy  => ???????\n",
    "# add plot loss function accuracy => validation accu\n",
    "\n",
    "# y - axis: loss function; validation accuracy\n",
    "# x - axis: epochs\n",
    "\n",
    "\n",
    "# epochs, learning rate => ok\n",
    "\n",
    "# 90% training => 10% test  => ok\n",
    "# how to use random batch of training set for validation\n",
    "\n",
    "# clean notebook\n",
    "\n",
    "# clean data sheet => check\n",
    "\n",
    "# Check Pytorch: EarlyStopping add => check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Causality-BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
