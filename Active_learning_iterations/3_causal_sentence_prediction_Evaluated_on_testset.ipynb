{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A model build using TweetBERT to classify tweet as causal or non-causal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The causal sentence prediction model will be trained in several steps using an active learning approach, where in each step the training dataset will be augmented.\n",
    "In each step the causal sentence classifier is trained and applied on a subsample of unlabeled tweets to identify tweets with causal elements. Those tweets are then manually labeled for the two tasks: causal sentence prediction and cause-effect identification (NER). The newly labeled data will be added to the training dataset and the causal sentence classifier will be retrained with the augmented dataset to increase performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "id": "Dy8FcfUkfobe",
    "outputId": "c8784c32-777a-4b4d-c735-cff4472e017a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy \n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import transformers\n",
    "from tqdm import tqdm, trange\n",
    "import io\n",
    "from utils import normalizeTweet, split_into_sentences, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "########################### Check if cuda available ############################\n",
    "# print(\"Cuda available: \", torch.cuda.is_available())\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "########################### DATA FILE ###################################\n",
    "# dataPath = \"/home/adrian/workspace/causality/Causal-associations-diabetes-twitter/data/Causality_tweets_data.xlsx\"\n",
    "# dataPath = \"data/Causality_tweets_data.xlsx\"\n",
    "dataPath = \"data/Causality_tweets_data.xlsx\"\n",
    "\n",
    "\n",
    "########################### MODEL PARAMETERS ############################\n",
    "active_learning_round = 3 # will change the saved model name \n",
    "lr = 1e-3    \n",
    "adam_eps = 1e-8\n",
    "epochs = 35\n",
    "num_warmup_steps = 0\n",
    "early_patience = 7# how long to wait after last time validation loss improved\n",
    "\n",
    "train_batch_size = 16\n",
    "val_batch_size = 16\n",
    "test_batch_size = 16\n",
    "test_to_train_ratio = 0.1 # 10% test and 90% train\n",
    "val_to_train_ratio = 0.2\n",
    "\n",
    "#metrics_average = \"binary\" # this will give measure for class_1,i.e., causal class\n",
    "finetuned_model = \"./model-causal-tweet/model_3_finetuned-5-epochs-lr_0.001.pth\"\n",
    "\n",
    "#\n",
    "# saveModelName = \"./model-causal-model/model_1_finetuned-{}-epochs-lr_{}.pth\".format(epochs, lr) # it should be epoch so that the name shows at what epoch teh mdel ws saved\n",
    "#finetuned_model = \"./model-causal-model/model_2_finetuned-30-epochs-lr_0.001.pth\" # load finetuned model from previous round to continue fine-tuning on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if thec cuda is available and then select the `gpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  False\n",
      "Selected cpu for this notebook\n"
     ]
    }
   ],
   "source": [
    "########################### Check if cuda available ############################\n",
    "print(\"Cuda available: \", torch.cuda.is_available())\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Selected {} for this notebook\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data round 0 (tweets!):\n",
      "0.0    3710\n",
      "1.0    1290\n",
      "Name: Causal association, dtype: int64\n",
      "-------------------------\n",
      "Sentences round 1:\n",
      "0.0    1763\n",
      "1.0     429\n",
      "Name: Causal association, dtype: int64\n",
      "-------------------------\n",
      "sentences round 2:\n",
      "0.0    1658\n",
      "1.0     150\n",
      "Name: Causal association, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "After merge old data:\n",
      "0.0    7131\n",
      "1.0    1869\n",
      "Name: Causal association, dtype: int64\n",
      "-------------------------\n",
      "sentences round 3:\n",
      "0.0    1886\n",
      "1.0     215\n",
      "Name: Causal association, dtype: int64\n",
      "--------------------\n",
      " 0:non causal tweet \n",
      " 1: causal tweet.\n",
      " \n",
      " each tweet may have more than one sentence and we are splitting them and labelling by checking if cause or effect occur in them or not\n"
     ]
    }
   ],
   "source": [
    "##### DATA TO LOAD ######\n",
    "\n",
    "data_round0 = pd.read_excel(dataPath, sheet_name=\"round0\")\n",
    "data_round0 = data_round0[data_round0[\"Causal association\"].notnull()] # some tweets at the end are not labeled yet\n",
    "data_round0 = data_round0[[\"full_text\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\"]]\n",
    "print(\"Data round 0 (tweets!):\")\n",
    "print(data_round0[\"Causal association\"].value_counts())\n",
    "print(\"-----\"*5)\n",
    "\n",
    "\n",
    "##### additional data labeled through active learning strategy - round 1 ########\n",
    "data_round1 = pd.read_excel(dataPath, sheet_name=\"round1\")\n",
    "data_round1 = data_round1[data_round1[\"Causal association\"].notnull()]\n",
    "data_round1 = data_round1[[\"sentence\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\"]]\n",
    "data_round1.rename(columns ={\"sentence\":\"full_text\"}, inplace=True) # rename for merge\n",
    "print(\"Sentences round 1:\")\n",
    "print(data_round1[\"Causal association\"].value_counts())\n",
    "print(\"-----\"*5)\n",
    "\n",
    "##### additional data labeled through active learning strategy - round 2 ########\n",
    "data_round2 = pd.read_excel(dataPath, sheet_name=\"round2\")\n",
    "data_round2 = data_round2[data_round2[\"Causal association\"].notnull()]\n",
    "data_round2 = data_round2[[\"sentence\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\"]]\n",
    "data_round2.rename(columns ={\"sentence\":\"full_text\"}, inplace=True) # rename for merge\n",
    "print(\"sentences round 2:\")\n",
    "print(data_round2[\"Causal association\"].value_counts())\n",
    "print(\"-----\"*5)\n",
    "#### merge datasets ######\n",
    "data_old = data_round0.append(data_round1).append(data_round2)\n",
    "print(\"\\nAfter merge old data:\")\n",
    "print(data_old[\"Causal association\"].value_counts())\n",
    "data_old.head()\n",
    "print(\"-----\"*5)\n",
    "\n",
    "##### new additional data labeled through active learning strategy - round 3 (model is only retrained with this data) #####################\n",
    "data_round3 = pd.read_excel(dataPath, sheet_name=\"round3\")\n",
    "data_round3 = data_round3[data_round3[\"Causal association\"].notnull()]\n",
    "data_round3 = data_round3[[\"sentence\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\"]]\n",
    "data_round3.rename(columns ={\"sentence\":\"full_text\"}, inplace=True) # rename for merge\n",
    "data_new = data_round3\n",
    "print(\"sentences round 3:\")\n",
    "print(data_new[\"Causal association\"].value_counts())\n",
    "\n",
    "print(\"----\"*5)\n",
    "print(\" 0:non causal tweet \\n 1: causal tweet.\\n \\n each tweet may have more than one sentence and we are splitting them and labelling by checking if cause or effect occur in them or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_CQUEp5g_NC"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_index_of_sentence_in_tweet(tweet, sentence):\n",
    "    \"\"\" \n",
    "    The sentence tokens are included in the tweet tokens.\n",
    "    Return the start end end indices of the sentence tokens in the tweet tokens\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_start_word = sentence[0]\n",
    "    start_indices = [i for i, x in enumerate(tweet) if x == sentence_start_word] # find all indices of the start word of the sentence \n",
    "    try:\n",
    "        for start_index in start_indices:\n",
    "            isTrueStartIndex = all([tweet[start_index+i] == sentence[i] for i in range(len(sentence))])\n",
    "            #print(\"start_index:\", start_index, \"isTrueStartIndex:\", isTrueStartIndex)\n",
    "            if isTrueStartIndex:\n",
    "                return start_index, start_index + len(sentence) \n",
    "    except:\n",
    "        print(\"ERROR: StartIndex should have been found for sentence:\")\n",
    "        print(\"tweet:\")\n",
    "        print(tweet)\n",
    "        print(\"sentence:\")\n",
    "        print(sentence)\n",
    "    return -1, -2 # should not be returned\n",
    "\n",
    "\n",
    "def split_tweets_to_sentences(data):\n",
    "    \"\"\" \n",
    "        Splits tweets into sentences and associates the appropriate intent, causes, effects and causal association\n",
    "        to each sentence.\n",
    "        \n",
    "        Parameters:\n",
    "        - min_words_in_sentences: Minimal number of words in a sentence such that the sentence is kept. \n",
    "                                  Assumption: A sentence with too few words does not have enough information\n",
    "                              \n",
    "                              \n",
    "                              \n",
    "        Ex.:\n",
    "        full_text                              | Intent | Cause | Effect | Causal association | ...\n",
    "        --------------------------------------------------------------------------------------------\n",
    "        what? type 1 causes insulin dependence | q;msS  | type 1|insulin dependence | 1       | ...  \n",
    "        \n",
    "        New dataframe returned: \n",
    "        full_text                              | Intent | Cause | Effect | Causal association | ...\n",
    "        --------------------------------------------------------------------------------------------\n",
    "        what?                                  |   q    |       |        |       0            | ...\n",
    "        type 1 causes insulin dependence       |        | type 1| insulin dependence | 1       | ...  \n",
    "    \"\"\"\n",
    "\n",
    "    newDF = pd.DataFrame(columns=[\"sentence\", \"Intent\", \"Cause\", \"Effect\", \"Causal association\", \"tokenized\"])\n",
    "    \n",
    "    for i,row in data.iterrows():\n",
    "        causes = row[\"Cause\"]\n",
    "        effects = row[\"Effect\"]\n",
    "        sentences = split_into_sentences(normalizeTweet(row[\"full_text\"]))\n",
    "        \n",
    "        # single sentence in tweet\n",
    "        if len(sentences) == 1:\n",
    "            singleSentenceIntent = \"\"\n",
    "            if isinstance(row[\"Intent\"], str):\n",
    "                if len(row[\"Intent\"].split(\";\")) > 1:\n",
    "                    singleSentenceIntent = row[\"Intent\"].strip().replace(\";msS\", \"\").replace(\"msS;\", \"\").replace(\";mS\", \"\").replace(\"mS;\", \"\")\n",
    "                else:\n",
    "                    if row[\"Intent\"] == \"mS\" or row[\"Intent\"] == \"msS\":\n",
    "                        singleSentenceIntent = \"\"\n",
    "                    else:\n",
    "                        singleSentenceIntent = row[\"Intent\"].strip()\n",
    "                    \n",
    "            newDF=newDF.append(pd.Series({\"sentence\": sentences[0] # only one sentence\n",
    "                         , \"Intent\": singleSentenceIntent\n",
    "                         , \"Cause\" : row[\"Cause\"]\n",
    "                         , \"Effect\": row[\"Effect\"]\n",
    "                         , \"Causal association\" : row[\"Causal association\"]\n",
    "                         , \"tokenized\": row[\"tokenized\"]}), ignore_index=True)\n",
    "        \n",
    "        # tweet has several sentences\n",
    "        else: \n",
    "            intents = str(row[\"Intent\"]).strip().split(\";\")\n",
    "            for sentence in sentences:\n",
    "                sent_tokenized = sentence.split(\" \")\n",
    "                causeInSentence = np.nan if not isinstance(causes, str) or not any([cause in sentence for cause in causes.split(\";\")]) else \";\".join([cause for cause in causes.split(\";\") if cause in sentence])\n",
    "                effectInSentence = np.nan if not isinstance(effects, str) or not any([effect in sentence for effect in effects.split(\";\")]) else \";\".join([effect for effect in effects.split(\";\") if effect in sentence])\n",
    "                causalAssociationInSentence = 1 if isinstance(causeInSentence, str) and isinstance(effectInSentence, str) else 0\n",
    "                startIndex, endIndex = get_start_end_index_of_sentence_in_tweet(row[\"tokenized\"], sent_tokenized)\n",
    "                sentence_tokenized = row[\"tokenized\"][startIndex:endIndex]\n",
    "                \n",
    "                if \"q\" in intents and sentence[-1] == \"?\": # if current sentence is question\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"q\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized}), ignore_index=True)                    \n",
    "                elif \"joke\" in intents: # all sentences with \"joke\" in tweet keep the intent \"joke\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"joke\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized}), ignore_index=True)   \n",
    "                elif \"neg\" in intents: # all sentences with \"neg\" in tweet keep intent \"neg\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": \"neg\", \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized}), ignore_index=True)               \n",
    "                elif isinstance(causeInSentence, str) and isinstance(effectInSentence, str): # cause effect sentence\n",
    "                    causalIntent = \"\"\n",
    "                    if len(causeInSentence.split(\";\")) > 1:\n",
    "                        causalIntent = \"mC\"\n",
    "                        if len(effectInSentence.split(\";\")) > 1:\n",
    "                            causalIntent = \"mC;mE\"\n",
    "                    elif len(effectInSentence.split(\";\")) > 1:\n",
    "                        causalIntent = \"mE\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": causalIntent, \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized}), ignore_index=True)                                  \n",
    "                else:\n",
    "                    nonCausalIntent = \"\"\n",
    "                    if isinstance(causeInSentence, str): # only cause is given\n",
    "                        if len(causeInSentence.split(\";\")) > 1:\n",
    "                            nonCausalIntent = \"mC\"\n",
    "                    elif isinstance(effectInSentence, str): # only effect is given\n",
    "                        if len(effectInSentence.split(\";\")) > 1:\n",
    "                            nonCausalIntent = \"mE\"\n",
    "                    newDF=newDF.append(pd.Series({\"sentence\": sentence, \"Intent\": nonCausalIntent, \"Cause\" : causeInSentence\n",
    "                                                , \"Effect\": effectInSentence, \"Causal association\" : causalAssociationInSentence\n",
    "                                                , \"tokenized\": sentence_tokenized}), ignore_index=True)\n",
    "\n",
    "    return newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of  tweets old: 9000\n",
      "Count of  tweets new: 2101\n",
      "Count of sentences old: 15756\n",
      "Count of sentences new: 2101\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I got super sick the last day I was in Mexico ...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[I, got, super, sick, the, last, day, I, was, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER @USER I know this is just twitter , but ...</td>\n",
       "      <td>mC;mE</td>\n",
       "      <td>type 1 diabetic;low blood sugar</td>\n",
       "      <td>die;sad</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[@USER, @USER, I, know, this, is, just, twitte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>While I was lifting today spotify reminded me ...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[While, I, was, lifting, today, spotify, remin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER Many already send for private finger pri...</td>\n",
       "      <td>mC</td>\n",
       "      <td>finger prick tests;medicine</td>\n",
       "      <td>alive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[@USER, Many, already, send, for, private, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER @USER Speaking of testimony , I beat typ...</td>\n",
       "      <td></td>\n",
       "      <td>Daniel diet</td>\n",
       "      <td>beat type 2 diabetes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[@USER, @USER, Speaking, of, testimony, ,, I, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence Intent  \\\n",
       "0  I got super sick the last day I was in Mexico ...          \n",
       "1  @USER @USER I know this is just twitter , but ...  mC;mE   \n",
       "2  While I was lifting today spotify reminded me ...          \n",
       "3  @USER Many already send for private finger pri...     mC   \n",
       "4  @USER @USER Speaking of testimony , I beat typ...          \n",
       "\n",
       "                             Cause                Effect  Causal association  \\\n",
       "0                              NaN                   NaN                 0.0   \n",
       "1  type 1 diabetic;low blood sugar               die;sad                 1.0   \n",
       "2                              NaN                   NaN                 0.0   \n",
       "3      finger prick tests;medicine                 alive                 1.0   \n",
       "4                      Daniel diet  beat type 2 diabetes                 1.0   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [I, got, super, sick, the, last, day, I, was, ...  \n",
       "1  [@USER, @USER, I, know, this, is, just, twitte...  \n",
       "2  [While, I, was, lifting, today, spotify, remin...  \n",
       "3  [@USER, Many, already, send, for, private, fin...  \n",
       "4  [@USER, @USER, Speaking, of, testimony, ,, I, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Split tweets into sentences (train classifier on sentence level) ####\n",
    "\n",
    "print(\"Count of  tweets old:\", data_old.shape[0])\n",
    "print(\"Count of  tweets new:\", data_new.shape[0])\n",
    "\n",
    "data_old[\"tokenized\"] = data_old[\"full_text\"].map(lambda tweet: normalizeTweet(tweet).split(\" \"))\n",
    "dataSentences_old = split_tweets_to_sentences(data_old)\n",
    "print(\"Count of sentences old:\", dataSentences_old.shape[0])\n",
    "\n",
    "\n",
    "data_new[\"tokenized\"] = data_new[\"full_text\"].map(lambda tweet: normalizeTweet(tweet).split(\" \"))\n",
    "dataSentences_new = split_tweets_to_sentences(data_new)\n",
    "print(\"Count of sentences new:\", dataSentences_new.shape[0])\n",
    "dataSentences_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count ofsentences old before filtering:  15756\n",
      "Count of sentences old after filtering:  12229\n",
      "\n",
      "\n",
      "Distribution old:\n",
      "0.0    10625\n",
      "1.0     1604\n",
      "Name: Causal association, dtype: int64\n",
      "--------------------\n",
      "Count of sentences new before filtering:  2101\n",
      "Count of sentences new after filtering:  2056\n",
      "Distribution new:\n",
      "\n",
      "\n",
      "0.0    1855\n",
      "1.0     201\n",
      "Name: Causal association, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Causal association</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I got super sick the last day I was in Mexico ...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[I, got, super, sick, the, last, day, I, was, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER @USER I know this is just twitter , but ...</td>\n",
       "      <td>mC;mE</td>\n",
       "      <td>type 1 diabetic;low blood sugar</td>\n",
       "      <td>die;sad</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[@USER, @USER, I, know, this, is, just, twitte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>While I was lifting today spotify reminded me ...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[While, I, was, lifting, today, spotify, remin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER Many already send for private finger pri...</td>\n",
       "      <td>mC</td>\n",
       "      <td>finger prick tests;medicine</td>\n",
       "      <td>alive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[@USER, Many, already, send, for, private, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER @USER Speaking of testimony , I beat typ...</td>\n",
       "      <td></td>\n",
       "      <td>Daniel diet</td>\n",
       "      <td>beat type 2 diabetes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[@USER, @USER, Speaking, of, testimony, ,, I, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence Intent  \\\n",
       "0  I got super sick the last day I was in Mexico ...          \n",
       "1  @USER @USER I know this is just twitter , but ...  mC;mE   \n",
       "2  While I was lifting today spotify reminded me ...          \n",
       "3  @USER Many already send for private finger pri...     mC   \n",
       "4  @USER @USER Speaking of testimony , I beat typ...          \n",
       "\n",
       "                             Cause                Effect  Causal association  \\\n",
       "0                              NaN                   NaN                 0.0   \n",
       "1  type 1 diabetic;low blood sugar               die;sad                 1.0   \n",
       "2                              NaN                   NaN                 0.0   \n",
       "3      finger prick tests;medicine                 alive                 1.0   \n",
       "4                      Daniel diet  beat type 2 diabetes                 1.0   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [I, got, super, sick, the, last, day, I, was, ...  \n",
       "1  [@USER, @USER, I, know, this, is, just, twitte...  \n",
       "2  [While, I, was, lifting, today, spotify, remin...  \n",
       "3  [@USER, Many, already, send, for, private, fin...  \n",
       "4  [@USER, @USER, Speaking, of, testimony, ,, I, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## Remove sentences with joke, question, negation and keep only sentences with more than 3 tokens #####\n",
    "\n",
    "print(\"Count ofsentences old before filtering: \", dataSentences_old.shape[0])\n",
    "dataSentFiltered_old = dataSentences_old[~dataSentences_old[\"Intent\"].str.contains(\"neg|joke|q\")] \n",
    "dataSentFiltered_old = dataSentFiltered_old[dataSentFiltered_old[\"tokenized\"].map(len) > 3] \n",
    "print(\"Count of sentences old after filtering: \", dataSentFiltered_old.shape[0])\n",
    "print(\"\\n\")\n",
    "print(\"Distribution old:\")\n",
    "print(dataSentFiltered_old[\"Causal association\"].value_counts())\n",
    "print(\"----\"*5)\n",
    "\n",
    "print(\"Count of sentences new before filtering: \", dataSentences_new.shape[0])\n",
    "dataSentFiltered_new = dataSentences_new[~dataSentences_new[\"Intent\"].str.contains(\"neg|joke|q\")] \n",
    "dataSentFiltered_new = dataSentFiltered_new[dataSentFiltered_new[\"tokenized\"].map(len) > 3] \n",
    "print(\"Count of sentences new after filtering: \", dataSentFiltered_new.shape[0])\n",
    "print(\"Distribution new:\")\n",
    "print(\"\\n\")\n",
    "print(dataSentFiltered_new[\"Causal association\"].value_counts())\n",
    "dataSentFiltered_new.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89yCGSj6hWSY"
   },
   "source": [
    "### Data split and calculate class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: Count = 5481, % of 0 = 0.8814, % of 1 = 0.1186\n",
      "\n",
      "\n",
      "Train: Count = 2056, % of 0 = 0.9022, % of 1 = 0.0978\n",
      "\n",
      "\n",
      "Test: Count = 1223, % of 0 = 0.8692, % of 1 = 0.1308\n",
      "\n",
      "\n",
      "Balancing class wts: for 0 = 0.5542, for 1 = 5.1144\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################### Stratified splits ####################\n",
    "\n",
    "\n",
    "## ONLY FOR TESTING ---------------\n",
    "#dataSentFiltered = dataSentFiltered[0:500] # for testing\n",
    "\n",
    "text_old = dataSentFiltered_old[\"sentence\"].map(normalizeTweet).values.tolist()\n",
    "labels_old = dataSentFiltered_old[\"Causal association\"].values.tolist()\n",
    "\n",
    "# first split the data into training and testing label in the ratio of 90:10\n",
    "train_texts_old, test_texts, train_labels_old, test_labels = train_test_split(text_old, labels_old, test_size=test_to_train_ratio, stratify=labels_old, random_state=9)\n",
    "train_texts_old, val_texts, train_labels_old, val_labels = train_test_split(train_texts_old, train_labels_old, test_size=val_to_train_ratio, stratify=train_labels_old, random_state=9)\n",
    "\n",
    "# Redefine training set:Take only new labeled tweets from round 3 for training; test and val set come from old data\n",
    "train_texts = dataSentFiltered_new[\"sentence\"].map(normalizeTweet).values.tolist()\n",
    "train_labels = dataSentFiltered_new[\"Causal association\"].values.tolist()\n",
    "\n",
    "labels = train_labels + val_labels +test_labels # combining new train with validataion and test data from previous rounds\n",
    "\n",
    "\n",
    "data_count_info = pd.Series(labels).value_counts(normalize=True)\n",
    "train_count_info = pd.Series(train_labels).value_counts(normalize=True)\n",
    "test_count_info = pd.Series(test_labels).value_counts(normalize=True)\n",
    "\n",
    "# for class-imbalanced dataset, the class weight for a ith class\n",
    "# to be specified for balancing in the loss function is given by:\n",
    "# weight[i] = num_samples / (num_classes * num_samples[i])\n",
    "# since train_count_info obtained above has fraction of samples\n",
    "# for ith class, hence the corresponding weight calculation is:\n",
    "class_weight = (1/train_count_info)/len(train_count_info)\n",
    "\n",
    "print(\"All: Count = {}, % of 0 = {}, % of 1 = {}\".format(len(labels), *data_count_info.round(4).to_list()))\n",
    "print(\"\\n\")\n",
    "print(\"Train: Count = {}, % of 0 = {}, % of 1 = {}\".format(len(train_labels), *train_count_info.round(4).to_list()))\n",
    "print(\"\\n\")\n",
    "print(\"Test: Count = {}, % of 0 = {}, % of 1 = {}\".format(len(test_labels), *test_count_info.round(4).to_list()))\n",
    "print(\"\\n\")\n",
    "print(\"Balancing class wts: for 0 = {}, for 1 = {}\".format(*class_weight.round(4).to_list()))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining our DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zm2V9HvYhcDj",
    "outputId": "71ae75b7-ccb2-48e0-f6c7-4ee71c785f82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2056\n",
      "1223\n"
     ]
    }
   ],
   "source": [
    "class TweetDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, labels, tokenizer):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.text, padding=True, truncation=True, return_token_type_ids=True)\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        return {\n",
    "                \"input_ids\" : torch.tensor(ids[idx], dtype=torch.long)\n",
    "              , \"attention_mask\" : torch.tensor(mask[idx], dtype=torch.long)\n",
    "              , \"token_type_ids\" : torch.tensor(token_type_ids[idx], dtype=torch.long)\n",
    "              , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "train_dataset = TweetDataSet(train_texts, train_labels, tokenizer)\n",
    "test_dataset = TweetDataSet(test_texts, test_labels, tokenizer)\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "\n",
    "# During training: In each epoch one part of the training data will be used as validation set\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we are measuring weighted metrics - as our dataset is imbalanced \n",
    "# Calculate metrics for each label, and find their average weighted by support\n",
    "# (the number of true instances for each label). \n",
    "# This alters ‘macro’ to account for label imbalance; \n",
    "# it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef\n",
    "\n",
    "def compute_metrics(pred, labels, average=\"macro\"):\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='weighted')\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels,pred, average=average)\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CausalityBERT(torch.nn.Module):\n",
    "    \"\"\" Model Bert\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CausalityBERT, self).__init__()\n",
    "        self.num_labels = 2\n",
    "        self.bert = transformers.BertModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear1 = torch.nn.Linear(768, 256)\n",
    "        self.linear2 = torch.nn.Linear(256, self.num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, output_1 = self.bert(input_ids, attention_mask = attention_mask, token_type_ids=token_type_ids, return_dict=False) # if output 1 is our cls token        \n",
    "        output_2 = self.dropout(output_1)\n",
    "        output_3 = self.linear1(output_2)  \n",
    "        output_4 = self.dropout(output_3)\n",
    "        output_5 = self.linear2(output_4)\n",
    "        return output_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving the model to  GPU and defining training parameters: \n",
    "    * num_training_steps \n",
    "    * optimizers \n",
    "    * scheduler \n",
    "    * loss funciton (weighted) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing BertModel: ['roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.0.output.dense.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.8.output.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['encoder.layer.11.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.attention.self.query.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CausalityBERT(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = CausalityBERT() ## just load the model trained in previous round here \n",
    "model.load_state_dict(torch.load(finetuned_model, map_location='cpu')) # load model trained in previous round\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation on the test dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 0/77 [00:00<?, ?it/s]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "  1%|▎                           | 1/77 [00:06<08:18,  6.56s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "  3%|▋                           | 2/77 [00:11<07:17,  5.83s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "  4%|█                           | 3/77 [00:17<06:57,  5.65s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "  5%|█▍                          | 4/77 [00:22<06:42,  5.51s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "  6%|█▊                          | 5/77 [00:28<06:41,  5.58s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  8%|██▏                         | 6/77 [00:34<06:56,  5.87s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "  9%|██▌                         | 7/77 [00:40<06:43,  5.76s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 10%|██▉                         | 8/77 [00:47<07:00,  6.10s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 12%|███▎                        | 9/77 [00:55<07:37,  6.73s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 13%|███▌                       | 10/77 [01:01<07:28,  6.69s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 14%|███▊                       | 11/77 [01:08<07:22,  6.70s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 16%|████▏                      | 12/77 [01:15<07:20,  6.77s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 17%|████▌                      | 13/77 [01:22<07:27,  6.99s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 18%|████▉                      | 14/77 [01:29<07:17,  6.95s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 19%|█████▎                     | 15/77 [01:37<07:23,  7.15s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 21%|█████▌                     | 16/77 [01:44<07:07,  7.01s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████▉                     | 17/77 [01:50<06:56,  6.93s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 23%|██████▎                    | 18/77 [01:58<07:06,  7.23s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 25%|██████▋                    | 19/77 [02:04<06:40,  6.91s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 26%|███████                    | 20/77 [02:10<06:17,  6.62s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 27%|███████▎                   | 21/77 [02:16<05:58,  6.41s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 29%|███████▋                   | 22/77 [02:22<05:48,  6.33s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 30%|████████                   | 23/77 [02:30<06:00,  6.68s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 31%|████████▍                  | 24/77 [02:37<05:52,  6.65s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 32%|████████▊                  | 25/77 [02:43<05:43,  6.61s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 34%|█████████                  | 26/77 [02:50<05:43,  6.73s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 35%|█████████▍                 | 27/77 [02:58<05:55,  7.12s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 36%|█████████▊                 | 28/77 [03:05<05:46,  7.06s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 38%|██████████▏                | 29/77 [03:11<05:28,  6.85s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 39%|██████████▌                | 30/77 [03:18<05:20,  6.83s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 40%|██████████▊                | 31/77 [03:25<05:15,  6.87s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 42%|███████████▏               | 32/77 [03:31<04:59,  6.66s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 43%|███████████▌               | 33/77 [03:38<04:47,  6.53s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████▉               | 34/77 [03:44<04:41,  6.54s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 45%|████████████▎              | 35/77 [03:51<04:36,  6.58s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 47%|████████████▌              | 36/77 [03:59<04:49,  7.06s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 48%|████████████▉              | 37/77 [04:07<04:57,  7.43s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 49%|█████████████▎             | 38/77 [04:16<05:07,  7.87s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 51%|█████████████▋             | 39/77 [04:25<05:05,  8.04s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 52%|██████████████             | 40/77 [04:33<04:56,  8.02s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 53%|██████████████▍            | 41/77 [04:41<04:48,  8.00s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 55%|██████████████▋            | 42/77 [04:49<04:40,  8.01s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 56%|███████████████            | 43/77 [04:59<04:56,  8.72s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 57%|███████████████▍           | 44/77 [05:09<05:04,  9.23s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 58%|███████████████▊           | 45/77 [05:23<05:35, 10.47s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 60%|████████████████▏          | 46/77 [05:34<05:33, 10.77s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 61%|████████████████▍          | 47/77 [05:47<05:38, 11.28s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 62%|████████████████▊          | 48/77 [05:59<05:33, 11.50s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 64%|█████████████████▏         | 49/77 [06:14<05:54, 12.67s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 65%|█████████████████▌         | 50/77 [06:27<05:47, 12.86s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 66%|█████████████████▉         | 51/77 [06:43<05:53, 13.61s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 68%|██████████████████▏        | 52/77 [06:54<05:25, 13.02s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████████████████▌        | 53/77 [07:05<04:52, 12.20s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 70%|██████████████████▉        | 54/77 [07:18<04:51, 12.67s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 71%|███████████████████▎       | 55/77 [07:33<04:50, 13.20s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 73%|███████████████████▋       | 56/77 [07:45<04:30, 12.90s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 74%|███████████████████▉       | 57/77 [08:04<04:56, 14.81s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 75%|████████████████████▎      | 58/77 [08:16<04:25, 14.00s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 77%|████████████████████▋      | 59/77 [08:31<04:15, 14.17s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 78%|█████████████████████      | 60/77 [08:46<04:07, 14.57s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 79%|█████████████████████▍     | 61/77 [08:59<03:44, 14.02s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 81%|█████████████████████▋     | 62/77 [09:14<03:32, 14.18s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 82%|██████████████████████     | 63/77 [09:30<03:25, 14.70s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 83%|██████████████████████▍    | 64/77 [09:42<03:00, 13.85s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 84%|██████████████████████▊    | 65/77 [09:52<02:32, 12.74s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 86%|███████████████████████▏   | 66/77 [10:01<02:09, 11.78s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████▍   | 67/77 [10:12<01:54, 11.44s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 88%|███████████████████████▊   | 68/77 [10:23<01:41, 11.23s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 90%|████████████████████████▏  | 69/77 [10:35<01:33, 11.66s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 91%|████████████████████████▌  | 70/77 [10:48<01:23, 11.94s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 92%|████████████████████████▉  | 71/77 [10:59<01:11, 11.84s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 94%|█████████████████████████▏ | 72/77 [11:12<01:00, 12.00s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 95%|█████████████████████████▌ | 73/77 [11:31<00:56, 14.14s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 96%|█████████████████████████▉ | 74/77 [11:44<00:41, 13.90s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 97%|██████████████████████████▎| 75/77 [11:57<00:26, 13.44s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      " 99%|██████████████████████████▋| 76/77 [12:08<00:12, 12.86s/it]/var/folders/kr/xl7k0ks17bq191p_5d8z3x700000gn/T/ipykernel_2939/2981513742.py:16: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  , \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|███████████████████████████| 77/77 [12:14<00:00,  9.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ttest loss: nan\n",
      "\n",
      "\ttest acc: 0.6527133580705009\n",
      "\n",
      "\ttest prec: 0.5951582328205705\n",
      "\n",
      "\ttest rec: 0.6677101037490646\n",
      "\n",
      "\ttest f1: 0.5432614223257578\n",
      "\n",
      "\n",
      "\ttest acc weighted: 0.6527133580705009\n",
      "\n",
      "\ttest prec weighted: 0.8760435136652668\n",
      "\n",
      "\ttest rec weighted: 0.6527133580705009\n",
      "\n",
      "\ttest f1 weighted: 0.7125962518412591\n",
      "\n",
      "\n",
      "\ttest acc binary: 0.6527133580705009\n",
      "\n",
      "\ttest prec binary: 0.23735359157437072\n",
      "\n",
      "\ttest rec binary: 0.7045454545454546\n",
      "\n",
      "\ttest f1 binary: 0.3364920072712281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/adrianahne/miniconda3/envs/causality/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#loss_fn = CrossEntropyLoss()\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "test_prec = []\n",
    "test_rec = []\n",
    "test_f1 = []\n",
    "\n",
    "test_acc_w = [] # weighted\n",
    "test_prec_w = []\n",
    "test_rec_w = []\n",
    "test_f1_w = []\n",
    "\n",
    "test_acc_b = [] # binary\n",
    "test_prec_b = []\n",
    "test_rec_b = []\n",
    "test_f1_b = []\n",
    "\n",
    "for batch in tqdm(test_loader):\n",
    "    batch = tuple(batch[t].to(device) for t in batch)      # batch to GPU\n",
    "    b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch     # unpack inputs from dataloader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits = model(**{\"input_ids\":b_input_ids, \"attention_mask\":b_input_mask, \"token_type_ids\":b_token_type_ids}) # forward pass, calculates logit predictions \n",
    "    \n",
    "    # move logits and labels to CPU\n",
    "    logits = logits.detach().to('cpu').numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "    \n",
    "    metrics = compute_metrics(pred_flat, labels_flat, \"macro\")\n",
    "    test_acc.append(metrics[\"accuracy\"])\n",
    "    test_prec.append(metrics[\"precision\"])\n",
    "    test_rec.append(metrics[\"recall\"])\n",
    "    test_f1.append(metrics[\"f1\"])\n",
    "\n",
    "    metrics = compute_metrics(pred_flat, labels_flat, \"weighted\")\n",
    "    test_acc_w.append(metrics[\"accuracy\"])\n",
    "    test_prec_w.append(metrics[\"precision\"])\n",
    "    test_rec_w.append(metrics[\"recall\"])\n",
    "    test_f1_w.append(metrics[\"f1\"])\n",
    "    \n",
    "    metrics = compute_metrics(pred_flat, labels_flat, \"binary\")\n",
    "    test_acc_b.append(metrics[\"accuracy\"])\n",
    "    test_prec_b.append(metrics[\"precision\"])\n",
    "    test_rec_b.append(metrics[\"recall\"])\n",
    "    test_f1_b.append(metrics[\"f1\"])\n",
    "    \n",
    "print(F'\\n\\ttest loss: {np.mean(test_loss)}')\n",
    "print(F'\\n\\ttest acc: {np.mean(test_acc)}')\n",
    "print(F'\\n\\ttest prec: {np.mean(test_prec)}')\n",
    "print(F'\\n\\ttest rec: {np.mean(test_rec)}')\n",
    "print(F'\\n\\ttest f1: {np.mean(test_f1)}')\n",
    "print()\n",
    "print(F'\\n\\ttest acc weighted: {np.mean(test_acc_w)}')\n",
    "print(F'\\n\\ttest prec weighted: {np.mean(test_prec_w)}')\n",
    "print(F'\\n\\ttest rec weighted: {np.mean(test_rec_w)}')\n",
    "print(F'\\n\\ttest f1 weighted: {np.mean(test_f1_w)}')\n",
    "print()\n",
    "print(F'\\n\\ttest acc binary: {np.mean(test_acc_b)}')\n",
    "print(F'\\n\\ttest prec binary: {np.mean(test_prec_b)}')\n",
    "print(F'\\n\\ttest rec binary: {np.mean(test_rec_b)}')\n",
    "print(F'\\n\\ttest f1 binary: {np.mean(test_f1_b)}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49090909090909096 0.48717948717948717 0.3650793650793651\n",
      "0.7833333333333333 0.8090909090909091 0.7922077922077921\n",
      "0.5 0.21875 0.30434782608695654\n",
      "0.6875 0.8076923076923077 0.6536796536796536\n",
      "0.625 0.6538461538461539 0.43529411764705883\n",
      "0.6666666666666667 0.6666666666666667 0.625\n",
      "0.5625 0.7666666666666666 0.4589371980676329\n",
      "0.5833333333333334 0.8333333333333333 0.5428571428571429\n",
      "0.625 0.9 0.6444444444444445\n",
      "0.4583333333333333 0.36666666666666664 0.4074074074074074\n",
      "0.75 0.8846153846153846 0.7681159420289856\n",
      "0.5714285714285714 0.8 0.49999999999999994\n",
      "0.6545454545454545 0.717948717948718 0.6666666666666666\n",
      "0.625 0.7857142857142857 0.5636363636363637\n",
      "0.5 0.375 0.42857142857142855\n",
      "0.6666666666666666 0.8571428571428572 0.6666666666666666\n",
      "0.625 0.9 0.6444444444444445\n",
      "0.6111111111111112 0.75 0.5151515151515151\n",
      "0.5714285714285714 0.8 0.49999999999999994\n",
      "0.8333333333333333 0.9166666666666667 0.8545454545454545\n",
      "0.5416666666666667 0.5512820512820513 0.5428571428571429\n",
      "0.6111111111111112 0.75 0.5151515151515151\n",
      "0.7 0.75 0.6190476190476191\n",
      "0.6545454545454545 0.717948717948718 0.6666666666666666\n",
      "0.5 0.5 0.4181818181818182\n",
      "0.7 0.8928571428571428 0.7257142857142858\n",
      "0.625 0.6333333333333333 0.6190476190476191\n",
      "0.6 0.7142857142857143 0.4666666666666667\n",
      "0.45454545454545453 0.3333333333333333 0.3846153846153846\n",
      "0.7 0.8928571428571428 0.7257142857142858\n",
      "0.5873015873015872 0.641025641025641 0.5636363636363637\n",
      "0.625 0.7857142857142857 0.5636363636363637\n",
      "0.5238095238095238 0.5272727272727272 0.4920634920634921\n",
      "0.6666666666666666 0.7692307692307692 0.6000000000000001\n",
      "0.6363636363636364 0.6923076923076923 0.4920634920634921\n",
      "0.6545454545454545 0.717948717948718 0.6666666666666666\n",
      "0.6875 0.7181818181818183 0.6761133603238867\n",
      "0.6 0.8666666666666667 0.5897435897435896\n",
      "0.5833333333333334 0.8333333333333333 0.5428571428571429\n",
      "0.4375 0.42727272727272725 0.41700404858299595\n",
      "0.6666666666666666 0.8571428571428572 0.6666666666666666\n",
      "0.5158730158730158 0.5357142857142857 0.45893719806763283\n",
      "0.5545454545454546 0.6071428571428572 0.5428571428571429\n",
      "0.5833333333333334 0.8333333333333333 0.5428571428571429\n",
      "0.5 0.5 0.4181818181818182\n",
      "0.5833333333333334 0.8333333333333333 0.5428571428571429\n",
      "0.5 0.40625 0.4482758620689655\n",
      "0.5625 0.7666666666666666 0.4589371980676329\n",
      "0.5666666666666667 0.5833333333333333 0.5636363636363636\n",
      "0.4375 0.23333333333333334 0.3043478260869565\n",
      "0.9666666666666667 0.75 0.8160919540229885\n",
      "0.6111111111111112 0.75 0.5151515151515151\n",
      "0.6 0.8666666666666667 0.5897435897435896\n",
      "0.7 0.8928571428571428 0.7257142857142858\n",
      "0.5714285714285714 0.8 0.49999999999999994\n",
      "0.5 0.375 0.42857142857142855\n",
      "0.5555555555555556 0.7333333333333334 0.41818181818181815\n",
      "0.6666666666666666 0.8571428571428572 0.6666666666666666\n",
      "0.6 0.7142857142857143 0.4666666666666667\n",
      "0.5 0.34375 0.4074074074074074\n",
      "0.5833333333333334 0.8333333333333333 0.5428571428571429\n",
      "0.5833333333333334 0.8333333333333333 0.5428571428571429\n",
      "0.7142857142857143 0.8461538461538461 0.709090909090909\n",
      "0.65 0.7307692307692308 0.5465587044534412\n",
      "0.5833333333333333 0.6428571428571428 0.5897435897435898\n",
      "0.6428571428571428 0.8214285714285714 0.6135265700483092\n",
      "0.7 0.75 0.6190476190476191\n",
      "0.5 0.5 0.5\n",
      "0.6 0.8666666666666667 0.5897435897435896\n",
      "0.75 0.9285714285714286 0.7948717948717948\n",
      "0.45454545454545453 0.3333333333333333 0.3846153846153846\n",
      "0.6111111111111112 0.75 0.5151515151515151\n",
      "0.5714285714285714 0.8 0.49999999999999994\n",
      "0.55 0.7 0.37662337662337664\n",
      "0.45 0.3 0.36000000000000004\n",
      "0.5714285714285714 0.8 0.49999999999999994\n",
      "0.5 0.35714285714285715 0.41666666666666663\n"
     ]
    }
   ],
   "source": [
    "for p, r, f in zip(test_prec, test_rec, test_f1):\n",
    "    print(p,r,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print predictions of last test set batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded Sentence:\n",
      "['<s>', '@USER', 'Hey', 'Chee@@', 'to', 'man', ',', 'I', 'am', 'older', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "prediction: 0\n",
      "\n",
      "Padded Sentence:\n",
      "['<s>', 'My', 'body', 'almost', 'went', 'into', 'diabetic', 'coma', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "prediction: 0\n",
      "\n",
      "Padded Sentence:\n",
      "['<s>', 'He', 'froze', 'the', 'rule', 'that', 'lowered', 'insulin', 'prices', 'that', 'Trump', 'made', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "prediction: 0\n",
      "\n",
      "Padded Sentence:\n",
      "['<s>', 'It', \"'\", 's', 'scary', 'how', 'progressive', 'it', 'is', 'and', 'how', 'doctors', 'do', \"n't\", 'tell', 'you', 'it', \"'\", 's', 'reversible', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "prediction: 1\n"
     ]
    }
   ],
   "source": [
    "# take last batch of test set:\n",
    "\n",
    "for i in range(len(batch)):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(b_input_ids[i])\n",
    "    print(\"\\nPadded Sentence:\")\n",
    "    print(tokens)\n",
    "    print(\"prediction:\", pred_flat[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add seed => check\n",
    "# add binary accuracy  => ???????\n",
    "# add plot loss function accuracy => validation accu\n",
    "\n",
    "# y - axis: loss function; validation accuracy\n",
    "# x - axis: epochs\n",
    "\n",
    "\n",
    "# epochs, learning rate => ok\n",
    "\n",
    "# 90% training => 10% test  => ok\n",
    "# how to use random batch of training set for validation\n",
    "\n",
    "# clean notebook\n",
    "\n",
    "# clean data sheet => check\n",
    "\n",
    "# Check Pytorch: EarlyStopping add => check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Causality-BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
